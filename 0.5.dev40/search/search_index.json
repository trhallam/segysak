{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"SEGY-SAK","text":"<p>SEGY-SAK: A library for loading and manipulating SEG-Y data with Python using Xarray</p> <p>SEGY-SAK can be use as a tool to handle SEG-Y files inside Python environment.</p> <p>By including  SEGY-SAK in your toolbox you will be able to load or transform the original binary SEG-Y data into more accessible and Python friendly formats. It leverages the work of  Segyio to simplify loading of common SEG-Y formats into <code>xarray.Dataset</code> objects for ease of use and to NetCDF4 files for better on disk  and large file performance using Dask. Tools to help users create new volumes and to return data to SEG-Y are also included.</p>"},{"location":"index.html#feature","title":"Feature","text":"<p>Loading Assistance</p> <ul> <li>Loading of nDimensional SEG-Y data</li> <li>Header editing</li> <li>Loading of seismic data with missing traces</li> </ul> <p>Cube geometry functions</p> <ul> <li>Generate cube affine transform</li> <li>Fill cdp_x and cdp_y</li> </ul> <p>Arbitrary slicing and extration</p> <ul> <li>Label based slicing</li> <li>Arbitrary line slicing</li> <li>Horizon extraction and sculpting</li> <li>Well path extraction</li> </ul> <p>Integrates with the existing Python scientific stack</p> <ul> <li>Leverage Xarray for easy coordinate management merging and plotting</li> <li>Inherited compatibility with core Python libraries (     NumPy, Scipy and Pandas).</li> <li> <p>Works well with matplotlib and Pyvista</p> </li> <li> <p>Experimental baked in ZGY support via PyZgy and Open-ZGY</p> </li> </ul> <p>Scalability</p> <ul> <li>Lazy loading of SEGY data (do not load the full volume into memory) for large files.</li> <li>NetCDF4 files work with Dask to scale your Python code to multi-core and     distributed memory computing</li> </ul>"},{"location":"index.html#documentation","title":"Documentation","text":"<p>Getting Started</p> <ul> <li>Why Segysak?why-segysak`</li> <li>:doc:<code>installation</code></li> <li>:doc:<code>tutorial</code></li> </ul> <p>.. toctree::    :maxdepth: 1    :hidden:    :caption: Getting Started</p> <p>why-segysak    installation    examples/QuickOverview    tutorial</p> <p>User Guide</p> <ul> <li>:doc:<code>seisnc-standard</code></li> <li>:doc:<code>examples</code></li> <li>:doc:<code>command-line-tool</code></li> <li>:doc:<code>faq</code></li> </ul> <p>.. toctree::    :maxdepth: 1    :caption: Contents    :hidden:</p> <p>seisnc-standard    examples    command-line-tool    faq</p> <p>Help &amp; Reference</p> <ul> <li>:doc:<code>_temp/contributing</code></li> <li>:doc:<code>api</code></li> </ul> <p>.. * :doc:<code>related-projects</code></p> <p>.. toctree::    :maxdepth: 2    :caption: Help &amp; reference    :hidden:</p> <p>_temp/contributing    api</p>"},{"location":"index.html#see-also","title":"See also","text":"<p>Fundamental Python libraries to SEGY-SAK are Segyio  and Xarray.</p> <p>Many of the examples in this documentation use a subset of the the full Volve dataset which was published by Equinor and you can read about it or get a copy of it here.</p>"},{"location":"index.html#license","title":"License","text":"<p>Segysak use the GPL-3 license.</p> <p>The GNU General Public License is a free, copyleft license for software and other kinds of works.</p>"},{"location":"api.html","title":"API reference","text":"<p>This set of pages provides an auto-generated summary of the <code>segysak</code> package API.  For more details and examples, refer to the relevant chapters in the User Guide part of the documentation.</p>"},{"location":"history.html","title":"History","text":""},{"location":"history.html#history","title":"History","text":"<p>Segysak was originally conceived out of a need for a better interface to SEG-Y data in Python. The groundwork was layed by Tony Hallam but development really began during the Transform 2020 Software Underground Hackathon held online across the world due to the cancellation of of the EAGE Annual in June of that year. Significant contributions during the hackathon were made by Steve Purves, Gijs Straathof, Fabio Contreras and Alessandro Amato del Monte.</p> <p>Significant updates were made at Transform 2021.  Multiple new and advanced examples were released. A 2 hour video tutorial and notebook as a demonstration of key functionality and an introduction to Xarry for seismic applications streamed.  Experimental ZGY support was introduced.</p>"},{"location":"installation.html","title":"Installation","text":"<p>SEGY-SAK can be installed by using <code>pip</code> from PyPi and from source.</p>"},{"location":"installation.html#python-package-index-via-pip","title":"Python Package Index via <code>pip</code>","text":"<p>From the command line run the <code>pip</code> package manager</p> Bash<pre><code>python -m pip install segysak\n</code></pre>"},{"location":"installation.html#install-from-source","title":"Install from source","text":"<p>Clone the SEGY-SAK Github repository and in the top level directory run</p> Bash<pre><code>python -m pip install .\n</code></pre> <p>To run the tests install the test dependencies and run <code>pytest</code></p> Bash<pre><code>python -m pip install .[test]\npytest\n</code></pre>"},{"location":"seisnc-standard.html","title":"SEISNC Standard for <code>xarray</code>","text":"<p>The <code>xarray</code> seismic specification termed <code>seisnc</code> can be used by SEGY-SAK to output NETCDF4 files is more performant for Python operations than standard SEG-Y. Unlike SEG-Y, <code>xarray</code> compatible files fit neatly into the Python scientific stack providing operations like lazy loading, easy slicing, compatibility with multi-core and multi-node operations using <code>dask</code> as well as important features such as labelled axes and coordinates.</p> <p>This specification is not meant to be prescriptive but outlines some basic requirements for <code>xarray</code> datasets to work with SEGY-SAK functionality.</p> <p>SEGY-SAK uses the convention <code>.seisnc</code> for the suffix on NETCDF4 files it creates. These files are datasets with specific 1D and 2D coordinates and have a single variable called <code>data</code>. The <code>data</code> variable contains the seismic cube volume or 2D line traces. Attributes can be used to provide further metadata about the cube.</p>"},{"location":"seisnc-standard.html#3d-and-3d-gathers","title":"3D and 3D Gathers","text":"<p>SEGY-SAK uses the convention labels of <code>iline</code>, <code>xline</code> and <code>offset</code> to describe the bins of 3D data. Vertical dimensions are <code>twt</code> and <code>depth</code>. A typical <code>xarray</code> dataset created by SEGY-SAK will return for example</p> Python<pre><code>seisnc_3d = segysak.segy_loader('test3d.sgy', iline=189, xline=193)\nseisnc_3d.sizes\n\n   Frozen(SortedKeysDict({'iline': 61, 'xline': 202, 'twt': 850}))\n</code></pre>"},{"location":"seisnc-standard.html#2d-and-2d-gathers","title":"2D and 2D Gathers","text":"<p>For 2D data SEGY-SAK uses the dimension labels <code>cdp</code> and <code>offset</code>. This allows the package to distinguish between 2D and 3D data to allow automation on saving and convenience wrappers. The same vertical dimensions apply as for 3D. A typical <code>xarray</code> in 2D format would return</p> Text Only<pre><code>seisnc_2d = segysak.segy_loader('test2d.sgy', cdp=21)\nseisnc_2d.sizes\n\n   Frozen(SortedKeysDict({'cdp': 61, 'twt': 850}))\n</code></pre>"},{"location":"seisnc-standard.html#coordinates","title":"Coordinates","text":"<p>If the <code>cdp_x</code> and <code>cdp_y</code> byte locations are specified during loading the SEG-Y the coordinates will be populated from the headers with the variable names <code>cdp_x</code> and <code>cdp_y</code>. These will have dimensions equivalent to the horizontal dimensions of the data (<code>iline</code>, <code>xline</code> for 3D and <code>cdp</code> for 2D).</p>"},{"location":"seisnc-standard.html#attributes","title":"Attributes","text":"<p>Any number of attributes can be added to a <code>siesnc</code> file. Currently the following attributes are extracted or reserved for use by SEGY-SAK.</p> <ul> <li><code>ns</code> number of samples per trace</li> <li><code>ds</code> sample interval</li> <li><code>text</code> ebcidc header as ascii text</li> <li><code>measurement_sys</code> vertical units of the data</li> <li><code>d3_domain</code> vertical domain of the data</li> <li><code>epsg</code> data epsg code</li> <li><code>corner_points</code> corner points of the dataset in grid coordinates</li> <li><code>corner_points_xy</code> corner points of the dataset in xy</li> <li><code>source_file</code> name of the file the dataset was created from</li> <li><code>srd</code> seismic reference datum of the data in vertical units <code>measurement_sys</code>    and <code>d3_domain</code></li> <li><code>datatype</code> the data type e.g. amplitude, velocity, attribute</li> <li><code>percentiles</code> this is an array of approximate percentile values created during    scanning from SEG-Y. Primarily this is useful for plotting by limiting the dynamic    range of the display. The percentiles are in percent 0, 0.1, 10, 50, 90, 99.9 &amp; 100.</li> </ul>"},{"location":"tutorial.html","title":"Tutorial","text":"<p>A notebook tutorial and accompanying YouTube video were created and recorded for Transform 2021. The tutorial covers much of the example material and more with deeper explanations for integrating with Xarray.</p> <p>Checkout the tutorial repository(https://github.com/trhallam/segysak-t21-tutorial) or launch an interactive session on binder via  </p>"},{"location":"why-segysak.html","title":"Overview: Why SEGY-SAK?","text":"<p>The objective of SEGY-SAK was to bring together the usefulness of <code>segyio</code> and <code>xarray</code> to improve accessibility of seismic data for geoscientists using Python. Key objectives for this project are to make loading and exporting SEG-Y easier by offering common simple interfaces to <code>segyio</code> and to then load data into an <code>xarray</code> format either directly into memory or by streaming large files to disk.</p> <p>Currently SEGY-SAK can load 2D, 2D gathers, 3D and 3D gathers into a common format stable format called <code>seisnc</code> that you can use faithfully in your downstream applications. The <code>seisnc</code> format is outlined in the standard section of the documentation and is the basis for all of the other functionality provided by SEGY-SAK.</p> <p>Beyond that, the <code>segysak</code> package has enhancements to <code>xarray</code> that reduce complexity for common seismic related tasks to extracting information from seismic such as arbitrary line, well-deviation or horizon extractions. We also offer a long list of notebook examples to help new and experienced Python users alike traverse the intricacies of Xarray and demonstrate how to do common tasks with <code>segysak</code>.</p> <p>Finally, the use of Xarray and the NetCDF4 format allows you to scale your problems using Dask. Dask can lazily load and process data across multiple cores and even distributed memory allowing you to apply your Python code to large seismic volumes. This process also means you can plot slices from your volumes in Python without loading the full dataset into memory.</p>"},{"location":"api/segy_inspecting.html","title":"Inspecting SEG-Y","text":""},{"location":"api/segy_inspecting.html#segysak.segy.segy_header_scrape","title":"<code>segy_header_scrape(segyfile, partial_scan=None, silent=False, bytes_filter=None, chunk=100000, **segyio_kwargs)</code>","text":"<p>Scape all data from segy trace headers</p> <p>Parameters:</p> Name Type Description Default <code>segyfile</code> <code>str</code> <p>SEG-Y File path</p> required <code>partial_scan</code> <code>int</code> <p>Setting partial scan to a positive int will scan only that many traces. Defaults to None.</p> <code>None</code> <code>silent</code> <code>bool</code> <p>Disable progress bar.</p> <code>False</code> <code>bytes_filter</code> <code>list</code> <p>List of byte locations to load exclusively.</p> <code>None</code> <code>chunk</code> <code>int</code> <p>Number of traces to read in one go.</p> <code>100000</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: Raw header information in table for scanned traces.</p> Source code in <code>segysak/segy/_segy_headers.py</code> Python<pre><code>def segy_header_scrape(\n    segyfile: str,\n    partial_scan: Union[int, None] = None,\n    silent: bool = False,\n    bytes_filter: Union[List[int], None] = None,\n    chunk: int = 100_000,\n    **segyio_kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Scape all data from segy trace headers\n\n    Args:\n        segyfile (str): SEG-Y File path\n        partial_scan (int): Setting partial scan to a positive int will scan only\n            that many traces. Defaults to None.\n        silent (bool): Disable progress bar.\n        bytes_filter (list): List of byte locations to load exclusively.\n        chunk (int): Number of traces to read in one go.\n\n    Returns:\n        pandas.DataFrame: Raw header information in table for scanned traces.\n    \"\"\"\n    check_tracefield(bytes_filter)\n\n    assert (chunk &gt; 0) and isinstance(chunk, int)\n    header_keys = _active_tracefield_segyio()\n    enum_byte_index = {\n        int(byte_loc): i for i, byte_loc in enumerate(header_keys.values())\n    }\n\n    if bytes_filter:\n        for byte_loc in bytes_filter:\n            assert byte_loc in enum_byte_index\n        bytes_filter_index = [enum_byte_index[byte_loc] for byte_loc in bytes_filter]\n    else:\n        bytes_filter_index = list(enum_byte_index.values())\n\n    columns = [list(header_keys.keys())[i] for i in bytes_filter_index]\n    segyio_kwargs[\"ignore_geometry\"] = True\n\n    with segyio.open(segyfile, \"r\", **segyio_kwargs) as segyf:\n        segyf_hgen = segyf.header[:]\n        ntraces = segyf.tracecount\n        if partial_scan is not None:\n            ntraces = min(ntraces, int(partial_scan))\n\n        head_df = pd.DataFrame(index=pd.Index(range(ntraces)), columns=columns)\n        slc_end = chunk\n        with tqdm(total=ntraces, disable=silent, **TQDM_ARGS) as pbar:\n            while slc_end &lt;= ntraces + chunk - 1:\n                slc = slice(slc_end - chunk, min(slc_end, ntraces), 1)\n                # take headers returned from segyio and create lists for a dataframe\n                head_df.iloc[slc, :] = np.vstack(\n                    [\n                        list(next(segyf_hgen).values())\n                        for _ in range(slc.stop - slc.start)\n                    ]\n                )[:, bytes_filter_index]\n                slc_end += chunk\n                pbar.update(slc.stop - slc.start)\n\n    head_df.replace(to_replace=-2147483648, value=np.nan, inplace=True)\n\n    for col in head_df:\n        head_df[col] = pd.to_numeric(head_df[col], downcast=\"unsigned\")\n\n    return head_df\n</code></pre>"},{"location":"api/segy_inspecting.html#segysak.segy.segy_bin_scrape","title":"<code>segy_bin_scrape(segyfile, **segyio_kwargs)</code>","text":"<p>Scrape binary header</p> <p>Parameters:</p> Name Type Description Default <code>segyfile</code> <code>str</code> <p>SEG-Y File path</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Binary header</p> Source code in <code>segysak/segy/_segy_headers.py</code> Python<pre><code>def segy_bin_scrape(segyfile: str, **segyio_kwargs) -&gt; Dict:\n    \"\"\"Scrape binary header\n\n    Args:\n        segyfile: SEG-Y File path\n\n    Returns:\n        Binary header\n    \"\"\"\n    bk = _active_binfield_segyio()\n    segyio_kwargs[\"ignore_geometry\"] = True\n    with segyio.open(segyfile, \"r\", **segyio_kwargs) as segyf:\n        return {key: segyf.bin[item] for key, item in bk.items()}\n</code></pre>"},{"location":"api/segy_inspecting.html#segysak.segy.segy_header_scan","title":"<code>segy_header_scan(segyfile, max_traces_scan=1000, silent=False, **segyio_kwargs)</code>","text":"<p>Perform a scan of the segy file headers and return ranges.</p> <p>To get the complete raw header values see <code>segy_header_scrape</code></p> <p>Parameters:</p> Name Type Description Default <code>segyfile</code> <code>str</code> <p>Segy File Path</p> required <code>max_traces_scan</code> <code>int</code> <p>Number of traces to scan. For scan all traces set to &lt;= 0. Defaults to 1000.</p> <code>1000</code> <code>silent</code> <code>bool</code> <p>Disable progress bar.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Uses pandas describe to return statistics of your headers.</p> Source code in <code>segysak/segy/_segy_headers.py</code> Python<pre><code>def segy_header_scan(\n    segyfile: str, max_traces_scan: int = 1000, silent: bool = False, **segyio_kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"Perform a scan of the segy file headers and return ranges.\n\n    To get the complete raw header values see `segy_header_scrape`\n\n    Args:\n        segyfile: Segy File Path\n        max_traces_scan: Number of traces to scan.\n            For scan all traces set to &lt;= 0. Defaults to 1000.\n        silent: Disable progress bar.\n\n    Returns:\n        Uses pandas describe to return statistics of your headers.\n    \"\"\"\n    if max_traces_scan &lt;= 0:\n        max_traces_scan = None\n    else:\n        if not isinstance(max_traces_scan, int):\n            raise ValueError(\"max_traces_scan must be int\")\n\n    head_df = segy_header_scrape(\n        segyfile, max_traces_scan, silent=silent, **segyio_kwargs\n    )\n\n    header_keys = head_df.describe().T\n    pre_cols = list(header_keys.columns)\n    header_keys[\"byte_loc\"] = list(_active_tracefield_segyio().values())\n    header_keys = header_keys[[\"byte_loc\"] + pre_cols]\n    header_keys.nscan = head_df.shape[0]\n    return header_keys\n</code></pre>"},{"location":"api/segy_inspecting.html#segysak.segy.get_segy_texthead","title":"<code>get_segy_texthead(segyfile, ext_headers=False, no_richstr=False, **segyio_kwargs)</code>","text":"<p>Return the ebcidc</p> <p>Parameters:</p> Name Type Description Default <code>segyfile</code> <code>str</code> <p>Segy File Path</p> required <code>ext_headers</code> <code>bool</code> <p>Return EBCIDC and extended headers in list. Defaults to False</p> <code>False</code> <code>no_richstr</code> <code>bool</code> <p>Defaults to False. If true the returned string will not be updated for pretty HTML printing.</p> <code>False</code> <code>segyio_kwargs</code> <p>Key word arguments to pass to segyio.open</p> <code>{}</code> <p>Returns:     str: Returns the EBCIDC text as a formatted paragraph.</p> Source code in <code>segysak/segy/_segy_text.py</code> Python<pre><code>def get_segy_texthead(segyfile, ext_headers=False, no_richstr=False, **segyio_kwargs):\n    \"\"\"Return the ebcidc\n\n    Args:\n        segyfile (str): Segy File Path\n        ext_headers (bool): Return EBCIDC and extended headers in list.\n            Defaults to False\n        no_richstr (bool, optional): Defaults to False. If true the returned string\n            will not be updated for pretty HTML printing.\n        segyio_kwargs: Key word arguments to pass to segyio.open\n    Returns:\n        str: Returns the EBCIDC text as a formatted paragraph.\n    \"\"\"\n\n    with open(segyfile, mode=\"rb\") as f:\n        f.seek(0, 0)  # Locate our position to first byte of file\n        data = f.read(3200)  # Read the first 3200 byte from our position\n\n    if _isascii(data) and ext_headers == False:\n        text = data.decode(\"ascii\")  # EBCDIC encoding\n        text = _text_fixes(text)\n        text = segyio.tools.wrap(text)\n    elif ext_headers == False:\n        text = data.decode(\"cp500\")  # text is ebcidc\n        text = _text_fixes(text)\n        text = segyio.tools.wrap(text)\n    else:\n        segyio_kwargs[\"ignore_geometry\"] = True\n        try:  # pray that the encoding is ebcidc\n            with segyio.open(segyfile, \"r\", **segyio_kwargs) as segyf:\n                text = segyf.text[0].decode(\"ascii\", \"replace\")\n                text = _text_fixes(text)\n                text = segyio.tools.wrap(text)\n                if segyf.ext_headers and ext_headers:\n                    text2 = segyf.text[1].decode(\"ascii\", \"replace\")\n                    text = [text, text2]\n        except UnicodeDecodeError as err:\n            print(err)\n            print(\"The segy text header could not be decoded.\")\n\n    if no_richstr:\n        return text\n    else:\n        return _upgrade_txt_richstr(text)\n</code></pre>"},{"location":"api/segy_inspecting.html#segysak.segy.header_as_dimensions","title":"<code>header_as_dimensions(head_df, dims)</code>","text":"<p>Convert dim_kwargs to a diction of dimensions. Also useful for checking geometry is correct and unique for each trace in a segy file header.</p> <p>Parameters:</p> Name Type Description Default <code>head_df</code> <code>DataFrame</code> <p>The header DataFrame from <code>segy_header_scrape</code>.</p> required <code>dims</code> <code>tuple</code> <p>Dimension names (str) as per head_df.</p> required Source code in <code>segysak/segy/_segy_headers.py</code> Python<pre><code>def header_as_dimensions(head_df: pd.DataFrame, dims: tuple) -&gt; Dict[str, np.array]:\n    \"\"\"Convert dim_kwargs to a diction of dimensions. Also useful for checking\n    geometry is correct and unique for each trace in a segy file header.\n\n    Args:\n        head_df: The header DataFrame from `segy_header_scrape`.\n        dims: Dimension names (str) as per head_df.\n    \"\"\"\n    unique_dims = dict()\n    for dim in dims:\n        # get unique values of dimension and sort them ascending\n        as_unique = head_df[dim].unique()\n        unique_dims[dim] = np.sort(as_unique)\n\n    if head_df[list(dims)].shape != head_df[list(dims)].drop_duplicates().shape:\n        raise ValueError(\n            \"The selected dimensions results in multiple traces per \"\n            \"dimension location, add additional dimensions or use \"\n            \"trace numbering byte location to load as 2D.\"\n        )\n\n    return unique_dims\n</code></pre>"},{"location":"api/segy_loading.html","title":"Loading SEG-Y","text":""},{"location":"api/segy_loading.html#segysak.segy.segy_loader","title":"<code>segy_loader(segyfile, cdp=None, iline=None, xline=None, cdp_x=None, cdp_y=None, offset=None, vert_domain='TWT', data_type='AMP', ix_crop=None, cdp_crop=None, xy_crop=None, z_crop=None, return_geometry=False, silent=False, extra_byte_fields=None, head_df=None, **segyio_kwargs)</code>","text":"<p>Load SEG-Y file into xarray.Dataset</p> <p>The output dataset has the following structure     Dimensions:         cdp/iline - CDP or Inline axis         xline - Xline axis         twt/depth - The vertical axis         offset - Offset/Angle Axis     Coordinates:         iline - The inline numbering         xline - The xline numbering         cdp_x - Eastings         cdp_y - Northings         cdp - Trace Number for 2d     Variables         data - The data volume     Attributes:         ns - number of samples vertical         sample_rate - sample rate in ms/m         test - text header         measurement_system : m/ft         source_file : segy source         srd : seismic reference datum         percentiles : data amplitude percentiles         coord_scalar : from trace headers</p> <p>Parameters:</p> Name Type Description Default <code>segyfile</code> <code>str</code> <p>Input segy file path</p> required <code>cdp</code> <code>int</code> <p>The CDP byte location, usually 21.</p> <code>None</code> <code>iline</code> <code>int</code> <p>Inline byte location, usually 189</p> <code>None</code> <code>xline</code> <code>int</code> <p>Cross-line byte location, usually 193</p> <code>None</code> <code>cdp_x</code> <code>int</code> <p>UTMX byte location, usually 181</p> <code>None</code> <code>cdp_y</code> <code>int</code> <p>UTMY byte location, usually 185</p> <code>None</code> <code>offset</code> <code>int</code> <p>Offset/angle byte location</p> <code>None</code> <code>vert_domain</code> <code>str</code> <p>Vertical sampling domain. One of ['TWT', 'DEPTH']. Defaults to 'TWT'.</p> <code>'TWT'</code> <code>data_type</code> <code>str</code> <p>Data type ['AMP', 'VEL']. Defaults to 'AMP'.</p> <code>'AMP'</code> <code>ix_crop</code> <code>list</code> <p>List of minimum and maximum inline and crossline to output. Has the form '[min_il, max_il, min_xl, max_xl]'. Ignored for 2D data.</p> <code>None</code> <code>cdp_crop</code> <code>list</code> <p>List of minimum and maximum cmp values to output. Has the form '[min_cmp, max_cmp]'. Ignored for 3D data.</p> <code>None</code> <code>xy_crop</code> <code>list</code> <p>List of minimum and maximum cdp_x and cdp_y to output. Has the form '[min_x, max_x, min_y, max_y]'. Ignored for 2D data.</p> <code>None</code> <code>z_crop</code> <code>list</code> <p>List of minimum and maximum vertical samples to output. Has the form '[min, max]'.</p> <code>None</code> <code>return_geometry</code> <code>bool</code> <p>If true returns an xarray.dataset which doesn't contain data but mirrors the input volume header information.</p> <code>False</code> <code>silent</code> <code>bool</code> <p>Disable progress bar.</p> <code>False</code> <code>extra_byte_fields</code> <code>list / mapping</code> <p>A list of int or mapping of byte fields that should be returned as variables in the dataset.</p> <code>None</code> <code>head_df</code> <code>DataFrame</code> <p>The DataFrame output from <code>segy_header_scrape</code>. This DataFrame can be filtered by the user to load select trace sets. Trace loading is based upon the DataFrame index.</p> <code>None</code> <code>**segyio_kwargs</code> <p>Extra keyword arguments for segyio.open</p> <code>{}</code> <p>Returns:</p> Type Description <p>xarray.Dataset: If ncfile keyword is specified returns open handle to disk netcdf4, otherwise the data in memory. If return_geometry is True does not load trace data and returns headers in geometry.</p> Source code in <code>segysak/segy/_segy_loader.py</code> Python<pre><code>def segy_loader(\n    segyfile,\n    cdp=None,\n    iline=None,\n    xline=None,\n    cdp_x=None,\n    cdp_y=None,\n    offset=None,\n    vert_domain=\"TWT\",\n    data_type=\"AMP\",\n    ix_crop=None,\n    cdp_crop=None,\n    xy_crop=None,\n    z_crop=None,\n    return_geometry=False,\n    silent=False,\n    extra_byte_fields=None,\n    head_df=None,\n    **segyio_kwargs,\n):\n    \"\"\"Load SEG-Y file into xarray.Dataset\n\n    The output dataset has the following structure\n        Dimensions:\n            cdp/iline - CDP or Inline axis\n            xline - Xline axis\n            twt/depth - The vertical axis\n            offset - Offset/Angle Axis\n        Coordinates:\n            iline - The inline numbering\n            xline - The xline numbering\n            cdp_x - Eastings\n            cdp_y - Northings\n            cdp - Trace Number for 2d\n        Variables\n            data - The data volume\n        Attributes:\n            ns - number of samples vertical\n            sample_rate - sample rate in ms/m\n            test - text header\n            measurement_system : m/ft\n            source_file : segy source\n            srd : seismic reference datum\n            percentiles : data amplitude percentiles\n            coord_scalar : from trace headers\n\n    Args:\n        segyfile (str): Input segy file path\n        cdp (int, optional): The CDP byte location, usually 21.\n        iline (int, optional): Inline byte location, usually 189\n        xline (int, optional): Cross-line byte location, usually 193\n        cdp_x (int, optional): UTMX byte location, usually 181\n        cdp_y (int, optional): UTMY byte location, usually 185\n        offset (int, optional): Offset/angle byte location\n        vert_domain (str, optional): Vertical sampling domain. One of ['TWT', 'DEPTH']. Defaults to 'TWT'.\n        data_type (str, optional): Data type ['AMP', 'VEL']. Defaults to 'AMP'.\n        ix_crop (list, optional): List of minimum and maximum inline and crossline to output.\n            Has the form '[min_il, max_il, min_xl, max_xl]'. Ignored for 2D data.\n        cdp_crop (list, optional): List of minimum and maximum cmp values to output.\n            Has the form '[min_cmp, max_cmp]'. Ignored for 3D data.\n        xy_crop (list, optional): List of minimum and maximum cdp_x and cdp_y to output.\n            Has the form '[min_x, max_x, min_y, max_y]'. Ignored for 2D data.\n        z_crop (list, optional): List of minimum and maximum vertical samples to output.\n            Has the form '[min, max]'.\n        return_geometry (bool, optional): If true returns an xarray.dataset which doesn't contain data but mirrors\n            the input volume header information.\n        silent (bool): Disable progress bar.\n        extra_byte_fields (list/mapping): A list of int or mapping of byte fields that should be returned as variables in the dataset.\n        head_df (pandas.DataFrame): The DataFrame output from `segy_header_scrape`. This DataFrame can be filtered by the user\n            to load select trace sets. Trace loading is based upon the DataFrame index.\n        **segyio_kwargs: Extra keyword arguments for segyio.open\n\n    Returns:\n        xarray.Dataset: If ncfile keyword is specified returns open handle to disk netcdf4,\n            otherwise the data in memory. If return_geometry is True does not load trace data and\n            returns headers in geometry.\n    \"\"\"\n    extra_byte_fields = _loader_converter_checks(cdp, iline, xline, extra_byte_fields)\n\n    head_df, head_bin, head_loc = _loader_converter_header_handling(\n        segyfile,\n        cdp=cdp,\n        iline=iline,\n        xline=xline,\n        cdp_x=cdp_x,\n        cdp_y=cdp_y,\n        offset=offset,\n        vert_domain=vert_domain,\n        data_type=data_type,\n        ix_crop=ix_crop,\n        cdp_crop=cdp_crop,\n        xy_crop=xy_crop,\n        z_crop=z_crop,\n        return_geometry=return_geometry,\n        silent=silent,\n        extra_byte_fields=extra_byte_fields,\n        head_df=head_df,\n        **segyio_kwargs,\n    )\n\n    byte_loc = [\n        bytel\n        for bytel in [cdp, iline, xline, cdp_x, cdp_y, offset]\n        if bytel is not None\n    ]\n    check_tracefield(byte_loc)\n\n    common_args = (segyfile, head_df, head_bin, head_loc)\n\n    common_kwargs = dict(\n        zcrop=z_crop,\n        vert_domain=vert_domain,\n        data_type=data_type,\n        return_geometry=return_geometry,\n        silent=silent,\n    )\n\n    # 3d data needs iline and xline\n    if all(v is not None for v in (head_loc.iline, head_loc.xline)):\n        print(\"Loading as 3D\")\n        ds = _3dsegy_loader(\n            *common_args,\n            **common_kwargs,\n            **segyio_kwargs,\n        )\n        indexer = [\"il_index\", \"xl_index\"]\n        dims = (\n            DimensionKeyField.threed_head\n            if offset is None\n            else DimensionKeyField.threed_ps_head\n        )\n        is3d2d = True\n\n    # 2d data\n    elif head_loc.cdp is not None:\n        print(\"Loading as 2D\")\n        ds = _2dsegy_loader(*common_args, **common_kwargs, **segyio_kwargs)\n        indexer = [\"cdp_index\"]\n        dims = (\n            DimensionKeyField.twod_head\n            if offset is None\n            else DimensionKeyField.twod_ps_head\n        )\n        is3d2d = True\n\n    # fallbak to just a 2d array of traces\n    else:\n        ds = _2dsegy_loader(*common_args, **common_kwargs, **segyio_kwargs)\n        indexer = []\n        dims = DimensionKeyField.cdp_2d\n        is3d2d = False\n\n    indexer = indexer + [\"off_index\"] if offset is not None else indexer\n\n    ds = _loader_converter_write_headers(\n        ds, head_df, indexer, dims, extra_byte_fields, is3d2d=is3d2d\n    )\n\n    # ds.seis.get_corner_points()\n    return ds\n</code></pre>"},{"location":"api/segy_loading.html#segysak.segy.segy_freeloader","title":"<code>segy_freeloader(segyfile, vert_domain='TWT', data_type='AMP', return_geometry=False, silent=False, extra_byte_fields=None, head_df=None, segyio_kwargs=None, **dim_kwargs)</code>","text":"<p>Freeform loader for SEG-Y data. This loader allows you to load SEG-Y into an xarray.Dataset using an arbitrary number of header locations to create othogonal dimensions. This is an eager loader and will transfer the entire SEG-Y and requested header information to memory.</p> <p>From the dimension header locations specified the freeloader will try to create a Dataset where each trace is assigned to a dimension.</p> <p>Parameters:</p> Name Type Description Default <code>segyfile</code> <code>string</code> <p>The SEG-Y file/path.</p> required <code>vert_domain</code> <code>str</code> <p>One of ('TWT', 'DEPTH'). Defaults to 'TWT'.</p> <code>'TWT'</code> <code>data_type</code> <code>str</code> <p>Defaults to \"AMP\".</p> <code>'AMP'</code> <code>return_geometry</code> <code>bool</code> <p>If true, just returned the empty dataset based upon the calcuated header geometry. Defaults to False.</p> <code>False</code> <code>silent</code> <code>bool</code> <p>Turn off progress bars. Defaults to False.</p> <code>False</code> <code>extra_byte_fields</code> <code>dict</code> <p>Additional header information to load into the Dataset. Defaults to None.</p> <code>None</code> <code>head_df</code> <code>DataFrame</code> <p>The DataFrame output from <code>segy_header_scrape</code>. This DataFrame can be filtered by the user to load select trace sets. Trace loading is based upon the DataFrame index.</p> <code>None</code> <code>segyio_kwargs</code> <code>dict</code> <p>Extra keyword arguments for segyio.open</p> <code>None</code> <code>**dim_kwargs</code> <p>Dimension names and byte location pairs.</p> <code>{}</code> Source code in <code>segysak/segy/_segy_loader.py</code> Python<pre><code>def segy_freeloader(\n    segyfile,\n    vert_domain=\"TWT\",\n    data_type=\"AMP\",\n    return_geometry=False,\n    silent=False,\n    extra_byte_fields=None,\n    head_df=None,\n    segyio_kwargs=None,\n    **dim_kwargs,\n):\n    \"\"\"Freeform loader for SEG-Y data. This loader allows you to load SEG-Y into\n    an xarray.Dataset using an arbitrary number of header locations to create\n    othogonal dimensions. This is an eager loader and will transfer the entire\n    SEG-Y and requested header information to memory.\n\n    From the dimension header locations specified the freeloader will try to\n    create a Dataset where each trace is assigned to a dimension.\n\n    Args:\n        segyfile (string): The SEG-Y file/path.\n        vert_domain (str, optional): One of ('TWT', 'DEPTH'). Defaults to 'TWT'.\n        data_type (str, optional): Defaults to \"AMP\".\n        return_geometry (bool, optional): If true, just returned the empty\n            dataset based upon the calcuated header geometry. Defaults to False.\n        silent (bool, optional): Turn off progress bars. Defaults to False.\n        extra_byte_fields (dict, optional): Additional header information to\n            load into the Dataset. Defaults to None.\n        head_df (pandas.DataFrame): The DataFrame output from `segy_header_scrape`.\n            This DataFrame can be filtered by the user\n            to load select trace sets. Trace loading is based upon the DataFrame index.\n        segyio_kwargs (dict, optional): Extra keyword arguments for segyio.open\n        **dim_kwargs: Dimension names and byte location pairs.\n    \"\"\"\n    if segyio_kwargs is None:\n        segyio_kwargs = dict()\n\n    if head_df is None:\n        # Start by scraping the headers.\n        head_df = segy_header_scrape(segyfile, silent=silent, **segyio_kwargs)\n\n    head_bin = segy_bin_scrape(segyfile, **segyio_kwargs)\n\n    # get vertical sample ranges\n    n0 = 0\n    nsamp = head_bin[\"Samples\"]\n    ns0 = head_df.DelayRecordingTime.min()\n\n    # binary header translation\n    nsamp = head_bin[\"Samples\"]\n    sample_rate = head_bin[\"Interval\"] / 1000.0\n    msys = _SEGY_MEASUREMENT_SYSTEM[head_bin[\"MeasurementSystem\"]]\n    vert_samples = np.arange(ns0, ns0 + sample_rate * nsamp, sample_rate, dtype=int)\n\n    # creating dimensions and new dataset\n    dims = dict()\n    dim_index_names = list()\n    dim_fields = list()\n    for dim in dim_kwargs:\n        trace_field = str(segyio.TraceField(dim_kwargs[dim]))\n        if trace_field == \"Unknown Enum\":\n            raise ValueError(f\"{dim}:{dim_kwargs[dim]} was not a valid byte header\")\n        dim_fields.append(trace_field)\n        as_unique = head_df[trace_field].unique()\n        dims[dim] = np.sort(as_unique)\n        d_map = {dval: i for i, dval in enumerate(as_unique)}\n        index_name = f\"{dim}_index\"\n        dim_index_names.append(index_name)\n        head_df.loc[:, index_name] = head_df[trace_field].map(d_map)\n\n    if (\n        head_df[dim_index_names].shape\n        != head_df[dim_index_names].drop_duplicates().shape\n    ):\n        raise ValueError(\n            \"The selected dimensions results in multiple traces per \"\n            \"dimension location, add additional dimensions or use \"\n            \"trace numbering to load as 2D.\"\n        )\n\n    builder, domain = _dataset_coordinate_helper(vert_samples, vert_domain, **dims)\n    ds = create_seismic_dataset(**builder)\n\n    # getting attributes\n    text = get_segy_texthead(segyfile, **segyio_kwargs)\n    ds.attrs[AttrKeyField.text] = text\n    ds.attrs[AttrKeyField.source_file] = pathlib.Path(segyfile).name\n    ds.attrs[AttrKeyField.measurement_system] = msys\n    ds.attrs[AttrKeyField.sample_rate] = sample_rate\n\n    # map extra byte fields into ds\n    if extra_byte_fields is not None:\n        to_add = list()\n        for name, byte in extra_byte_fields.items():\n            trace_field = str(segyio.TraceField(byte))\n            if trace_field == \"Unknown Enum\":\n                raise ValueError(f\"{name}:{byte} was not a valid byte header\")\n            to_add.append(trace_field)\n        to_add = to_add + dim_fields\n        extras = head_df[to_add].set_index(dim_fields).to_xarray()\n        extras = extras.rename_dims(\n            {b: a for a, b in zip(dim_kwargs, dim_fields) if a != b}\n        )\n        for name, xtr in zip(extra_byte_fields, to_add):\n            ds[name] = extras[xtr]\n\n    if return_geometry:\n        # return geometry -&gt; e.g. don't process segy traces\n        return ds\n\n    segyio_kwargs.update(dict(ignore_geometry=True))\n\n    with segyio.open(segyfile, \"r\", **segyio_kwargs) as segyf:\n\n        segyf.mmap()\n        shape = [ds.sizes[d] for d in dim_kwargs] + [vert_samples.size]\n        volume = np.zeros(shape, dtype=np.float32)\n\n        # this can probably be done as a block - leaving for now just incase sorting becomes an issue\n        indexes = tuple([head_df[idx].values for idx in dim_index_names])\n        volume[indexes] = segyf.trace.raw[:][head_df.index.values]\n\n    percentiles = np.percentile(volume, PERCENTILES)\n    ds[VariableKeyField.data] = (\n        list(dim_kwargs) + [VerticalKeyDim[domain]],\n        volume,\n    )\n    ds.attrs[AttrKeyField.percentiles] = list(percentiles)\n\n    return ds\n</code></pre>"},{"location":"api/segy_loading.html#segysak.segy.segy_converter","title":"<code>segy_converter(segyfile, ncfile, cdp=None, iline=None, xline=None, cdp_x=None, cdp_y=None, offset=None, vert_domain='TWT', data_type='AMP', ix_crop=None, cdp_crop=None, xy_crop=None, z_crop=None, return_geometry=False, silent=False, extra_byte_fields=None, **segyio_kwargs)</code>","text":"<p>Convert SEG-Y data to NetCDF4 File</p> <p>The output ncfile has the following structure     Dimensions:         cdp/iline - CDP or Inline axis         xline - Xline axis         twt/depth - The vertical axis         offset - Offset/Angle Axis     Coordinates:         iline - The inline numbering         xline - The xline numbering         cdp_x - Eastings         cdp_y - Northings         cdp - Trace Number for 2d     Variables         data - The data volume     Attributes:         ns - number of samples vertical         sample_rate - sample rate in ms/m         test - text header         measurement_system : m/ft         source_file : segy source         srd : seismic reference datum         percentiles : data amplitude percentiles         coord_scalar : from trace headers</p> <p>Parameters:</p> Name Type Description Default <code>segyfile</code> <code>str</code> <p>Input segy file path</p> required <code>ncfile</code> <code>str</code> <p>Output SEISNC file path. If none the loaded data will be returned in memory as an xarray.Dataset.</p> required <code>cdp</code> <code>int</code> <p>The CDP byte location, usually 21.</p> <code>None</code> <code>iline</code> <code>int</code> <p>Inline byte location, usually 189</p> <code>None</code> <code>xline</code> <code>int</code> <p>Cross-line byte location, usually 193</p> <code>None</code> <code>cdp_x</code> <code>int</code> <p>UTMX byte location, usually 181</p> <code>None</code> <code>cdp_y</code> <code>int</code> <p>UTMY byte location, usually 185</p> <code>None</code> <code>offset</code> <code>int</code> <p>Offset/angle byte location</p> <code>None</code> <code>vert_domain</code> <code>str</code> <p>Vertical sampling domain. One of ['TWT', 'DEPTH']. Defaults to 'TWT'.</p> <code>'TWT'</code> <code>data_type</code> <code>str</code> <p>Data type ['AMP', 'VEL']. Defaults to 'AMP'.</p> <code>'AMP'</code> <code>ix_crop</code> <code>list</code> <p>List of minimum and maximum inline and crossline to output. Has the form '[min_il, max_il, min_xl, max_xl]'. Ignored for 2D data.</p> <code>None</code> <code>cdp_crop</code> <code>list</code> <p>List of minimum and maximum cmp values to output. Has the form '[min_cmp, max_cmp]'. Ignored for 3D data.</p> <code>None</code> <code>xy_crop</code> <code>list</code> <p>List of minimum and maximum cdp_x and cdp_y to output. Has the form '[min_x, max_x, min_y, max_y]'. Ignored for 2D data.</p> <code>None</code> <code>z_crop</code> <code>list</code> <p>List of minimum and maximum vertical samples to output. Has the form '[min, max]'.</p> <code>None</code> <code>return_geometry</code> <code>bool</code> <p>If true returns an xarray.dataset which doesn't contain data but mirrors the input volume header information.</p> <code>False</code> <code>silent</code> <code>bool</code> <p>Disable progress bar.</p> <code>False</code> <code>extra_byte_fields</code> <code>list / mapping</code> <p>A list of int or mapping of byte fields that should be returned as variables in the dataset.a</p> <code>None</code> <code>**segyio_kwargs</code> <p>Extra keyword arguments for segyio.open</p> <code>{}</code> Source code in <code>segysak/segy/_segy_loader.py</code> Python<pre><code>def segy_converter(\n    segyfile,\n    ncfile,\n    cdp=None,\n    iline=None,\n    xline=None,\n    cdp_x=None,\n    cdp_y=None,\n    offset=None,\n    vert_domain=\"TWT\",\n    data_type=\"AMP\",\n    ix_crop=None,\n    cdp_crop=None,\n    xy_crop=None,\n    z_crop=None,\n    return_geometry=False,\n    silent=False,\n    extra_byte_fields=None,\n    **segyio_kwargs,\n):\n    \"\"\"Convert SEG-Y data to NetCDF4 File\n\n    The output ncfile has the following structure\n        Dimensions:\n            cdp/iline - CDP or Inline axis\n            xline - Xline axis\n            twt/depth - The vertical axis\n            offset - Offset/Angle Axis\n        Coordinates:\n            iline - The inline numbering\n            xline - The xline numbering\n            cdp_x - Eastings\n            cdp_y - Northings\n            cdp - Trace Number for 2d\n        Variables\n            data - The data volume\n        Attributes:\n            ns - number of samples vertical\n            sample_rate - sample rate in ms/m\n            test - text header\n            measurement_system : m/ft\n            source_file : segy source\n            srd : seismic reference datum\n            percentiles : data amplitude percentiles\n            coord_scalar : from trace headers\n\n    Args:\n        segyfile (str): Input segy file path\n        ncfile (str): Output SEISNC file path. If none the loaded data will be\n            returned in memory as an xarray.Dataset.\n        cdp (int, optional): The CDP byte location, usually 21.\n        iline (int, optional): Inline byte location, usually 189\n        xline (int, optional): Cross-line byte location, usually 193\n        cdp_x (int, optional): UTMX byte location, usually 181\n        cdp_y (int, optional): UTMY byte location, usually 185\n        offset (int, optional): Offset/angle byte location\n        vert_domain (str, optional): Vertical sampling domain. One of ['TWT', 'DEPTH']. Defaults to 'TWT'.\n        data_type (str, optional): Data type ['AMP', 'VEL']. Defaults to 'AMP'.\n        ix_crop (list, optional): List of minimum and maximum inline and crossline to output.\n            Has the form '[min_il, max_il, min_xl, max_xl]'. Ignored for 2D data.\n        cdp_crop (list, optional): List of minimum and maximum cmp values to output.\n            Has the form '[min_cmp, max_cmp]'. Ignored for 3D data.\n        xy_crop (list, optional): List of minimum and maximum cdp_x and cdp_y to output.\n            Has the form '[min_x, max_x, min_y, max_y]'. Ignored for 2D data.\n        z_crop (list, optional): List of minimum and maximum vertical samples to output.\n            Has the form '[min, max]'.\n        return_geometry (bool, optional): If true returns an xarray.dataset which doesn't contain data but mirrors\n            the input volume header information.\n        silent (bool): Disable progress bar.\n        extra_byte_fields (list/mapping): A list of int or mapping of byte fields that should be returned as variables in the dataset.a\n        **segyio_kwargs: Extra keyword arguments for segyio.open\n\n    \"\"\"\n    # Input sanity checks\n    extra_byte_fields = _loader_converter_checks(cdp, iline, xline, extra_byte_fields)\n    byte_loc = [\n        bytel\n        for bytel in [cdp, iline, xline, cdp_x, cdp_y, offset]\n        if bytel is not None\n    ]\n    check_tracefield(byte_loc)\n\n    head_df, head_bin, head_loc = _loader_converter_header_handling(\n        segyfile,\n        cdp=cdp,\n        iline=iline,\n        xline=xline,\n        cdp_x=cdp_x,\n        cdp_y=cdp_y,\n        offset=offset,\n        vert_domain=vert_domain,\n        data_type=data_type,\n        ix_crop=ix_crop,\n        cdp_crop=cdp_crop,\n        xy_crop=xy_crop,\n        z_crop=z_crop,\n        return_geometry=return_geometry,\n        silent=silent,\n        extra_byte_fields=extra_byte_fields,\n        optimised_load=True,\n        **segyio_kwargs,\n    )\n    print(\"header_loaded\")\n    common_args = (segyfile, head_df, head_bin, head_loc)\n\n    common_kwargs = dict(\n        zcrop=z_crop,\n        ncfile=ncfile,\n        vert_domain=vert_domain,\n        data_type=data_type,\n        return_geometry=return_geometry,\n        silent=silent,\n    )\n\n    # 3d data needs iline and xline\n    if iline is not None and xline is not None:\n\n        print(\"is_3d\")\n        ds = _3dsegy_loader(\n            *common_args,\n            **common_kwargs,\n            **segyio_kwargs,\n        )\n        indexer = [\"il_index\", \"xl_index\"]\n        dims = (\n            DimensionKeyField.threed_head\n            if offset is None\n            else DimensionKeyField.threed_ps_head\n        )\n        is3d2d = True\n\n    # 2d data\n    elif cdp is not None:\n        ds = _2dsegy_loader(*common_args, **common_kwargs, **segyio_kwargs)\n        indexer = [\"cdp_index\"]\n        dims = (\n            DimensionKeyField.twod_head\n            if offset is None\n            else DimensionKeyField.twod_ps_head\n        )\n        is3d2d = True\n\n    # fallbak to just a 2d array of traces\n    else:\n        ds = _2dsegy_loader(*common_args, **common_kwargs, **segyio_kwargs)\n        indexer = []\n        dims = DimensionKeyField.cdp_2d\n        is3d2d = False\n\n    indexer = indexer + [\"off_index\"] if offset is not None else indexer\n\n    ds = _loader_converter_write_headers(\n        ds, head_df, indexer, dims, extra_byte_fields, is3d2d=is3d2d\n    )\n    new_vars = {key: ds[key] for key in extra_byte_fields}\n    ds.close()\n    del ds\n\n    with h5netcdf.File(ncfile, \"a\") as seisnc:\n        for var, darray in new_vars.items():\n            seisnc_var = seisnc.create_variable(\n                var, dimensions=darray.dims, dtype=darray.dtype\n            )\n            seisnc_var[...] = darray[...]\n            seisnc.flush()\n\n    return None\n</code></pre>"},{"location":"api/segy_loading.html#segysak.segy.well_known_byte_locs","title":"<code>well_known_byte_locs(name)</code>","text":"<p>Return common bytes position kwargs_dict for segy_loader and segy_converter.</p> <p>Returns a dict containing the byte locations for well known SEG-Y variants in the wild.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Takes one of keys from KNOWN_BYTES</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of SEG-Y byte positions.</p> <p>Example:</p> <p>Use the output of this function to unpack arguments into <code>segy_loader</code></p> <p>seismic = segy_loader(filepath, **well_known_byte_locs('petrel_3d'))</p> Source code in <code>segysak/segy/_segy_loader.py</code> Python<pre><code>def well_known_byte_locs(name):\n    \"\"\"Return common bytes position kwargs_dict for segy_loader and segy_converter.\n\n    Returns a dict containing the byte locations for well known SEG-Y variants in the wild.\n\n    Args:\n        name (str): Takes one of keys from KNOWN_BYTES\n\n    Returns:\n        dict: A dictionary of SEG-Y byte positions.\n\n    Example:\n\n    Use the output of this function to unpack arguments into ``segy_loader``\n\n    &gt;&gt;&gt; seismic = segy_loader(filepath, **well_known_byte_locs('petrel_3d'))\n\n    \"\"\"\n    try:\n        return KNOWN_BYTES[name]\n    except KeyError:\n        raise ValueError(\n            f\"No byte locatons for {name}, select from {list(KNOWN_BYTES.keys())}\"\n        )\n</code></pre>"},{"location":"api/segy_text.html","title":"SEG-Y text header operations","text":"<p>Use these functions to view or modify existing SEGY text headers or create new segysak compatable text headers.</p>"},{"location":"api/segy_text.html#segysak.segy.get_segy_texthead","title":"<code>get_segy_texthead(segyfile, ext_headers=False, no_richstr=False, **segyio_kwargs)</code>","text":"<p>Return the ebcidc</p> <p>Parameters:</p> Name Type Description Default <code>segyfile</code> <code>str</code> <p>Segy File Path</p> required <code>ext_headers</code> <code>bool</code> <p>Return EBCIDC and extended headers in list. Defaults to False</p> <code>False</code> <code>no_richstr</code> <code>bool</code> <p>Defaults to False. If true the returned string will not be updated for pretty HTML printing.</p> <code>False</code> <code>segyio_kwargs</code> <p>Key word arguments to pass to segyio.open</p> <code>{}</code> <p>Returns:     str: Returns the EBCIDC text as a formatted paragraph.</p> Source code in <code>segysak/segy/_segy_text.py</code> Python<pre><code>def get_segy_texthead(segyfile, ext_headers=False, no_richstr=False, **segyio_kwargs):\n    \"\"\"Return the ebcidc\n\n    Args:\n        segyfile (str): Segy File Path\n        ext_headers (bool): Return EBCIDC and extended headers in list.\n            Defaults to False\n        no_richstr (bool, optional): Defaults to False. If true the returned string\n            will not be updated for pretty HTML printing.\n        segyio_kwargs: Key word arguments to pass to segyio.open\n    Returns:\n        str: Returns the EBCIDC text as a formatted paragraph.\n    \"\"\"\n\n    with open(segyfile, mode=\"rb\") as f:\n        f.seek(0, 0)  # Locate our position to first byte of file\n        data = f.read(3200)  # Read the first 3200 byte from our position\n\n    if _isascii(data) and ext_headers == False:\n        text = data.decode(\"ascii\")  # EBCDIC encoding\n        text = _text_fixes(text)\n        text = segyio.tools.wrap(text)\n    elif ext_headers == False:\n        text = data.decode(\"cp500\")  # text is ebcidc\n        text = _text_fixes(text)\n        text = segyio.tools.wrap(text)\n    else:\n        segyio_kwargs[\"ignore_geometry\"] = True\n        try:  # pray that the encoding is ebcidc\n            with segyio.open(segyfile, \"r\", **segyio_kwargs) as segyf:\n                text = segyf.text[0].decode(\"ascii\", \"replace\")\n                text = _text_fixes(text)\n                text = segyio.tools.wrap(text)\n                if segyf.ext_headers and ext_headers:\n                    text2 = segyf.text[1].decode(\"ascii\", \"replace\")\n                    text = [text, text2]\n        except UnicodeDecodeError as err:\n            print(err)\n            print(\"The segy text header could not be decoded.\")\n\n    if no_richstr:\n        return text\n    else:\n        return _upgrade_txt_richstr(text)\n</code></pre>"},{"location":"api/segy_text.html#segysak.segy.put_segy_texthead","title":"<code>put_segy_texthead(segyfile, ebcidc, line_counter=True, **segyio_kwargs)</code>","text":"<p>Puts a text header (ebcidc) into a segyfile.</p> <p>Parameters:</p> Name Type Description Default <code>segyfile</code> <code>str</code> <p>The path to the file to update.</p> required <code>ebcidc</code> <code>(str, list, dict, bytes)</code> <p>A standard string, new lines will be preserved. A list or lines to add. A dict with numeric keys for line numbers e.g. {1: 'line 1'}. A pre-encoded byte header to add to the segyfile directly.</p> required <code>line_counter</code> <code>(bool, opt)</code> <p>Add a line counter with format \"CXX \" to the start of each line. This reduces the maximum content per line to 76 chars.</p> <code>True</code> Source code in <code>segysak/segy/_segy_text.py</code> Python<pre><code>def put_segy_texthead(segyfile, ebcidc, line_counter=True, **segyio_kwargs):\n    \"\"\"Puts a text header (ebcidc) into a segyfile.\n\n    Args:\n        segyfile (str): The path to the file to update.\n        ebcidc (str, list, dict, bytes):\n           A standard string, new lines will be preserved.\n           A list or lines to add.\n           A dict with numeric keys for line numbers e.g. {1: 'line 1'}.\n           A pre-encoded byte header to add to the segyfile directly.\n        line_counter (bool, opt): Add a line counter with format \"CXX \" to the start of each line.\n            This reduces the maximum content per line to 76 chars.\n    \"\"\"\n    header = \"\"\n    n = 76 if line_counter else 80\n\n    if not isinstance(ebcidc, (str, list, dict, bytes)):\n        raise ValueError(f\"Unknown type for ebcidc: {type(ebcidc)}\")\n\n    if isinstance(ebcidc, dict):\n        lines = _process_dict_texthead(ebcidc, n)\n    elif isinstance(ebcidc, str):\n        lines = _process_string_texthead(ebcidc, n)\n    elif isinstance(ebcidc, list):\n        lines = ebcidc\n    else:\n        lines = []\n\n    if not isinstance(ebcidc, bytes):\n        if line_counter:\n            lines = [f\"C{i+1:2d} {line:s}\" for i, line in enumerate(lines)]\n\n        # check lengths\n        for i, line in enumerate(lines):\n            if len(line) &gt; 80:\n                lines[i] = line[:80]\n                warn(f\"EBCIDC line {line} is too long - truncating\", UserWarning)\n        # convert to bytes\n        header = bytes(\"\".join(lines), \"utf8\")\n    else:\n        header = ebcidc\n\n    # check size\n    if len(header) &gt; 3200:\n        warn(\"Byte EBCIDC is too large - truncating\", UserWarning)\n        header = header[:3200]\n\n    segyio_kwargs[\"ignore_geometry\"] = True\n    with segyio.open(segyfile, \"r+\", **segyio_kwargs) as segyf:\n        segyf.text[0] = header\n</code></pre>"},{"location":"api/segy_text.html#segysak.segy.create_default_texthead","title":"<code>create_default_texthead(override=None)</code>","text":"<p>Returns a simple default textual header dictionary.</p> <p>Basic fields are auto populated and a dictionary indexing lines 1-40 can be passed to override keyword for adjustment. By default lines 6-34 are empty.</p> <p>Parameters:</p> Name Type Description Default <code>override</code> <code>dict</code> <p>Overide any line . Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with keys 1-40 for textual header of segy file</p> Example <p>create_default_texthead(override={7:'Hello', 8:'World!'}) {1: 'segysak SEG-Y Output', 2: 'Data created by: username ', 3: '', 4: 'DATA FORMAT: SEG-Y;  DATE: 2019-06-09 15:14:00', 5: 'DATA DESCRIPTION: SEG-Y format data output from segysak', 6: '', 7: 'Hello', 8: 'World!', 9: '', ... 34: '', 35: '* BYTE LOCATION OF KEY HEADERS *', 36: 'CMP UTM-X 181-184, ALL COORDS X100, CMP UTM-Y 185-188', 37: 'INLINE 189-193, XLINE 194-198, ', 38: '', 39: '', 40: 'END TEXTUAL HEADER'}</p> Source code in <code>segysak/segy/_segy_text.py</code> Python<pre><code>def create_default_texthead(override=None):\n    \"\"\"Returns a simple default textual header dictionary.\n\n    Basic fields are auto populated and a dictionary indexing lines 1-40 can\n    be passed to override keyword for adjustment. By default lines 6-34 are\n    empty.\n\n    Args:\n        override (dict, optional): Overide any line . Defaults to None.\n\n    Returns:\n        (dict): Dictionary with keys 1-40 for textual header of segy file\n\n    Example:\n        &gt;&gt;&gt; create_default_texthead(override={7:'Hello', 8:'World!'})\n        {1: 'segysak SEG-Y Output',\n        2: 'Data created by: username ',\n        3: '',\n        4: 'DATA FORMAT: SEG-Y;  DATE: 2019-06-09 15:14:00',\n        5: 'DATA DESCRIPTION: SEG-Y format data output from segysak',\n        6: '',\n        7: 'Hello',\n        8: 'World!',\n        9: '',\n        ...\n        34: '',\n        35: '*** BYTE LOCATION OF KEY HEADERS ***',\n        36: 'CMP UTM-X 181-184, ALL COORDS X100, CMP UTM-Y 185-188',\n        37: 'INLINE 189-193, XLINE 194-198, ',\n        38: '',\n        39: '',\n        40: 'END TEXTUAL HEADER'}\n    \"\"\"\n    user = _get_userid()\n    today, time = _get_datetime()\n    text_dict = {\n        #      123456789012345678901234567890123456789012345678901234567890123456\n        1: \"segysak Python Library SEG-Y Output\",\n        2: f\"Data created by: {user} \",\n        4: f\"DATA FORMAT: SEG-Y;  DATE: {today} {time}\",\n        5: \"DATA DESCRIPTION: SEG-Y format data output from segysak using segyio\",\n        6: \"\",\n        40: \"END TEXTUAL HEADER\",\n    }\n    if override is not None:\n        for key, line in override.items():\n            text_dict[key] = line\n    return _clean_texthead(text_dict)\n</code></pre>"},{"location":"api/segy_writing.html","title":"Writing SEG-Y","text":""},{"location":"api/segy_writing.html#segysak.segy.segy_writer","title":"<code>segy_writer(seisnc, segyfile, trace_header_map=None, il_chunks=None, dimension=None, silent=False, use_text=False)</code>","text":"<p>Convert siesnc format (NetCDF4) to SEGY.</p> <p>Parameters:</p> Name Type Description Default <code>seisnc</code> <code>(Dataset, string)</code> <p>The input SEISNC file either a path or the in memory xarray.Dataset</p> required <code>segyfile</code> <code>string</code> <p>The output SEG-Y file</p> required <code>trace_header_map</code> <code>dict</code> <p>Defaults to None. A dictionary of seisnc variables and byte locations. The variable will be written to the trace headers in the assigned byte location. By default CMP=23, cdp_x=181, cdp_y=185, iline=189, xline=193.</p> <code>None</code> <code>il_chunks</code> <code>int</code> <p>The size of data to work on - if you have memory limitations. Defaults to 10. This is primarily used for large 3D and ignored for 2D data.</p> <code>None</code> <code>dimension</code> <code>str</code> <p>Data dimension to output, defaults to 'twt' or 'depth' whichever is present</p> <code>None</code> <code>silent</code> <code>bool</code> <p>Turn off progress reporting. Defaults to False.</p> <code>False</code> <code>use_text</code> <code>bool</code> <p>Use the seisnc text for the EBCIDC output. This text usally comes from the loaded SEG-Y file and may not match the segysak SEG-Y output. Defaults to False and writes the default segysak EBCIDC</p> <code>False</code> Source code in <code>segysak/segy/_segy_writer.py</code> Python<pre><code>def segy_writer(\n    seisnc,\n    segyfile,\n    trace_header_map=None,\n    il_chunks=None,\n    dimension=None,\n    silent=False,\n    use_text=False,\n):\n    \"\"\"Convert siesnc format (NetCDF4) to SEGY.\n\n    Args:\n        seisnc (xarray.Dataset, string): The input SEISNC file either a path or the in memory xarray.Dataset\n        segyfile (string): The output SEG-Y file\n        trace_header_map (dict, optional): Defaults to None. A dictionary of seisnc variables\n            and byte locations. The variable will be written to the trace headers in the\n            assigned byte location. By default CMP=23, cdp_x=181, cdp_y=185, iline=189,\n            xline=193.\n        il_chunks (int, optional): The size of data to work on - if you have memory\n            limitations. Defaults to 10. This is primarily used for large 3D and ignored for 2D data.\n        dimension (str): Data dimension to output, defaults to 'twt' or 'depth' whichever is present\n        silent (bool, optional): Turn off progress reporting. Defaults to False.\n        use_text (bool, optional): Use the seisnc text for the EBCIDC output. This text usally comes from\n            the loaded SEG-Y file and may not match the segysak SEG-Y output. Defaults to False and writes\n            the default segysak EBCIDC\n    \"\"\"\n    if trace_header_map:\n        check_tracefield(trace_header_map.values())\n\n    if isinstance(seisnc, xr.Dataset):\n        _segy_writer_input_handler(\n            seisnc, segyfile, trace_header_map, dimension, silent, None, use_text\n        )\n    else:\n        ncfile = seisnc\n        if isinstance(il_chunks, int):\n            chunking = {\"iline\": il_chunks}\n        else:\n            chunking = None\n        with open_seisnc(ncfile, chunks=chunking) as seisnc:\n            _segy_writer_input_handler(\n                seisnc,\n                segyfile,\n                trace_header_map,\n                dimension,\n                silent,\n                il_chunks,\n                use_text,\n            )\n</code></pre>"},{"location":"api/segy_writing.html#segysak.segy.segy_freewriter","title":"<code>segy_freewriter(seisnc, segyfile, data_array='data', trace_header_map=None, use_text=False, dead_trace_key=None, vert_dimension='twt', chunk_spec=None, silent=False, **dim_kwargs)</code>","text":"<p>Convert siesnc format (NetCDF4) to SEG-Y.</p> <p>Parameters:</p> Name Type Description Default <code>seisnc</code> <code>(Dataset, string)</code> <p>The input SEISNC file either a path or the in memory xarray.Dataset</p> required <code>segyfile</code> <code>string</code> <p>The output SEG-Y file</p> required <code>data_array</code> <code>string</code> <p>The Dataset variable name for the output volume.</p> <code>'data'</code> <code>trace_header_map</code> <code>dict</code> <p>Defaults to None. A dictionary of seisnc variables and byte locations. The variable will be written to the trace headers in the assigned byte location. By default CMP=23, cdp_x=181, cdp_y=185, iline=189, xline=193.</p> <code>None</code> <code>use_text</code> <code>book</code> <p>Use the seisnc text for the EBCIDC output. This text usally comes from the loaded SEG-Y file and may not match the segysak SEG-Y output. Defaults to False and writes the default segysak EBCIDC</p> <code>False</code> <code>dead_trace_key</code> <code>str</code> <p>The key for the Dataset variable to use for trace output filter.</p> <code>None</code> <code>vert_dimension</code> <code>str</code> <p>Data dimension to output, defaults to 'twt' or 'depth' whichever is present.</p> <code>'twt'</code> <code>chunk_spec</code> <code>dict</code> <p>Xarray open_dataset chunking spec.</p> <code>None</code> <code>silent</code> <code>bool</code> <p>Turn off progress reporting. Defaults to False.</p> <code>False</code> <code>dim_kwargs</code> <code>int</code> <p>The dimension/byte location pairs to output dimensions to. The number of dim_kwargs should be equal to the number of dimensions on the output data_array. The sort order will be as per the order passed to the function.</p> <code>{}</code> Source code in <code>segysak/segy/_segy_freewriter.py</code> Python<pre><code>def segy_freewriter(\n    seisnc,\n    segyfile,\n    data_array=\"data\",\n    trace_header_map=None,\n    use_text=False,\n    dead_trace_key=None,\n    vert_dimension=\"twt\",\n    chunk_spec=None,\n    silent=False,\n    **dim_kwargs,\n):\n    \"\"\"Convert siesnc format (NetCDF4) to SEG-Y.\n\n    Args:\n        seisnc (xarray.Dataset, string): The input SEISNC file either a path or the in memory xarray.Dataset\n        segyfile (string): The output SEG-Y file\n        data_array (string): The Dataset variable name for the output volume.\n        trace_header_map (dict, optional): Defaults to None. A dictionary of seisnc variables\n            and byte locations. The variable will be written to the trace headers in the\n            assigned byte location. By default CMP=23, cdp_x=181, cdp_y=185, iline=189,\n            xline=193.\n        use_text (book, optional): Use the seisnc text for the EBCIDC output. This text usally comes from\n            the loaded SEG-Y file and may not match the segysak SEG-Y output. Defaults to False and writes\n            the default segysak EBCIDC\n        dead_trace_key (str): The key for the Dataset variable to use for trace output filter.\n        vert_dimension (str): Data dimension to output, defaults to 'twt' or 'depth' whichever\n            is present.\n        chunk_spec (dict, optional): Xarray open_dataset chunking spec.\n        silent (bool, optional): Turn off progress reporting. Defaults to False.\n        dim_kwargs (int): The dimension/byte location pairs to output dimensions to. The number of dim_kwargs should be\n            equal to the number of dimensions on the output data_array. The sort order will be as per the order passed\n            to the function.\n    \"\"\"\n    kwargs = dict(\n        data_array=data_array,\n        trace_header_map=trace_header_map,\n        silent=silent,\n        use_text=use_text,\n        dead_trace_key=dead_trace_key,\n        vert_dimension=vert_dimension,\n    )\n\n    if isinstance(seisnc, xr.Dataset):\n        _segy_freewriter(\n            seisnc,\n            segyfile,\n            **kwargs,\n            **dim_kwargs,\n        )\n    else:\n        ncfile = seisnc\n        with open_seisnc(ncfile, chunks=chunk_spec) as seisnc:\n            _segy_freewriter(\n                seisnc,\n                segyfile,\n                **kwargs,\n                **dim_kwargs,\n            )\n</code></pre>"},{"location":"api/segy_writing.html#segysak.segy.output_byte_locs","title":"<code>output_byte_locs(name)</code>","text":"<p>Return common bytes position variable_dict for segy_writer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>One of [standard_3d, petrel_3d]</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of SEG-Y byte positions and default seisnc variable pairs.</p> Example <p>segywriter(ncfile, 'segyfile.sgy', trace_header_map=output_byte_loc('petrel')) </p> Source code in <code>segysak/segy/_segy_writer.py</code> Python<pre><code>def output_byte_locs(name):\n    \"\"\"Return common bytes position variable_dict for segy_writer.\n\n    Args:\n        name (str): One of [standard_3d, petrel_3d]\n\n    Returns:\n        dict: A dictionary of SEG-Y byte positions and default seisnc variable\n            pairs.\n\n    Example:\n        &gt;&gt;&gt; segywriter(ncfile, 'segyfile.sgy', trace_header_map=output_byte_loc('petrel'))\n        &gt;&gt;&gt;\n    \"\"\"\n    try:\n        return OUTPUT_BYTES[name]\n    except KeyError:\n        raise ValueError(\n            f\"No byte locatons for {name}, select from {list(OUTPUT_BYTES.keys())}\"\n        )\n</code></pre>"},{"location":"api/xarray_accessor.html","title":"Xarray Accessor modules","text":"<p>Accessor modules are accessed as namespaces within the Xarray.Dataset objects created by SEGY-SAK. When <code>segysak</code> is imported, all <code>xarray.Dataset</code> objects will contain the <code>.seis</code> and <code>.seisio</code> namespaces.</p>"},{"location":"api/xarray_accessor.html#segysak-xarray-accessor-modules","title":"segysak Xarray accessor modules","text":""},{"location":"api/xarray_accessor.html#segysak.SeisIO","title":"<code>SeisIO</code>","text":"Source code in <code>segysak/_accessor.py</code> Python<pre><code>@xr.register_dataset_accessor(\"seisio\")\nclass SeisIO:\n    def __init__(self, xarray_obj):\n        self._obj = xarray_obj\n\n    def to_netcdf(self, seisnc, **kwargs):\n        \"\"\"Output to netcdf4 with specs for seisnc.\n\n        Args:\n            seisnc (string/path-like): The output file path. Preferably with .seisnc extension.\n            **kwargs: As per xarray function to_netcdf.\n        \"\"\"\n        # First remove all None attr.\n        remove_keys = list()\n        for key, val in self._obj.attrs.items():\n            if val is None:\n                remove_keys.append(key)\n        for key in remove_keys:\n            _ = self._obj.attrs.pop(key)\n\n        # Convert richstring back to normal string\n        try:\n            self._obj.attrs[\"text\"] = str(self._obj.attrs[\"text\"])\n        except:\n            self._obj.attrs[\"text\"] = \"\"\n\n        kwargs[\"engine\"] = \"h5netcdf\"\n\n        self._obj.to_netcdf(seisnc, **kwargs)\n\n    def to_subsurface(self):\n        \"\"\"Convert seismic data to a subsurface StructuredData Object\n\n        Raises:\n            err: [description]\n            NotImplementedError: [description]\n\n        Returns:\n            subsurface.structs.base_structures.StructuredData: subsurface struct\n        \"\"\"\n        try:\n            from subsurface.structs.base_structures import StructuredData\n        except ImportError as err:\n            print(\"subsurface optional dependency required\")\n            raise err\n\n        if self._obj.seis.is_twt():\n            vdom = \"twt\"\n        else:\n            vdom = \"depth\"\n\n        if self._obj.seis.is_3d():\n            ds = self._obj.rename_dims({\"iline\": \"x\", \"xline\": \"y\", vdom: \"z\"})\n            ds[\"z\"] = ds[\"z\"] * -1\n            subsurface_struct = StructuredData(ds, \"data\")\n        elif self._obj.seis.is_2d():\n            subsurface_struct = StructuredData(self._obj, \"data\")\n        else:\n            raise NotImplementedError\n\n        return subsurface_struct\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisIO.to_netcdf","title":"<code>to_netcdf(seisnc, **kwargs)</code>","text":"<p>Output to netcdf4 with specs for seisnc.</p> <p>Parameters:</p> Name Type Description Default <code>seisnc</code> <code>string / path - like</code> <p>The output file path. Preferably with .seisnc extension.</p> required <code>**kwargs</code> <p>As per xarray function to_netcdf.</p> <code>{}</code> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def to_netcdf(self, seisnc, **kwargs):\n    \"\"\"Output to netcdf4 with specs for seisnc.\n\n    Args:\n        seisnc (string/path-like): The output file path. Preferably with .seisnc extension.\n        **kwargs: As per xarray function to_netcdf.\n    \"\"\"\n    # First remove all None attr.\n    remove_keys = list()\n    for key, val in self._obj.attrs.items():\n        if val is None:\n            remove_keys.append(key)\n    for key in remove_keys:\n        _ = self._obj.attrs.pop(key)\n\n    # Convert richstring back to normal string\n    try:\n        self._obj.attrs[\"text\"] = str(self._obj.attrs[\"text\"])\n    except:\n        self._obj.attrs[\"text\"] = \"\"\n\n    kwargs[\"engine\"] = \"h5netcdf\"\n\n    self._obj.to_netcdf(seisnc, **kwargs)\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisIO.to_subsurface","title":"<code>to_subsurface()</code>","text":"<p>Convert seismic data to a subsurface StructuredData Object</p> <p>Raises:</p> Type Description <code>err</code> <p>[description]</p> <code>NotImplementedError</code> <p>[description]</p> <p>Returns:</p> Type Description <p>subsurface.structs.base_structures.StructuredData: subsurface struct</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def to_subsurface(self):\n    \"\"\"Convert seismic data to a subsurface StructuredData Object\n\n    Raises:\n        err: [description]\n        NotImplementedError: [description]\n\n    Returns:\n        subsurface.structs.base_structures.StructuredData: subsurface struct\n    \"\"\"\n    try:\n        from subsurface.structs.base_structures import StructuredData\n    except ImportError as err:\n        print(\"subsurface optional dependency required\")\n        raise err\n\n    if self._obj.seis.is_twt():\n        vdom = \"twt\"\n    else:\n        vdom = \"depth\"\n\n    if self._obj.seis.is_3d():\n        ds = self._obj.rename_dims({\"iline\": \"x\", \"xline\": \"y\", vdom: \"z\"})\n        ds[\"z\"] = ds[\"z\"] * -1\n        subsurface_struct = StructuredData(ds, \"data\")\n    elif self._obj.seis.is_2d():\n        subsurface_struct = StructuredData(self._obj, \"data\")\n    else:\n        raise NotImplementedError\n\n    return subsurface_struct\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom","title":"<code>SeisGeom</code>","text":"Source code in <code>segysak/_accessor.py</code> Python<pre><code>@xr.register_dataset_accessor(\"seis\")\nclass SeisGeom:\n    def __init__(self, xarray_obj):\n        self._obj = xarray_obj\n\n    @property\n    def humanbytes(self):\n        \"\"\"Prints Human Friendly size of Dataset to return the bytes as an\n        int use ``xarray.Dataset.nbytes``\n\n        Returns:\n            str: Human readable size of dataset.\n\n        \"\"\"\n        nbytes = self._obj.nbytes\n        suffixes = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n        i = 0\n        while nbytes &gt;= 1024 and i &lt; len(suffixes) - 1:\n            nbytes /= 1024.0\n            i += 1\n        f = f\"{nbytes:.2f}\".rstrip(\"0\").rstrip(\".\")\n        return \"{} {}\".format(f, suffixes[i])\n\n    def _coord_as_dimension(self, points, drop):\n        \"\"\"Convert x and y points to iline and xline. If the affine transform\n        cannot be found this function will use a gridding interpolation approach.\n\n        Args:\n            cdp_x (float/array-like)\n            cdp_y (float/array-like)\n            method (str): Same a methods for xarray.Dataset.interp\n\n        Returns:\n            xarray.Dataset: At selected coordinates.\n        \"\"\"\n        affine = None\n        keys = (\"cdp_x\", \"cdp_y\")\n\n        try:\n            affine = self.get_affine_transform().inverted()\n        except:\n            pass\n\n        if affine is not None:\n            ilxl = affine.transform(np.dstack(points)[0])\n            ils, xls = ilxl[:, 0], ilxl[:, 1]\n\n        else:\n            # check drop keys actually on dims, might not be\n            drop = set(drop).intersection(*[set(self._obj[key].sizes) for key in keys])\n\n            grid = np.vstack(\n                [\n                    self._obj[key]\n                    .mean(dim=drop, skipna=True)\n                    .transpose(\"iline\", \"xline\", transpose_coords=True)\n                    .values.ravel()\n                    for key in keys\n                ]\n            ).transpose()\n            xlines_, ilines_ = np.meshgrid(\n                self._obj[\"xline\"],\n                self._obj[\"iline\"],\n            )\n\n            # these must all be the same length\n            ils = np.atleast_1d(griddata(grid, ilines_.ravel(), points))\n            xls = np.atleast_1d(griddata(grid, xlines_.ravel(), points))\n\n        return ils, xls\n\n    def xysel(self, cdp_x, cdp_y, method=\"nearest\", sample_dim_name=\"cdp\"):\n        \"\"\"Select data at x and y coordinates\n\n        Args:\n            cdp_x (float/array-like)\n            cdp_y (float/array-like)\n            method (str): Same as methods for xarray.Dataset.interp\n            sample_dim_name (str, optional): The name to give the output sampling dimension.\n\n        Returns:\n            xarray.Dataset: At selected coordinates.\n        \"\"\"\n\n        cdp_x = np.atleast_1d(cdp_x)\n        cdp_y = np.atleast_1d(cdp_y)\n\n        if self.is_twt():\n            core_dims = DimensionKeyField.threed_twt\n        elif self.is_depth():\n            core_dims = DimensionKeyField.threed_depth\n        else:\n            raise AttributeError(\"Dataset required twt or depth coordinates.\")\n\n        dims = set(self._obj.sizes.keys())\n\n        if self.is_3d() or self.is_3dgath():\n            # get other dims\n            other_dims = dims.difference(core_dims)\n            il, xl = self._coord_as_dimension((cdp_x, cdp_y), other_dims)\n\n            sampling_arrays = dict(\n                iline=xr.DataArray(\n                    il, dims=sample_dim_name, coords={sample_dim_name: range(il.size)}\n                ),\n                xline=xr.DataArray(\n                    xl, dims=sample_dim_name, coords={sample_dim_name: range(xl.size)}\n                ),\n            )\n            output_ds = self._obj.interp(**sampling_arrays, method=method)\n\n            # # populate back to 2d\n            # cdp_ds = [\n            #     None\n            #     if np.isnan(np.sum([i, x]))\n            #     else self._obj.interp(iline=i, xline=x, method=method)\n            #     for i, x in zip(il, xl)\n            # ]\n\n            # if all(map(lambda x: x is None, cdp_ds)):\n            #     raise ValueError(\"No points intersected the dataset\")\n\n            # for ds in cdp_ds:\n            #     if ds is not None:\n            #         none_replace = ds.copy(deep=True)\n            #         break\n\n            # none_replace[VariableKeyField.data][:] = np.nan\n            # none_replace[CoordKeyField.iline] = np.nan\n            # none_replace[CoordKeyField.xline] = np.nan\n\n            # for i, (point, xloc, yloc) in enumerate(zip(cdp_ds, cdp_x, cdp_y)):\n            #     if point is None:\n            #         point = none_replace\n            #     point[CoordKeyField.cdp_x] = point[CoordKeyField.cdp_x] * 0 + xloc\n            #     point[CoordKeyField.cdp_y] = point[CoordKeyField.cdp_y] * 0 + yloc\n            #     point[CoordKeyField.cdp] = i + 1\n            #     point.attrs = dict()\n            #     cdp_ds[i] = point.copy()\n\n        elif self.is_2d() or self.is_2dgath():\n            raise NotImplementedError(\"Not yet implemented for 2D\")\n        else:\n            raise AttributeError(\n                \"xysel not support for this volume, must be 3d, 3dgath, 2d or 2dgath\"\n            )\n\n        # cdp_ds = xr.concat(cdp_ds, \"cdp\")\n        # cdp_ds.attrs = self._obj.attrs.copy()\n\n        # return cdp_ds\n        output_ds.attrs = self._obj.attrs.copy()\n        return output_ds\n\n    def _has_dims(self, dimension_options, invalid_dimension_options=None):\n        \"\"\"Check dataset has one of dimension options.\"\"\"\n        current_dims = set(self._obj.sizes)\n\n        # check for a match\n        result = False\n        for opt in dimension_options:\n            if set(opt).issubset(current_dims):\n                result = True\n                break\n\n        # check for things that would invalidate this e.g. this might be a subset\n        if invalid_dimension_options is not None:\n            for opt in invalid_dimension_options:\n                if set(opt).issubset(current_dims):\n                    result = False\n                    break\n\n        return result\n\n    def _is_known_geometry(self):\n        \"\"\"Decide if known geometry this volume is and return the appropriate enum.\"\"\"\n        current_dims = set(self._obj.sizes)\n        for key in DimensionKeyField:\n            if set(key).issubset(current_dims):\n                return True\n        return False\n\n    def is_2d(self):\n        \"\"\"Returns True if the dataset is 2D peformant else False\"\"\"\n        dim_options = [\n            DimensionKeyField.twod_twt,\n            DimensionKeyField.twod_depth,\n        ]\n        invalid_dim_options = [\n            DimensionKeyField.twod_ps_twt,\n            DimensionKeyField.twod_ps_depth,\n        ]\n        return self._has_dims(dim_options, invalid_dim_options)\n\n    def is_3d(self):\n        \"\"\"Returns True if the dataset is 3D peformant else False\"\"\"\n        dim_options = [\n            DimensionKeyField.threed_twt,\n            DimensionKeyField.threed_depth,\n            DimensionKeyField.threed_xline_twt,\n            DimensionKeyField.threed_iline_twt,\n            DimensionKeyField.threed_xline_depth,\n            DimensionKeyField.threed_iline_depth,\n        ]\n        invalid_dim_options = [\n            DimensionKeyField.threed_ps_twt,\n            DimensionKeyField.threed_ps_depth,\n        ]\n        return self._has_dims(dim_options, invalid_dim_options)\n\n    def is_2dgath(self):\n        \"\"\"Returns True if the dataset is 2D peformant and has offset or angle else False\"\"\"\n        dim_options = [\n            DimensionKeyField.twod_ps_twt,\n            DimensionKeyField.twod_ps_depth,\n        ]\n        return self._has_dims(dim_options)\n\n    def is_3dgath(self):\n        \"\"\"Returns True if the dataset is 3D peformant and has offset or angle else False\"\"\"\n        dim_options = [\n            DimensionKeyField.threed_ps_twt,\n            DimensionKeyField.threed_ps_depth,\n        ]\n        return self._has_dims(dim_options)\n\n    def _check_multi_z(self):\n        if (\n            CoordKeyField.depth in self._obj.sizes\n            and CoordKeyField.twt in self._obj.sizes\n        ):\n            raise ValueError(\n                \"seisnc cannot determine domain both twt and depth dimensions found\"\n            )\n\n    def is_twt(self):\n        \"\"\"Check if seisnc volume is in twt\"\"\"\n        self._check_multi_z()\n        return True if CoordKeyField.twt in self._obj.sizes else False\n\n    def is_depth(self):\n        \"\"\"Check if seisnc volume is in depth\"\"\"\n        self._check_multi_z()\n        return True if CoordKeyField.depth in self._obj.sizes else False\n\n    def is_empty(self):\n        \"\"\"Check if empty\"\"\"\n        if VariableKeyField.data in self._obj.variables:\n            # This could be smartened up to check logic make sure dimensions of data\n            # are correct.\n            return False\n        else:\n            return True\n\n    def get_measurement_system(self):\n        \"\"\"Return measurement_system if present, else None\"\"\"\n        if hasattr(self._obj, \"measurement_system\"):\n            return self._obj.measurement_system\n        else:\n            return None\n\n    def zeros_like(self):\n        \"\"\"Create a new dataset with the same attributes and coordinates and\n        dimensions but with data filled by zeros.\n        \"\"\"\n        current_dims = {dim: val for dim, val in self._obj.sizes.items()}\n        dims = [dim for dim in current_dims.keys()]\n        shp = [val for val in current_dims.values()]\n        # TODO: Investigate copy options, might need to manually copy geometry\n        # also don't want to load into memory if large large file.\n        out = self._obj.copy()\n        out[VariableKeyField.data] = (dims, np.zeros(shp))\n        return out\n\n    def surface_from_points(\n        self,\n        points,\n        attr,\n        left=(\"cdp_x\", \"cdp_y\"),\n        right=None,\n        key=None,\n        method=\"linear\",\n    ):\n        \"\"\"Sample a 2D point set with an attribute (like Z) to the seisnc\n        geometry using interpolation.\n\n        Args:\n            points (array-like): Nx2 array-like of points with coordinates\n                corresponding to coord1 and coord2.\n            attr (array-like, str): If str points should be a DataFrame where\n                attr is the column name of the attribute to be interpolated to\n                the seisnc geometry. Else attr is a 1D array of length N.\n            left (tuple): Length 2 tuple of coordinate dimensions to interpolate to.\n            right (tuple, optional): If points is DataFrame right is a length 2\n                tuple of column keys corresponding to coordinates in argument left.\n            method (str): The interpolation method to use for griddata from scipy.\n                Defaults to 'linear'\n\n        Returns:\n            xr.Dataset: Surface with geometry specified in left.\n\n        \"\"\"\n\n        if len(left) != 2 and not isinstance(left, Iterable):\n            raise ValueError(\"left must be tuple of length 2\")\n        for var in left:\n            if var not in self._obj.variables:\n                raise ValueError(\"left coordinate names not found in dataset\")\n        for var in left:\n            if var in self._obj.sizes:\n                raise ValueError(\n                    \"surface_from_points is designed for coordinates that \"\n                    \"are not already Dataset Dimensions, use ... instead.\"\n                )\n\n        a, b = left\n        if self._obj[a].sizes != self._obj[b].sizes:\n            raise ValueError(\n                f\"left keys {a} and {b} should exist on the same dimensions\"\n            )\n\n        if isinstance(points, pd.DataFrame):\n            if len(right) != 2 and not isinstance(right, Iterable):\n                raise ValueError(\n                    \"points is DataFrame, right must a tuple specified \"\n                    \"with coordinate keys from column names, e.g right=('cdp_x', 'cdp_y')\"\n                )\n            names = [attr] + list(right)\n            if not set(names).issubset(points.columns):\n                raise ValueError(\"could not find specified keys in points DataFrame\")\n\n            _points = points[list(right)].values\n            _attr = points[attr].values\n\n        else:\n            points = np.asarray(points)\n            attr = np.asarray(attr)\n            if points.shape[1] != 2:\n                raise ValueError(\"points must have shape Nx2\")\n            if attr.size != points.shape[0]:\n                raise ValueError(\"First dimension of points must equal length of attr\")\n            _points = points\n            _attr = attr\n\n        temp_ds = self._obj.copy()\n        org_dims = temp_ds[a].sizes\n        org_shp = temp_ds[a].shape\n\n        var = griddata(\n            _points,\n            _attr,\n            np.dstack(\n                [\n                    temp_ds[a]\n                    .transpose(*org_dims, transpose_coords=True)\n                    .values.ravel(),\n                    temp_ds[b]\n                    .transpose(*org_dims, transpose_coords=True)\n                    .values.ravel(),\n                ]\n            ),\n            method=method,\n        )\n\n        out = temp_ds.drop_vars(temp_ds.var().keys())\n        temp_ds.attrs.clear()\n        if key is None and isinstance(attr, str):\n            key = attr\n        elif key is None:\n            key = \"data\"\n        out[key] = (org_dims, var.reshape(org_shp))\n        return out\n\n    # DRAFT\n    # def get_survey_extents(seismic_3d):\n    #     return SimpleNamespace(\n    #         iline=(seismic_3d.coords[\"iline\"].min().item(), seismic_3d.coords[\"iline\"].max().item()),\n    #         xline=(seismic_3d.coords[\"xline\"].min().item(), seismic_3d.coords[\"xline\"].max().item())\n    #     )\n\n    def calc_corner_points(self):\n        \"\"\"Calculate the corner points of the geometry or end points of a 2D line.\n\n        This puts two properties in the seisnc attrs with the calculated il/xl and\n        cdp_x and cdp_y if available.\n\n        Attr:\n            ds.attrs['corner_points']\n            ds.attrs['corner_points_xy']\n        \"\"\"\n        corner_points = False\n        corner_points_xy = False\n\n        if self.is_3d() or self.is_3dgath():\n            il, xl = DimensionKeyField.cdp_3d\n            ilines = self._obj[il].values\n            xlines = self._obj[xl].values\n            corner_points = (\n                (ilines[0], xlines[0]),\n                (ilines[0], xlines[-1]),\n                (ilines[-1], xlines[-1]),\n                (ilines[-1], xlines[0]),\n            )\n\n            cdp_x, cdp_y = CoordKeyField.cdp_x, CoordKeyField.cdp_y\n\n            if set((cdp_x, cdp_y)).issubset(self._obj.variables):\n                xs = self._obj[cdp_x].values\n                ys = self._obj[cdp_y].values\n                corner_points_xy = (\n                    (xs[0, 0], ys[0, 0]),\n                    (xs[0, -1], ys[0, -1]),\n                    (xs[-1, -1], ys[-1, -1]),\n                    (xs[-1, 0], ys[-1, 0]),\n                )\n\n        elif self.is_2d() or self.is_2dgath():\n            cdp = DimensionKeyField.cdp_2d[0]\n            cdps = self._obj[cdp].values\n            corner_points = (cdps[0], cdps[-1])\n\n            cdp_x, cdp_y = CoordKeyField.cdp_x, CoordKeyField.cdp_y\n\n            if set((cdp_x, cdp_y)).issubset(self._obj.variables):\n                xs = self._obj[cdp_x].values\n                ys = self._obj[cdp_y].values\n                corner_points_xy = (\n                    (xs[0], ys[0]),\n                    (xs[0], ys[0]),\n                    (xs[-1], ys[1]),\n                    (xs[-1], ys[1]),\n                )\n\n        if corner_points:\n            self._obj.attrs[AttrKeyField.corner_points] = corner_points\n        if corner_points_xy:\n            self._obj.attrs[AttrKeyField.corner_points_xy] = corner_points_xy\n\n    def interp_line(\n        self,\n        cdp_x,\n        cdp_y,\n        extra=None,\n        bin_spacing_hint=10,\n        line_method=\"slinear\",\n        xysel_method=\"linear\",\n    ):\n        \"\"\"Select data at x and y coordinates\n\n        Args:\n            cdp_x (float/array-like)\n            cdp_y (float/array-like)\n            bin_spacing_hint (number): a bin spacing to stay close to, in cdp world units. Default: 10\n            line_method (string): valid values for the *kind* argument in scipy.interpolate.interp1d\n            xysel_method (string): valid values for DataArray.interp\n\n        Returns:\n            xarray.Dataset: Interpolated traces along the arbitrary line\n        \"\"\"\n        extra = dict() if extra is None else extra\n\n        cdp_x_i, cdp_y_i, _, extra_i = get_uniform_spacing(\n            cdp_x,\n            cdp_y,\n            bin_spacing_hint=bin_spacing_hint,\n            extra=extra,\n            method=line_method,\n        )\n        ds = self._obj.seis.xysel(cdp_x_i, cdp_y_i, method=xysel_method)\n\n        for key in extra:\n            ds[key] = ((\"cdp\"), extra_i[key])\n\n        return ds\n\n    def plot_bounds(self, ax=None):\n        \"\"\"Plot survey bbounding box to a new or existing axis\n\n        Args:\n            ax: (optional) axis to plot to\n\n        Returns:\n            matplotlib axis used\n\n        \"\"\"\n        xy = self._obj.attrs[\"corner_points_xy\"]\n\n        if xy is None:\n            self._obj.seis.calc_corner_points()\n            xy = self._obj.attrs[\"corner_points_xy\"]\n\n        if not ax:\n            fig, ax = plt.subplots(1, 1)\n\n        x = [x for x, _ in xy]\n        y = [y for _, y in xy]\n        ax.scatter(x, y)\n        ax.plot(x + [x[0]], y + [y[0]], \"k:\")\n\n        ax.set_xlabel(\"cdp x\")\n        ax.set_ylabel(\"cdp y\")\n\n        return ax\n\n    def subsample_dims(self, **dim_kwargs):\n        \"\"\"Return a dictionary of subsampled dims suitable for xarray.interp.\n\n        This tool halves\n\n        Args:\n            dim_kwargs: dimension names as keyword arguments with values of how\n                many times we should divide the dimension by 2.\n        \"\"\"\n        output = dict()\n        for dim in dim_kwargs:\n            ar = self._obj[dim].values\n            while dim_kwargs[dim] &gt; 0:  # only for 3.8\n                ar = halfsample(ar)\n                dim_kwargs[dim] = dim_kwargs[dim] - 1\n            output[dim] = ar\n        return output\n\n    def fill_cdpna(self):\n        \"\"\"Fills NaN cdp locations by fitting known cdp x and y values to the\n        local grid using a planar surface relationshipt.\n        \"\"\"\n        coord_df = coordinate_df(self._obj)\n        self._obj[\"cdp_x\"] = (\n            (\"iline\", \"xline\"),\n            coord_df.cdp_x.values.reshape(self._obj.cdp_x.shape),\n        )\n        self._obj[\"cdp_y\"] = (\n            (\"iline\", \"xline\"),\n            coord_df.cdp_y.values.reshape(self._obj.cdp_y.shape),\n        )\n\n    def grid_rotation(self):\n        \"\"\"Calculate the rotation of the grid using the sum under the curve method.\n\n        (x2 \u2212 x1)(y2 + y1)\n\n        Returns a value &gt;0 if the rotation of points along inline is clockwise.\n        \"\"\"\n        self.calc_corner_points()\n        points = self._obj.corner_points_xy\n        p2p_sum = np.sum(\n            [(b[0] - a[0]) * (b[1] + a[1]) for a, b in windowed(points, 2)]\n        )\n        return p2p_sum\n\n    def get_affine_transform(self):\n        \"\"\"Calculate the forward iline/xline -&gt; cdp_x, cdp_y Affine transform\n        for Matplotlib using corner point geometry.\n\n        Returns:\n            matplotlib.transforms.Affine2D:\n\n        Raises:\n            ValueError: If Dataset is not 3D\n        \"\"\"\n        if not self.is_3d():\n            raise ValueError()\n        self.calc_corner_points()\n\n        # direct solve for affine transform via equation substitution\n        # https://cdn.sstatic.net/Sites/math/img/site-background-image.png?v=09a720444763\n        # ints for iline xline will often overflow\n        (x0p, y0p), (x1p, y1p), (x2p, y2p) = np.array(\n            self._obj.corner_points_xy[:3], dtype=float\n        )\n        (x0, y0), (x1, y1), (x2, y2) = np.array(\n            self._obj.corner_points[:3], dtype=float\n        )\n\n        xs = np.array([v[0] for v in self._obj.corner_points_xy])\n        ys = np.array([v[0] for v in self._obj.corner_points_xy])\n        if np.all(xs == xs[0]) or np.all(ys == ys[0]):\n            raise ValueError(\n                \"The coordinates cannot be transformed, check self.seis.corner_points_xy\"\n            )\n        a = (x1p * y0 - x2p * y0 - x0p * y1 + x2p * y1 + x0p * y2 - x1p * y2) / (\n            x1 * y0 - x2 * y0 - x0 * y1 + x2 * y1 + x0 * y2 - x1 * y2\n        )\n        c = (x1p * x0 - x2p * x0 - x0p * x1 + x2p * x1 + x0p * x2 - x1p * x2) / (\n            -x1 * y0 + x2 * y0 + x0 * y1 - x2 * y1 - x0 * y2 + x1 * y2\n        )\n        b = (y1p * y0 - y2p * y0 - y0p * y1 + y2p * y1 + y0p * y2 - y1p * y2) / (\n            x1 * y0 - x2 * y0 - x0 * y1 + x2 * y1 + x0 * y2 - x1 * y2\n        )\n        d = (y1p * x0 - y2p * x0 - y0p * x1 + y2p * x1 + y0p * x2 - y1p * x2) / (\n            -x1 * y0 + x2 * y0 + x0 * y1 - x2 * y1 - x0 * y2 + x1 * y2\n        )\n        e = (\n            x2p * x1 * y0\n            - x1p * x2 * y0\n            - x2p * x0 * y1\n            + x0p * x2 * y1\n            + x1p * x0 * y2\n            - x0p * x1 * y2\n        ) / (x1 * y0 - x2 * y0 - x0 * y1 + x2 * y1 + x0 * y2 - x1 * y2)\n        f = (\n            y2p * x1 * y0\n            - y1p * x2 * y0\n            - y2p * x0 * y1\n            + y0p * x2 * y1\n            + y1p * x0 * y2\n            - y0p * x1 * y2\n        ) / (x1 * y0 - x2 * y0 - x0 * y1 + x2 * y1 + x0 * y2 - x1 * y2)\n        values = (v if ~np.isnan(v) else 0.0 for v in (a, b, c, d, e, f))\n        affine_grid2loc = Affine2D.from_values(*values)\n        return affine_grid2loc\n\n    def get_dead_trace_map(self, scan=None, zeros_as_nan=False):\n        \"\"\"Scan the vertical axis of a volume to find traces that are all NaN\n        and return an DataArray which maps the all dead traces.\n\n        Faster scans can be performed by setting scan to an int or list of int\n        representing horizontal slice indexes to use for the scan.\n\n        Args:\n            scan (int/list of int, optional): Horizontal indexes to scan.\n                Defaults to None (scan full volume).\n            zeros_as_nan (bool, optional): Treat zeros as NaN during scan.\n\n        Returns:\n            xarray.DataArray: boolean dead trace map.\n        \"\"\"\n\n        if self.is_twt():\n            vdom = CoordKeyField.twt\n        else:\n            vdom = CoordKeyField.depth\n\n        if scan:\n            nan_map = (\n                self._obj.data.isel(**{vdom: scan}).isnull().reduce(np.all, dim=vdom)\n            )\n        else:\n            nan_map = self._obj.data.isnull().reduce(np.all, dim=vdom)\n\n        if zeros_as_nan:\n            zero_map = np.abs(self._obj.data).sum(dim=vdom)\n            nan_map = xr.where(zero_map == 0.0, 1, nan_map)\n\n        return nan_map\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.humanbytes","title":"<code>humanbytes</code>  <code>property</code>","text":"<p>Prints Human Friendly size of Dataset to return the bytes as an int use <code>xarray.Dataset.nbytes</code></p> <p>Returns:</p> Name Type Description <code>str</code> <p>Human readable size of dataset.</p>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.calc_corner_points","title":"<code>calc_corner_points()</code>","text":"<p>Calculate the corner points of the geometry or end points of a 2D line.</p> <p>This puts two properties in the seisnc attrs with the calculated il/xl and cdp_x and cdp_y if available.</p> Attr <p>ds.attrs['corner_points'] ds.attrs['corner_points_xy']</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def calc_corner_points(self):\n    \"\"\"Calculate the corner points of the geometry or end points of a 2D line.\n\n    This puts two properties in the seisnc attrs with the calculated il/xl and\n    cdp_x and cdp_y if available.\n\n    Attr:\n        ds.attrs['corner_points']\n        ds.attrs['corner_points_xy']\n    \"\"\"\n    corner_points = False\n    corner_points_xy = False\n\n    if self.is_3d() or self.is_3dgath():\n        il, xl = DimensionKeyField.cdp_3d\n        ilines = self._obj[il].values\n        xlines = self._obj[xl].values\n        corner_points = (\n            (ilines[0], xlines[0]),\n            (ilines[0], xlines[-1]),\n            (ilines[-1], xlines[-1]),\n            (ilines[-1], xlines[0]),\n        )\n\n        cdp_x, cdp_y = CoordKeyField.cdp_x, CoordKeyField.cdp_y\n\n        if set((cdp_x, cdp_y)).issubset(self._obj.variables):\n            xs = self._obj[cdp_x].values\n            ys = self._obj[cdp_y].values\n            corner_points_xy = (\n                (xs[0, 0], ys[0, 0]),\n                (xs[0, -1], ys[0, -1]),\n                (xs[-1, -1], ys[-1, -1]),\n                (xs[-1, 0], ys[-1, 0]),\n            )\n\n    elif self.is_2d() or self.is_2dgath():\n        cdp = DimensionKeyField.cdp_2d[0]\n        cdps = self._obj[cdp].values\n        corner_points = (cdps[0], cdps[-1])\n\n        cdp_x, cdp_y = CoordKeyField.cdp_x, CoordKeyField.cdp_y\n\n        if set((cdp_x, cdp_y)).issubset(self._obj.variables):\n            xs = self._obj[cdp_x].values\n            ys = self._obj[cdp_y].values\n            corner_points_xy = (\n                (xs[0], ys[0]),\n                (xs[0], ys[0]),\n                (xs[-1], ys[1]),\n                (xs[-1], ys[1]),\n            )\n\n    if corner_points:\n        self._obj.attrs[AttrKeyField.corner_points] = corner_points\n    if corner_points_xy:\n        self._obj.attrs[AttrKeyField.corner_points_xy] = corner_points_xy\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.fill_cdpna","title":"<code>fill_cdpna()</code>","text":"<p>Fills NaN cdp locations by fitting known cdp x and y values to the local grid using a planar surface relationshipt.</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def fill_cdpna(self):\n    \"\"\"Fills NaN cdp locations by fitting known cdp x and y values to the\n    local grid using a planar surface relationshipt.\n    \"\"\"\n    coord_df = coordinate_df(self._obj)\n    self._obj[\"cdp_x\"] = (\n        (\"iline\", \"xline\"),\n        coord_df.cdp_x.values.reshape(self._obj.cdp_x.shape),\n    )\n    self._obj[\"cdp_y\"] = (\n        (\"iline\", \"xline\"),\n        coord_df.cdp_y.values.reshape(self._obj.cdp_y.shape),\n    )\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.get_affine_transform","title":"<code>get_affine_transform()</code>","text":"<p>Calculate the forward iline/xline -&gt; cdp_x, cdp_y Affine transform for Matplotlib using corner point geometry.</p> <p>Returns:</p> Type Description <p>matplotlib.transforms.Affine2D:</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Dataset is not 3D</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def get_affine_transform(self):\n    \"\"\"Calculate the forward iline/xline -&gt; cdp_x, cdp_y Affine transform\n    for Matplotlib using corner point geometry.\n\n    Returns:\n        matplotlib.transforms.Affine2D:\n\n    Raises:\n        ValueError: If Dataset is not 3D\n    \"\"\"\n    if not self.is_3d():\n        raise ValueError()\n    self.calc_corner_points()\n\n    # direct solve for affine transform via equation substitution\n    # https://cdn.sstatic.net/Sites/math/img/site-background-image.png?v=09a720444763\n    # ints for iline xline will often overflow\n    (x0p, y0p), (x1p, y1p), (x2p, y2p) = np.array(\n        self._obj.corner_points_xy[:3], dtype=float\n    )\n    (x0, y0), (x1, y1), (x2, y2) = np.array(\n        self._obj.corner_points[:3], dtype=float\n    )\n\n    xs = np.array([v[0] for v in self._obj.corner_points_xy])\n    ys = np.array([v[0] for v in self._obj.corner_points_xy])\n    if np.all(xs == xs[0]) or np.all(ys == ys[0]):\n        raise ValueError(\n            \"The coordinates cannot be transformed, check self.seis.corner_points_xy\"\n        )\n    a = (x1p * y0 - x2p * y0 - x0p * y1 + x2p * y1 + x0p * y2 - x1p * y2) / (\n        x1 * y0 - x2 * y0 - x0 * y1 + x2 * y1 + x0 * y2 - x1 * y2\n    )\n    c = (x1p * x0 - x2p * x0 - x0p * x1 + x2p * x1 + x0p * x2 - x1p * x2) / (\n        -x1 * y0 + x2 * y0 + x0 * y1 - x2 * y1 - x0 * y2 + x1 * y2\n    )\n    b = (y1p * y0 - y2p * y0 - y0p * y1 + y2p * y1 + y0p * y2 - y1p * y2) / (\n        x1 * y0 - x2 * y0 - x0 * y1 + x2 * y1 + x0 * y2 - x1 * y2\n    )\n    d = (y1p * x0 - y2p * x0 - y0p * x1 + y2p * x1 + y0p * x2 - y1p * x2) / (\n        -x1 * y0 + x2 * y0 + x0 * y1 - x2 * y1 - x0 * y2 + x1 * y2\n    )\n    e = (\n        x2p * x1 * y0\n        - x1p * x2 * y0\n        - x2p * x0 * y1\n        + x0p * x2 * y1\n        + x1p * x0 * y2\n        - x0p * x1 * y2\n    ) / (x1 * y0 - x2 * y0 - x0 * y1 + x2 * y1 + x0 * y2 - x1 * y2)\n    f = (\n        y2p * x1 * y0\n        - y1p * x2 * y0\n        - y2p * x0 * y1\n        + y0p * x2 * y1\n        + y1p * x0 * y2\n        - y0p * x1 * y2\n    ) / (x1 * y0 - x2 * y0 - x0 * y1 + x2 * y1 + x0 * y2 - x1 * y2)\n    values = (v if ~np.isnan(v) else 0.0 for v in (a, b, c, d, e, f))\n    affine_grid2loc = Affine2D.from_values(*values)\n    return affine_grid2loc\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.get_dead_trace_map","title":"<code>get_dead_trace_map(scan=None, zeros_as_nan=False)</code>","text":"<p>Scan the vertical axis of a volume to find traces that are all NaN and return an DataArray which maps the all dead traces.</p> <p>Faster scans can be performed by setting scan to an int or list of int representing horizontal slice indexes to use for the scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan</code> <code>int/list of int</code> <p>Horizontal indexes to scan. Defaults to None (scan full volume).</p> <code>None</code> <code>zeros_as_nan</code> <code>bool</code> <p>Treat zeros as NaN during scan.</p> <code>False</code> <p>Returns:</p> Type Description <p>xarray.DataArray: boolean dead trace map.</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def get_dead_trace_map(self, scan=None, zeros_as_nan=False):\n    \"\"\"Scan the vertical axis of a volume to find traces that are all NaN\n    and return an DataArray which maps the all dead traces.\n\n    Faster scans can be performed by setting scan to an int or list of int\n    representing horizontal slice indexes to use for the scan.\n\n    Args:\n        scan (int/list of int, optional): Horizontal indexes to scan.\n            Defaults to None (scan full volume).\n        zeros_as_nan (bool, optional): Treat zeros as NaN during scan.\n\n    Returns:\n        xarray.DataArray: boolean dead trace map.\n    \"\"\"\n\n    if self.is_twt():\n        vdom = CoordKeyField.twt\n    else:\n        vdom = CoordKeyField.depth\n\n    if scan:\n        nan_map = (\n            self._obj.data.isel(**{vdom: scan}).isnull().reduce(np.all, dim=vdom)\n        )\n    else:\n        nan_map = self._obj.data.isnull().reduce(np.all, dim=vdom)\n\n    if zeros_as_nan:\n        zero_map = np.abs(self._obj.data).sum(dim=vdom)\n        nan_map = xr.where(zero_map == 0.0, 1, nan_map)\n\n    return nan_map\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.get_measurement_system","title":"<code>get_measurement_system()</code>","text":"<p>Return measurement_system if present, else None</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def get_measurement_system(self):\n    \"\"\"Return measurement_system if present, else None\"\"\"\n    if hasattr(self._obj, \"measurement_system\"):\n        return self._obj.measurement_system\n    else:\n        return None\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.grid_rotation","title":"<code>grid_rotation()</code>","text":"<p>Calculate the rotation of the grid using the sum under the curve method.</p> <p>(x2 \u2212 x1)(y2 + y1)</p> <p>Returns a value &gt;0 if the rotation of points along inline is clockwise.</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def grid_rotation(self):\n    \"\"\"Calculate the rotation of the grid using the sum under the curve method.\n\n    (x2 \u2212 x1)(y2 + y1)\n\n    Returns a value &gt;0 if the rotation of points along inline is clockwise.\n    \"\"\"\n    self.calc_corner_points()\n    points = self._obj.corner_points_xy\n    p2p_sum = np.sum(\n        [(b[0] - a[0]) * (b[1] + a[1]) for a, b in windowed(points, 2)]\n    )\n    return p2p_sum\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.interp_line","title":"<code>interp_line(cdp_x, cdp_y, extra=None, bin_spacing_hint=10, line_method='slinear', xysel_method='linear')</code>","text":"<p>Select data at x and y coordinates</p> <p>Parameters:</p> Name Type Description Default <code>bin_spacing_hint</code> <code>number</code> <p>a bin spacing to stay close to, in cdp world units. Default: 10</p> <code>10</code> <code>line_method</code> <code>string</code> <p>valid values for the kind argument in scipy.interpolate.interp1d</p> <code>'slinear'</code> <code>xysel_method</code> <code>string</code> <p>valid values for DataArray.interp</p> <code>'linear'</code> <p>Returns:</p> Type Description <p>xarray.Dataset: Interpolated traces along the arbitrary line</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def interp_line(\n    self,\n    cdp_x,\n    cdp_y,\n    extra=None,\n    bin_spacing_hint=10,\n    line_method=\"slinear\",\n    xysel_method=\"linear\",\n):\n    \"\"\"Select data at x and y coordinates\n\n    Args:\n        cdp_x (float/array-like)\n        cdp_y (float/array-like)\n        bin_spacing_hint (number): a bin spacing to stay close to, in cdp world units. Default: 10\n        line_method (string): valid values for the *kind* argument in scipy.interpolate.interp1d\n        xysel_method (string): valid values for DataArray.interp\n\n    Returns:\n        xarray.Dataset: Interpolated traces along the arbitrary line\n    \"\"\"\n    extra = dict() if extra is None else extra\n\n    cdp_x_i, cdp_y_i, _, extra_i = get_uniform_spacing(\n        cdp_x,\n        cdp_y,\n        bin_spacing_hint=bin_spacing_hint,\n        extra=extra,\n        method=line_method,\n    )\n    ds = self._obj.seis.xysel(cdp_x_i, cdp_y_i, method=xysel_method)\n\n    for key in extra:\n        ds[key] = ((\"cdp\"), extra_i[key])\n\n    return ds\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.is_2d","title":"<code>is_2d()</code>","text":"<p>Returns True if the dataset is 2D peformant else False</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def is_2d(self):\n    \"\"\"Returns True if the dataset is 2D peformant else False\"\"\"\n    dim_options = [\n        DimensionKeyField.twod_twt,\n        DimensionKeyField.twod_depth,\n    ]\n    invalid_dim_options = [\n        DimensionKeyField.twod_ps_twt,\n        DimensionKeyField.twod_ps_depth,\n    ]\n    return self._has_dims(dim_options, invalid_dim_options)\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.is_2dgath","title":"<code>is_2dgath()</code>","text":"<p>Returns True if the dataset is 2D peformant and has offset or angle else False</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def is_2dgath(self):\n    \"\"\"Returns True if the dataset is 2D peformant and has offset or angle else False\"\"\"\n    dim_options = [\n        DimensionKeyField.twod_ps_twt,\n        DimensionKeyField.twod_ps_depth,\n    ]\n    return self._has_dims(dim_options)\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.is_3d","title":"<code>is_3d()</code>","text":"<p>Returns True if the dataset is 3D peformant else False</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def is_3d(self):\n    \"\"\"Returns True if the dataset is 3D peformant else False\"\"\"\n    dim_options = [\n        DimensionKeyField.threed_twt,\n        DimensionKeyField.threed_depth,\n        DimensionKeyField.threed_xline_twt,\n        DimensionKeyField.threed_iline_twt,\n        DimensionKeyField.threed_xline_depth,\n        DimensionKeyField.threed_iline_depth,\n    ]\n    invalid_dim_options = [\n        DimensionKeyField.threed_ps_twt,\n        DimensionKeyField.threed_ps_depth,\n    ]\n    return self._has_dims(dim_options, invalid_dim_options)\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.is_3dgath","title":"<code>is_3dgath()</code>","text":"<p>Returns True if the dataset is 3D peformant and has offset or angle else False</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def is_3dgath(self):\n    \"\"\"Returns True if the dataset is 3D peformant and has offset or angle else False\"\"\"\n    dim_options = [\n        DimensionKeyField.threed_ps_twt,\n        DimensionKeyField.threed_ps_depth,\n    ]\n    return self._has_dims(dim_options)\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.is_depth","title":"<code>is_depth()</code>","text":"<p>Check if seisnc volume is in depth</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def is_depth(self):\n    \"\"\"Check if seisnc volume is in depth\"\"\"\n    self._check_multi_z()\n    return True if CoordKeyField.depth in self._obj.sizes else False\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.is_empty","title":"<code>is_empty()</code>","text":"<p>Check if empty</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def is_empty(self):\n    \"\"\"Check if empty\"\"\"\n    if VariableKeyField.data in self._obj.variables:\n        # This could be smartened up to check logic make sure dimensions of data\n        # are correct.\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.is_twt","title":"<code>is_twt()</code>","text":"<p>Check if seisnc volume is in twt</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def is_twt(self):\n    \"\"\"Check if seisnc volume is in twt\"\"\"\n    self._check_multi_z()\n    return True if CoordKeyField.twt in self._obj.sizes else False\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.plot_bounds","title":"<code>plot_bounds(ax=None)</code>","text":"<p>Plot survey bbounding box to a new or existing axis</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <p>(optional) axis to plot to</p> <code>None</code> <p>Returns:</p> Type Description <p>matplotlib axis used</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def plot_bounds(self, ax=None):\n    \"\"\"Plot survey bbounding box to a new or existing axis\n\n    Args:\n        ax: (optional) axis to plot to\n\n    Returns:\n        matplotlib axis used\n\n    \"\"\"\n    xy = self._obj.attrs[\"corner_points_xy\"]\n\n    if xy is None:\n        self._obj.seis.calc_corner_points()\n        xy = self._obj.attrs[\"corner_points_xy\"]\n\n    if not ax:\n        fig, ax = plt.subplots(1, 1)\n\n    x = [x for x, _ in xy]\n    y = [y for _, y in xy]\n    ax.scatter(x, y)\n    ax.plot(x + [x[0]], y + [y[0]], \"k:\")\n\n    ax.set_xlabel(\"cdp x\")\n    ax.set_ylabel(\"cdp y\")\n\n    return ax\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.subsample_dims","title":"<code>subsample_dims(**dim_kwargs)</code>","text":"<p>Return a dictionary of subsampled dims suitable for xarray.interp.</p> <p>This tool halves</p> <p>Parameters:</p> Name Type Description Default <code>dim_kwargs</code> <p>dimension names as keyword arguments with values of how many times we should divide the dimension by 2.</p> <code>{}</code> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def subsample_dims(self, **dim_kwargs):\n    \"\"\"Return a dictionary of subsampled dims suitable for xarray.interp.\n\n    This tool halves\n\n    Args:\n        dim_kwargs: dimension names as keyword arguments with values of how\n            many times we should divide the dimension by 2.\n    \"\"\"\n    output = dict()\n    for dim in dim_kwargs:\n        ar = self._obj[dim].values\n        while dim_kwargs[dim] &gt; 0:  # only for 3.8\n            ar = halfsample(ar)\n            dim_kwargs[dim] = dim_kwargs[dim] - 1\n        output[dim] = ar\n    return output\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.surface_from_points","title":"<code>surface_from_points(points, attr, left=('cdp_x', 'cdp_y'), right=None, key=None, method='linear')</code>","text":"<p>Sample a 2D point set with an attribute (like Z) to the seisnc geometry using interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>array - like</code> <p>Nx2 array-like of points with coordinates corresponding to coord1 and coord2.</p> required <code>attr</code> <code>(array - like, str)</code> <p>If str points should be a DataFrame where attr is the column name of the attribute to be interpolated to the seisnc geometry. Else attr is a 1D array of length N.</p> required <code>left</code> <code>tuple</code> <p>Length 2 tuple of coordinate dimensions to interpolate to.</p> <code>('cdp_x', 'cdp_y')</code> <code>right</code> <code>tuple</code> <p>If points is DataFrame right is a length 2 tuple of column keys corresponding to coordinates in argument left.</p> <code>None</code> <code>method</code> <code>str</code> <p>The interpolation method to use for griddata from scipy. Defaults to 'linear'</p> <code>'linear'</code> <p>Returns:</p> Type Description <p>xr.Dataset: Surface with geometry specified in left.</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def surface_from_points(\n    self,\n    points,\n    attr,\n    left=(\"cdp_x\", \"cdp_y\"),\n    right=None,\n    key=None,\n    method=\"linear\",\n):\n    \"\"\"Sample a 2D point set with an attribute (like Z) to the seisnc\n    geometry using interpolation.\n\n    Args:\n        points (array-like): Nx2 array-like of points with coordinates\n            corresponding to coord1 and coord2.\n        attr (array-like, str): If str points should be a DataFrame where\n            attr is the column name of the attribute to be interpolated to\n            the seisnc geometry. Else attr is a 1D array of length N.\n        left (tuple): Length 2 tuple of coordinate dimensions to interpolate to.\n        right (tuple, optional): If points is DataFrame right is a length 2\n            tuple of column keys corresponding to coordinates in argument left.\n        method (str): The interpolation method to use for griddata from scipy.\n            Defaults to 'linear'\n\n    Returns:\n        xr.Dataset: Surface with geometry specified in left.\n\n    \"\"\"\n\n    if len(left) != 2 and not isinstance(left, Iterable):\n        raise ValueError(\"left must be tuple of length 2\")\n    for var in left:\n        if var not in self._obj.variables:\n            raise ValueError(\"left coordinate names not found in dataset\")\n    for var in left:\n        if var in self._obj.sizes:\n            raise ValueError(\n                \"surface_from_points is designed for coordinates that \"\n                \"are not already Dataset Dimensions, use ... instead.\"\n            )\n\n    a, b = left\n    if self._obj[a].sizes != self._obj[b].sizes:\n        raise ValueError(\n            f\"left keys {a} and {b} should exist on the same dimensions\"\n        )\n\n    if isinstance(points, pd.DataFrame):\n        if len(right) != 2 and not isinstance(right, Iterable):\n            raise ValueError(\n                \"points is DataFrame, right must a tuple specified \"\n                \"with coordinate keys from column names, e.g right=('cdp_x', 'cdp_y')\"\n            )\n        names = [attr] + list(right)\n        if not set(names).issubset(points.columns):\n            raise ValueError(\"could not find specified keys in points DataFrame\")\n\n        _points = points[list(right)].values\n        _attr = points[attr].values\n\n    else:\n        points = np.asarray(points)\n        attr = np.asarray(attr)\n        if points.shape[1] != 2:\n            raise ValueError(\"points must have shape Nx2\")\n        if attr.size != points.shape[0]:\n            raise ValueError(\"First dimension of points must equal length of attr\")\n        _points = points\n        _attr = attr\n\n    temp_ds = self._obj.copy()\n    org_dims = temp_ds[a].sizes\n    org_shp = temp_ds[a].shape\n\n    var = griddata(\n        _points,\n        _attr,\n        np.dstack(\n            [\n                temp_ds[a]\n                .transpose(*org_dims, transpose_coords=True)\n                .values.ravel(),\n                temp_ds[b]\n                .transpose(*org_dims, transpose_coords=True)\n                .values.ravel(),\n            ]\n        ),\n        method=method,\n    )\n\n    out = temp_ds.drop_vars(temp_ds.var().keys())\n    temp_ds.attrs.clear()\n    if key is None and isinstance(attr, str):\n        key = attr\n    elif key is None:\n        key = \"data\"\n    out[key] = (org_dims, var.reshape(org_shp))\n    return out\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.xysel","title":"<code>xysel(cdp_x, cdp_y, method='nearest', sample_dim_name='cdp')</code>","text":"<p>Select data at x and y coordinates</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Same as methods for xarray.Dataset.interp</p> <code>'nearest'</code> <code>sample_dim_name</code> <code>str</code> <p>The name to give the output sampling dimension.</p> <code>'cdp'</code> <p>Returns:</p> Type Description <p>xarray.Dataset: At selected coordinates.</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def xysel(self, cdp_x, cdp_y, method=\"nearest\", sample_dim_name=\"cdp\"):\n    \"\"\"Select data at x and y coordinates\n\n    Args:\n        cdp_x (float/array-like)\n        cdp_y (float/array-like)\n        method (str): Same as methods for xarray.Dataset.interp\n        sample_dim_name (str, optional): The name to give the output sampling dimension.\n\n    Returns:\n        xarray.Dataset: At selected coordinates.\n    \"\"\"\n\n    cdp_x = np.atleast_1d(cdp_x)\n    cdp_y = np.atleast_1d(cdp_y)\n\n    if self.is_twt():\n        core_dims = DimensionKeyField.threed_twt\n    elif self.is_depth():\n        core_dims = DimensionKeyField.threed_depth\n    else:\n        raise AttributeError(\"Dataset required twt or depth coordinates.\")\n\n    dims = set(self._obj.sizes.keys())\n\n    if self.is_3d() or self.is_3dgath():\n        # get other dims\n        other_dims = dims.difference(core_dims)\n        il, xl = self._coord_as_dimension((cdp_x, cdp_y), other_dims)\n\n        sampling_arrays = dict(\n            iline=xr.DataArray(\n                il, dims=sample_dim_name, coords={sample_dim_name: range(il.size)}\n            ),\n            xline=xr.DataArray(\n                xl, dims=sample_dim_name, coords={sample_dim_name: range(xl.size)}\n            ),\n        )\n        output_ds = self._obj.interp(**sampling_arrays, method=method)\n\n        # # populate back to 2d\n        # cdp_ds = [\n        #     None\n        #     if np.isnan(np.sum([i, x]))\n        #     else self._obj.interp(iline=i, xline=x, method=method)\n        #     for i, x in zip(il, xl)\n        # ]\n\n        # if all(map(lambda x: x is None, cdp_ds)):\n        #     raise ValueError(\"No points intersected the dataset\")\n\n        # for ds in cdp_ds:\n        #     if ds is not None:\n        #         none_replace = ds.copy(deep=True)\n        #         break\n\n        # none_replace[VariableKeyField.data][:] = np.nan\n        # none_replace[CoordKeyField.iline] = np.nan\n        # none_replace[CoordKeyField.xline] = np.nan\n\n        # for i, (point, xloc, yloc) in enumerate(zip(cdp_ds, cdp_x, cdp_y)):\n        #     if point is None:\n        #         point = none_replace\n        #     point[CoordKeyField.cdp_x] = point[CoordKeyField.cdp_x] * 0 + xloc\n        #     point[CoordKeyField.cdp_y] = point[CoordKeyField.cdp_y] * 0 + yloc\n        #     point[CoordKeyField.cdp] = i + 1\n        #     point.attrs = dict()\n        #     cdp_ds[i] = point.copy()\n\n    elif self.is_2d() or self.is_2dgath():\n        raise NotImplementedError(\"Not yet implemented for 2D\")\n    else:\n        raise AttributeError(\n            \"xysel not support for this volume, must be 3d, 3dgath, 2d or 2dgath\"\n        )\n\n    # cdp_ds = xr.concat(cdp_ds, \"cdp\")\n    # cdp_ds.attrs = self._obj.attrs.copy()\n\n    # return cdp_ds\n    output_ds.attrs = self._obj.attrs.copy()\n    return output_ds\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak.SeisGeom.zeros_like","title":"<code>zeros_like()</code>","text":"<p>Create a new dataset with the same attributes and coordinates and dimensions but with data filled by zeros.</p> Source code in <code>segysak/_accessor.py</code> Python<pre><code>def zeros_like(self):\n    \"\"\"Create a new dataset with the same attributes and coordinates and\n    dimensions but with data filled by zeros.\n    \"\"\"\n    current_dims = {dim: val for dim, val in self._obj.sizes.items()}\n    dims = [dim for dim in current_dims.keys()]\n    shp = [val for val in current_dims.values()]\n    # TODO: Investigate copy options, might need to manually copy geometry\n    # also don't want to load into memory if large large file.\n    out = self._obj.copy()\n    out[VariableKeyField.data] = (dims, np.zeros(shp))\n    return out\n</code></pre>"},{"location":"api/xarray_new.html","title":"New Xarray datasets","text":"<p>These methods help to create datasets with appropriate coordinates for seismic data.</p>"},{"location":"api/xarray_new.html#segysak.create3d_dataset","title":"<code>create3d_dataset(dims, first_sample=0, sample_rate=1, first_iline=1, iline_step=1, first_xline=1, xline_step=1, first_offset=None, offset_step=None, vert_domain='TWT', vert_units=None)</code>","text":"<p>Create a regular 3D seismic dataset from basic grid geometry with optional offset dimension for pre-stack data.</p> <p>Parameters:</p> Name Type Description Default <code>dims</code> <code>tuple of int</code> <p>The dimensions of the dataset to create (iline, xline, vertical). If first_offset is specified then (iline, xline, vertical, offset)</p> required <code>first_sample</code> <code>int</code> <p>The first vertical sample. Defaults to 0.</p> <code>0</code> <code>sample_rate</code> <code>int</code> <p>The vertical sample rate. Defaults to 1.</p> <code>1</code> <code>first_iline</code> <code>int</code> <p>First inline number. Defaults to 1.</p> <code>1</code> <code>iline_step</code> <code>int</code> <p>Inline increment. Defaults to 1.</p> <code>1</code> <code>first_xline</code> <code>int</code> <p>First crossline number. Defaults to 1.</p> <code>1</code> <code>xline_step</code> <code>int</code> <p>Crossline increment. Defaults to 1.</p> <code>1</code> <code>first_offset</code> <code>int / float</code> <p>If not none, the offset dimension will be added starting at first offset. Defaults to None.</p> <code>None</code> <code>offset_step</code> <code>(int, float)</code> <p>Required if first_offset is specified. The offset increment.</p> <code>None</code> <code>vert_domain</code> <code>str</code> <p>Vertical domain, one of ('DEPTH', 'TWT'). Defaults to 'TWT'.</p> <code>'TWT'</code> <code>vert_units(str,</code> <code>optional</code> <p>Measurement system of of vertical coordinates. One of ('ms', 's', 'm', 'km', 'ft'): Defaults to None for unknown.</p> required Source code in <code>segysak/_seismic_dataset.py</code> Python<pre><code>def create3d_dataset(\n    dims,\n    first_sample=0,\n    sample_rate=1,\n    first_iline=1,\n    iline_step=1,\n    first_xline=1,\n    xline_step=1,\n    first_offset=None,\n    offset_step=None,\n    vert_domain=\"TWT\",\n    vert_units=None,\n):\n    \"\"\"Create a regular 3D seismic dataset from basic grid geometry with optional\n    offset dimension for pre-stack data.\n\n    Args:\n        dims (tuple of int): The dimensions of the dataset to create (iline, xline, vertical).\n            If first_offset is specified then (iline, xline, vertical, offset)\n        first_sample (int, optional): The first vertical sample. Defaults to 0.\n        sample_rate (int, optional): The vertical sample rate. Defaults to 1.\n        first_iline (int, optional): First inline number. Defaults to 1.\n        iline_step (int, optional): Inline increment. Defaults to 1.\n        first_xline (int, optional): First crossline number. Defaults to 1.\n        xline_step (int, optional): Crossline increment. Defaults to 1.\n        first_offset (int/float, optional): If not none, the offset dimension will be added starting\n            at first offset. Defaults to None.\n        offset_step (int, float, optional): Required if first_offset is specified. The offset increment.\n        vert_domain (str, optional): Vertical domain, one of ('DEPTH', 'TWT'). Defaults to 'TWT'.\n        vert_units(str, optional): Measurement system of of vertical coordinates.\n            One of ('ms', 's', 'm', 'km', 'ft'): Defaults to None for unknown.\n    \"\"\"\n    vert_domain = vert_domain.upper()\n\n    if first_offset is None:\n        ni, nx, ns = dims\n    else:\n        ni, nx, ns, no = dims\n\n    units = _check_vert_units(vert_units)\n\n    # 1e-10 to stabilise range on rounding errors for weird floats from hypothesis\n\n    vert = np.arange(\n        first_sample,\n        first_sample + sample_rate * ns - 1e-10,\n        sample_rate,\n        dtype=int,\n    )\n    ilines = np.arange(\n        first_iline,\n        first_iline + iline_step * ni - 1e-10,\n        iline_step,\n        dtype=int,\n    )\n    xlines = np.arange(\n        first_xline,\n        first_xline + xline_step * nx - 1e-10,\n        xline_step,\n        dtype=int,\n    )\n\n    if first_offset is None:\n        offset = None\n    else:\n        offset = np.arange(\n            first_offset,\n            first_offset + offset_step * no - 1e-10,\n            offset_step,\n            dtype=float,\n        )\n\n    builder, domain = _dataset_coordinate_helper(\n        vert, vert_domain, iline=ilines, xline=xlines, offset=offset\n    )\n\n    ds = create_seismic_dataset(**builder)\n    ds.attrs[AttrKeyField.measurement_system] = units\n    ds.attrs[AttrKeyField.d3_domain] = domain\n    ds.attrs[AttrKeyField.sample_rate] = sample_rate\n    ds.attrs[AttrKeyField.text] = \"SEGY-SAK Create 3D Dataset\"\n    ds.attrs[AttrKeyField.corner_points] = [\n        (ilines[0], xlines[0]),\n        (ilines[-1], xlines[0]),\n        (ilines[-1], xlines[-1]),\n        (ilines[0], xlines[-1]),\n        (ilines[0], xlines[0]),\n    ]\n\n    return ds\n</code></pre>"},{"location":"api/xarray_new.html#segysak.create2d_dataset","title":"<code>create2d_dataset(dims, first_sample=0, sample_rate=1, first_cdp=1, cdp_step=1, first_offset=None, offset_step=None, vert_domain='TWT', vert_units=None)</code>","text":"<p>Create a regular 2D seismic dataset from basic geometry.</p> <p>Parameters:</p> Name Type Description Default <code>dims</code> <code>tuple of int</code> <p>The dimensions of the dataset to create (ncdp, vertical). If first_offset is specified then (ncdp, vertical, offset)</p> required <code>first_sample</code> <code>int</code> <p>The first vertical sample. Defaults to 0.</p> <code>0</code> <code>sample_rate</code> <code>int</code> <p>The vertical sample rate. Defaults to 1.</p> <code>1</code> <code>first_cdp</code> <code>int</code> <p>First CDP number. Defaults to 1.</p> <code>1</code> <code>cdp_step</code> <code>int</code> <p>CDP increment. Defaults to 1.</p> <code>1</code> <code>first_offset</code> <code>int / float</code> <p>If not none, the offset dimension will be added starting at first offset. Defaults to None.</p> <code>None</code> <code>offset_step</code> <code>(int, float)</code> <p>Required if first_offset is specified. The offset increment.</p> <code>None</code> <code>vert_domain</code> <code>str</code> <p>Vertical domain, one of ('DEPTH', 'TWT'). Defaults to 'TWT'.</p> <code>'TWT'</code> <code>vert_units(str,</code> <code>optional</code> <p>Measurement system of of vertical coordinates. One of ('ms', 's', 'm', 'km', 'ft'): Defaults to None for unknown.</p> required Source code in <code>segysak/_seismic_dataset.py</code> Python<pre><code>def create2d_dataset(\n    dims,\n    first_sample=0,\n    sample_rate=1,\n    first_cdp=1,\n    cdp_step=1,\n    first_offset=None,\n    offset_step=None,\n    vert_domain=\"TWT\",\n    vert_units=None,\n):\n    \"\"\"Create a regular 2D seismic dataset from basic geometry.\n\n    Args:\n        dims (tuple of int): The dimensions of the dataset to create (ncdp, vertical).\n            If first_offset is specified then (ncdp, vertical, offset)\n        first_sample (int, optional): The first vertical sample. Defaults to 0.\n        sample_rate (int, optional): The vertical sample rate. Defaults to 1.\n        first_cdp (int, optional): First CDP number. Defaults to 1.\n        cdp_step (int, optional): CDP increment. Defaults to 1.\n        first_offset (int/float, optional): If not none, the offset dimension will be added starting\n            at first offset. Defaults to None.\n        offset_step (int, float, optional): Required if first_offset is specified. The offset increment.\n        vert_domain (str, optional): Vertical domain, one of ('DEPTH', 'TWT'). Defaults to 'TWT'.\n        vert_units(str, optional): Measurement system of of vertical coordinates.\n            One of ('ms', 's', 'm', 'km', 'ft'): Defaults to None for unknown.\n    \"\"\"\n    vert_domain = vert_domain.upper()\n\n    if first_offset is None:\n        ncdp, ns = dims\n    else:\n        ncdp, ns, no = dims\n\n    # check units\n    units = _check_vert_units(vert_units)\n\n    vert = np.arange(\n        first_sample, first_sample + sample_rate * ns, sample_rate, dtype=int\n    )\n    cdps = np.arange(first_cdp, first_cdp + cdp_step * ncdp, cdp_step, dtype=int)\n\n    if first_offset is None:\n        offset = None\n    else:\n        offset = np.arange(\n            first_offset, first_offset + offset_step * no, offset_step, dtype=float\n        )\n\n    builder, domain = _dataset_coordinate_helper(\n        vert, vert_domain, cdp=cdps, offset=offset\n    )\n\n    ds = create_seismic_dataset(**builder)\n    ds.attrs[AttrKeyField.measurement_system] = units\n    ds.attrs[AttrKeyField.d3_domain] = domain\n    ds.attrs[AttrKeyField.sample_rate] = sample_rate\n    ds.attrs[AttrKeyField.text] = \"SEGY-SAK Create 2D Dataset\"\n\n    return ds\n</code></pre>"},{"location":"api/xarray_new.html#segysak.create_seismic_dataset","title":"<code>create_seismic_dataset(twt=None, depth=None, cdp=None, iline=None, xline=None, offset=None, **dim_args)</code>","text":"<p>Create a blank seismic dataset by setting the dimension sizes (d#) or by passing arrays for known dimensions.</p> <p>iline and xline must be specified together and are mutually exclusive to cdp argument.</p> <p>Parameters:</p> Name Type Description Default <code>twt</code> <code>int / array - like</code> <p>Two-way time vertical sampling coordinates. Cannot be used with depth argument. Defaults to None.</p> <code>None</code> <code>depth</code> <code>int / array - like</code> <p>Depth vertical sampling coordinates. Cannot be used with twt argument. Defaults to None.</p> <code>None</code> <code>cdp</code> <code>int / array - like</code> <p>The CDP numbering for 2D data, cannot be used with iline or xline. Use for 2D seismic data. Defaults to None.</p> <code>None</code> <code>iline</code> <code>int / array - like</code> <p>The iline numbering, cannot be used with cdp argument. Use for 3D seismic data. Defaults to None.</p> <code>None</code> <code>xline</code> <code>int / array - like</code> <p>The xline numbering, cannot be used with cdp argument. Use for 3D seismic data. Defaults to None.</p> <code>None</code> <code>offset</code> <code>int / array - like</code> <p>The offset. This will fill dimension d4. Use for pre-stack data. Defaults to None.</p> <code>None</code> <code>dim_args</code> <code>int / array - like</code> <p>Other dimensions you would like in your dataset. The key will be the dimension name.</p> <code>{}</code> <p>Returns:</p> Type Description <p>xarray.Dataset: A dataset with the defined dimensions of input setup to work with seisnc standards.</p> Source code in <code>segysak/_seismic_dataset.py</code> Python<pre><code>def create_seismic_dataset(\n    twt=None, depth=None, cdp=None, iline=None, xline=None, offset=None, **dim_args\n):\n    \"\"\"Create a blank seismic dataset by setting the dimension sizes (d#) or by passing\n    arrays for known dimensions.\n\n    iline and xline must be specified together and are mutually exclusive to cdp argument.\n\n    Args:\n        twt (int/array-like, optional): Two-way time vertical sampling coordinates.\n            Cannot be used with depth argument. Defaults to None.\n        depth (int/array-like, optional): Depth vertical sampling coordinates.\n            Cannot be used with twt argument. Defaults to None.\n        cdp (int/array-like, optional): The CDP numbering for 2D data, cannot be used\n            with iline or xline. Use for 2D seismic data. Defaults to None.\n        iline (int/array-like, optional): The iline numbering, cannot be\n            used with cdp argument. Use for 3D seismic data. Defaults to None.\n        xline (int/array-like, optional): The xline numbering, cannot be\n            used with cdp argument. Use for 3D seismic data. Defaults to None.\n        offset (int/array-like, optional): The offset. This will fill dimension d4.\n            Use for pre-stack data. Defaults to None.\n        dim_args (int/array-like): Other dimensions you would like in your dataset. The key will be the dimension name.\n\n    Returns:\n        xarray.Dataset: A dataset with the defined dimensions of input setup to work with seisnc standards.\n    \"\"\"\n    # cdp not allowed with iline or xline\n    if cdp is not None and (iline is not None or xline is not None):\n        raise ValueError(\n            \"cdp argument cannot be used with 3D dimensions (iline or xline)\"\n        )\n\n    if iline is not None and xline is None:\n        raise ValueError(\"xline needed with iline argument, 3d geometry requires both\")\n\n    if xline is not None and iline is None:\n        raise ValueError(\"xline needed with iline argument, 3d geometry requires both\")\n\n    # check inputs\n    twt = _check_input(twt)\n    depth = _check_input(depth)\n    cdp = _check_input(cdp)\n    iline = _check_input(iline)\n    xline = _check_input(xline)\n    offset = _check_input(offset)\n    # # make sure other dim args are sensible and to spec\n    for key in dim_args.keys():\n        dim_args[key] = _check_input(dim_args[key])\n\n    dimensions = dict()\n    # create dimension d1\n    if cdp is not None:\n        dimensions[CoordKeyField.cdp] = ([CoordKeyField.cdp], cdp)\n    elif iline is not None:  # 3d data\n        dimensions[CoordKeyField.iline] = ([CoordKeyField.iline], iline)\n        dimensions[CoordKeyField.xline] = ([CoordKeyField.xline], xline)\n\n    # create dimension d3\n    if twt is not None:\n        dimensions[CoordKeyField.twt] = ([CoordKeyField.twt], twt)\n    if depth is not None:\n        dimensions[CoordKeyField.depth] = ([CoordKeyField.depth], depth)\n\n    # create dimension d4\n    if offset is not None:\n        dimensions[CoordKeyField.offset] = (\n            [CoordKeyField.offset],\n            offset,\n        )\n\n    # any left over dims\n    for arg, val in dim_args.items():\n        dimensions[arg] = ([arg], val)\n\n    ds = xr.Dataset(coords=dimensions)\n\n    if twt is not None:\n        ds.attrs[AttrKeyField.ns] = twt.size\n    elif depth is not None:\n        ds.attrs[AttrKeyField.ns] = depth.size\n\n    ds.attrs.update({name: None for name in AttrKeyField})\n\n    return ds\n</code></pre>"},{"location":"cli/about.html","title":"SEGY-SAK - Command Line Interface","text":"<p>SEGY-SAK offers an interface on the command line to inspect SEG-Y files and to convert between data formats. These are convenience wrappers around core <code>segysak</code> functions to allow fast an easy interrogation of/interaction wtih SEG-Y files.</p> <p>For example, it is possible to scrape the text header from a SEG-Y file</p> Bash<pre><code>segysak ebcidc volve10r12-full-twt-sub3d.sgy\nC 1 SEGY OUTPUT FROM Petrel 2017.2 Saturday, June 06 2020 10:15:00\nC 2 Name: ST10010ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534\n\u00ddCroC 3\nC 4 First inline: 10090  Last inline: 10150\nC 5 First xline:  2150   Last xline:  2351\n...\nC37\nC38\nC39\nC40 END EBCDIC\n</code></pre> <p>Standard linux redirects can be used to output the header to a file or to other command line tools.</p> <p>Bash<pre><code>segysak ebcidc volve10r12-full-twt-sub3d.sgy &gt; header.txt\nsegysak ebcidc volve10r12-full-twt-sub3d.sgy | less\n</code></pre> From <code>segysak&gt;=0.5</code> file conversion is conducted with lazy loading, this should allow very large SEG-Y files to be converted to more performant file formats such as ZGY<sup>1</sup> and NetCDF4.</p> Bash<pre><code>segysak convert \n</code></pre> <p>A full list of sub-commands is available in the CLI Reference or individually for each sub-command individually using the <code>--help</code> flag.</p> Bash<pre><code>segysak scan --help\n</code></pre> <ol> <li> <p>ZGY file operations require <code>pyzgy&gt;=0.10</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"cli/command-line-ref.html","title":"CLI Reference","text":""},{"location":"cli/command-line-ref.html#base-command","title":"Base command","text":"<p>segysak</p> Bash<pre><code>Usage: segysak [OPTIONS] COMMAND [ARGS]...\n\n  The SEG-Y Swiss Army Knife (segysak) is a tool for managing segy data. It\n  can read and dump ebcidc headers, scan trace headers, convert SEG-Y to\n  SEISNC and vice versa\n\nOptions:\n  -v, --version  Print application version name\n  --help         Show this message and exit.\n\nCommands:\n  convert  Convert file between SEG-Y and NETCDF (direction is guessed or...\n  ebcidc   Print SEG-Y EBCIDC header\n  scan     Scan trace headers and print value ranges\n  scrape   Scrape the file meta information and output it to text file.\n</code></pre>"},{"location":"cli/command-line-ref.html#ebcidc-headers","title":"EBCIDC headers","text":"<p>segysak ebcidc</p> Bash<pre><code>Usage: segysak ebcidc [OPTIONS] FILENAME\n\n  Print SEG-Y EBCIDC header\n\nOptions:\n  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/command-line-ref.html#trace-headers","title":"Trace Headers","text":"<p>segysak scan</p> Bash<pre><code>Usage: segysak scan [OPTIONS] FILENAME\n\n  Scan trace headers and print value ranges\n\nOptions:\n  -m, --max-traces INTEGER  Number of traces to scan\n  --help                    Show this message and exit.\n</code></pre> <p>segysak scrape</p> Bash<pre><code>Usage: segysak scrape [OPTIONS] [FILENAME]...\n\n  Scrape the file meta information and output it to text file.\n\n  If no options are specified both will be output. The output file will be\n  &lt;filename&gt;.txt for the EBCIDC and &lt;filename&gt;.csv for trace headers.\n\n  The trace headers can be read back into Python using\n  pandas.read_csv(&lt;filename&gt;.csv, index_col=0)\n\nOptions:\n  -e, --ebcidc         Output the text header\n  -h, --trace-headers  Output the trace headers to csv\n  --help               Show this message and exit.\n</code></pre>"},{"location":"cli/command-line-ref.html#file-format-conversion","title":"File format conversion","text":"<p>segysak convert</p> Bash<pre><code>Usage: segysak convert [OPTIONS] [INPUT_FILES]...\n\n  Convert file between SEG-Y and NETCDF (direction is guessed or can be made\n  explicit with the --output-type option)\n\nOptions:\n  -o, --output-file TEXT        Output file name\n  -il, --iline INTEGER          Inline byte location\n  -xl, --xline INTEGER          Crossline byte location\n  -x, --cdp-x INTEGER           CDP X byte location\n  -y, --cdp-y INTEGER           CDP Y byte location\n  --crop INTEGER...             Crop the input volume providing 4 parameters:\n                                minil maxil minxl maxxl\n  --output-type [SEG-Y|NETCDF]  Explicitly state the desired output file type\n                                by chosing one of the options\n  -d, --dimension TEXT          Data dimension (domain) to write out, will\n                                default to TWT or DEPTH. Only used for writing\n                                to SEG-Y.\n  --help                        Show this message and exit.\n</code></pre>"},{"location":"examples/QuickOverview.html","title":"Quick Overview","text":"<p>The library is imported as segysak and the loaded <code>xarray</code> objects are compatible with numpy and matplotlib.</p> <p>The cropped volume from the Volve field in the North Sea (made available by Equinor) is used for this example, and all the examples and data in this documentation are available from the <code>examples</code> folder of the Github respository.</p> In\u00a0[\u00a0]: Copied! <pre>import warnings\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import warnings  warnings.filterwarnings(\"ignore\") In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport pathlib\n</pre> import matplotlib.pyplot as plt import pathlib In\u00a0[\u00a0]: Copied! <pre>V3D_path = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\")\nprint(\"3D\", V3D_path, V3D_path.exists())\n</pre> V3D_path = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\") print(\"3D\", V3D_path, V3D_path.exists()) <p>A basic operation would be to check the text header included in the SEG-Y file. The get_segy_texthead function accounts for common encoding issues and returns the header as a text string.</p> In\u00a0[\u00a0]: Copied! <pre>from segysak.segy import get_segy_texthead\n\nget_segy_texthead(V3D_path)\n</pre> from segysak.segy import get_segy_texthead  get_segy_texthead(V3D_path) <p>If you need to investigate the trace header data more deeply, then segy_header_scan can be used to report basic statistics of each byte position for a limited number of traces.</p> <p>segy_header_scan returns a <code>pandas.DataFrame</code>. To see the full DataFrame use the <code>pandas</code> option_context manager.</p> In\u00a0[\u00a0]: Copied! <pre>from segysak.segy import segy_header_scan\n\nscan = segy_header_scan(V3D_path)\nscan\n</pre> from segysak.segy import segy_header_scan  scan = segy_header_scan(V3D_path) scan <p>The header report can also be reduced by filtering blank byte locations. Here we use the standard deviation <code>std</code> to filter away blank values which can help us to understand the composition of the data.</p> <p>For instance, key values like trace UTM coordinates are located in bytes 73 for X &amp; 77 for Y. We can also see the byte positions of the local grid for INLINE_3D in byte 189 and for CROSSLINE_3D in byte 193.</p> In\u00a0[\u00a0]: Copied! <pre>scan[scan[\"std\"] &gt; 0]\n</pre> scan[scan[\"std\"] &gt; 0] <p>To retreive the raw header content use <code>segy_header_scrape</code>. Setting <code>partial_scan=None</code> will return the full dataframe of trace header information.</p> In\u00a0[\u00a0]: Copied! <pre>from segysak.segy import segy_header_scrape\n\nscrape = segy_header_scrape(V3D_path, partial_scan=1000)\nscrape\n</pre> from segysak.segy import segy_header_scrape  scrape = segy_header_scrape(V3D_path, partial_scan=1000) scrape <p>All SEG-Y (2D, 2D gathers, 3D &amp; 3D gathers) are ingested into <code>xarray.Dataset</code> objects through the <code>segy_loader</code> function. It is best to be explicit about the byte locations of key information but <code>segy_loader</code> can attempt to guess the shape of your dataset. Some standard byte positions are defined in the <code>well_known_bytes</code> function and others can be added via pull requests to the Github repository if desired.</p> In\u00a0[\u00a0]: Copied! <pre>from segysak.segy import segy_loader, well_known_byte_locs\n\nV3D = segy_loader(V3D_path, iline=189, xline=193, cdp_x=73, cdp_y=77, vert_domain=\"TWT\")\nV3D\n</pre> from segysak.segy import segy_loader, well_known_byte_locs  V3D = segy_loader(V3D_path, iline=189, xline=193, cdp_x=73, cdp_y=77, vert_domain=\"TWT\") V3D In\u00a0[\u00a0]: Copied! <pre>fig, ax1 = plt.subplots(ncols=1, figsize=(15, 8))\niline_sel = 10093\nV3D.data.transpose(\"twt\", \"iline\", \"xline\", transpose_coords=True).sel(\n    iline=iline_sel\n).plot(yincrease=False, cmap=\"seismic_r\")\nplt.grid(\"grey\")\nplt.ylabel(\"TWT\")\nplt.xlabel(\"XLINE\")\n</pre> fig, ax1 = plt.subplots(ncols=1, figsize=(15, 8)) iline_sel = 10093 V3D.data.transpose(\"twt\", \"iline\", \"xline\", transpose_coords=True).sel(     iline=iline_sel ).plot(yincrease=False, cmap=\"seismic_r\") plt.grid(\"grey\") plt.ylabel(\"TWT\") plt.xlabel(\"XLINE\") In\u00a0[\u00a0]: Copied! <pre>V3D.seisio.to_netcdf(\"V3D.SEISNC\")\n</pre> V3D.seisio.to_netcdf(\"V3D.SEISNC\") In\u00a0[\u00a0]: Copied! <pre>from segysak.segy import segy_writer\n\nsegy_writer(\n    V3D, \"V3D.segy\", trace_header_map=dict(iline=5, xline=21)\n)  # Petrel Locations\n</pre> from segysak.segy import segy_writer  segy_writer(     V3D, \"V3D.segy\", trace_header_map=dict(iline=5, xline=21) )  # Petrel Locations"},{"location":"examples/QuickOverview.html#quick-overview","title":"Quick Overview\u00b6","text":"<p>Here you can find some quick examples of what you can do with segysak. For more details refer to the examples.</p>"},{"location":"examples/QuickOverview.html#scan-seg-y-headers","title":"Scan SEG-Y headers\u00b6","text":""},{"location":"examples/QuickOverview.html#load-seg-y-data","title":"Load SEG-Y data\u00b6","text":""},{"location":"examples/QuickOverview.html#visualising-data","title":"Visualising data\u00b6","text":"<p><code>xarray</code> objects use smart label based indexing techniques to retreive subsets of data. More details on <code>xarray</code> techniques for segysak are covered in the examples, but this demonstrates a general syntax for selecting data by label with <code>xarray</code>. Plotting is done by <code>matploblib</code> and <code>xarray</code> selections can be passed to normal <code>matplotlib.pyplot</code> functions.</p>"},{"location":"examples/QuickOverview.html#saving-data-to-netcdf4","title":"Saving data to NetCDF4\u00b6","text":"<p>SEGYSAK offers a convenience utility to make saving to NetCDF4 simple. This is accesssed through the <code>seisio</code> accessor on the loaded SEG-Y or SEISNC volume. The <code>to_netcdf</code> method accepts the same arguments as the <code>xarray</code> version.</p>"},{"location":"examples/QuickOverview.html#saving-data-to-seg-y","title":"Saving data to SEG-Y\u00b6","text":"<p>To return data to SEG-Y after modification use the <code>segy_writer</code> function. <code>segy_writer</code> takes as argument a SEISNC dataset which requires certain attributes be set. You can also specify the byte locations to write header information.</p>"},{"location":"examples/example_amplitude_extraction_displays.html","title":"Working with seismic and interpreted horizons","text":"In\u00a0[\u00a0]: Copied! <pre>import pathlib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import pathlib import numpy as np import pandas as pd import matplotlib.pyplot as plt import warnings  warnings.filterwarnings(\"ignore\") In\u00a0[\u00a0]: Copied! <pre>from segysak.segy import (\n    segy_loader,\n    get_segy_texthead,\n    segy_header_scan,\n    segy_header_scrape,\n    well_known_byte_locs,\n)\n</pre> from segysak.segy import (     segy_loader,     get_segy_texthead,     segy_header_scan,     segy_header_scrape,     well_known_byte_locs, ) In\u00a0[\u00a0]: Copied! <pre>segy_file = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\")\ncube = segy_loader(segy_file.absolute(), **well_known_byte_locs(\"petrel_3d\"))\n</pre> segy_file = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\") cube = segy_loader(segy_file.absolute(), **well_known_byte_locs(\"petrel_3d\")) In\u00a0[\u00a0]: Copied! <pre>print(\"Loaded cube size: {}\".format(cube.seis.humanbytes))\n</pre> print(\"Loaded cube size: {}\".format(cube.seis.humanbytes)) In\u00a0[\u00a0]: Copied! <pre>print(\"Is it two-way-time? {}\".format(cube.seis.is_twt()))\n</pre> print(\"Is it two-way-time? {}\".format(cube.seis.is_twt())) In\u00a0[\u00a0]: Copied! <pre>hrz_file = pathlib.Path(\"data/hor_twt_hugin_fm_top.dat\")\nhrz = pd.read_csv(hrz_file, names=[\"cdp_x\", \"cdp_y\", \"twt\"], sep=\"\\s+\")\nhrz.head()\n</pre> hrz_file = pathlib.Path(\"data/hor_twt_hugin_fm_top.dat\") hrz = pd.read_csv(hrz_file, names=[\"cdp_x\", \"cdp_y\", \"twt\"], sep=\"\\s+\") hrz.head() In\u00a0[\u00a0]: Copied! <pre>cube.seis.calc_corner_points()\ncorners = np.array(cube.attrs[\"corner_points_xy\"])\n</pre> cube.seis.calc_corner_points() corners = np.array(cube.attrs[\"corner_points_xy\"]) <p>To display the horizon we need to grid the raw data first:</p> In\u00a0[\u00a0]: Copied! <pre>from scipy.interpolate import griddata\n\nxi = np.linspace(hrz.cdp_x.min(), hrz.cdp_x.max(), 250)\nyi = np.linspace(hrz.cdp_y.min(), hrz.cdp_y.max(), 2500)\nX, Y = np.meshgrid(xi, yi)\nZ = griddata((hrz.cdp_x, hrz.cdp_y), hrz.twt, (X, Y))\n</pre> from scipy.interpolate import griddata  xi = np.linspace(hrz.cdp_x.min(), hrz.cdp_x.max(), 250) yi = np.linspace(hrz.cdp_y.min(), hrz.cdp_y.max(), 2500) X, Y = np.meshgrid(xi, yi) Z = griddata((hrz.cdp_x, hrz.cdp_y), hrz.twt, (X, Y)) <p>And this is the resulting plot with the extent of the loaded cube displayed as a thick red outline:</p> In\u00a0[\u00a0]: Copied! <pre>from matplotlib.patches import Polygon\n\nsurvey_limits = Polygon(\n    corners, fill=False, edgecolor=\"r\", linewidth=2, label=\"3D survey extent\"\n)\n\nf, ax = plt.subplots(figsize=(8, 6))\npp = ax.pcolormesh(X, Y, Z, cmap=\"terrain_r\")\nf.colorbar(pp, orientation=\"horizontal\", label=\"TWT [ms]\")\nax.add_patch(survey_limits)\nax.axis(\"equal\")\nax.legend()\nax.set_title(\"Top Hugin fm.\")\n</pre> from matplotlib.patches import Polygon  survey_limits = Polygon(     corners, fill=False, edgecolor=\"r\", linewidth=2, label=\"3D survey extent\" )  f, ax = plt.subplots(figsize=(8, 6)) pp = ax.pcolormesh(X, Y, Z, cmap=\"terrain_r\") f.colorbar(pp, orientation=\"horizontal\", label=\"TWT [ms]\") ax.add_patch(survey_limits) ax.axis(\"equal\") ax.legend() ax.set_title(\"Top Hugin fm.\") In\u00a0[\u00a0]: Copied! <pre>hrz_mapped = cube.seis.surface_from_points(hrz, \"twt\", right=(\"cdp_x\", \"cdp_y\"))\n</pre> hrz_mapped = cube.seis.surface_from_points(hrz, \"twt\", right=(\"cdp_x\", \"cdp_y\")) <p>And to extract seismic amplitudes along this horizon we use the magic of <code>xarray</code>:</p> In\u00a0[\u00a0]: Copied! <pre>amp = cube.data.interp(\n    {\"iline\": hrz_mapped.iline, \"xline\": hrz_mapped.xline, \"twt\": hrz_mapped.twt}\n)\n</pre> amp = cube.data.interp(     {\"iline\": hrz_mapped.iline, \"xline\": hrz_mapped.xline, \"twt\": hrz_mapped.twt} ) <p>For the next plot we use another attribute automatically calculated by segysak during loading to squeeze the colormap used when displaying amplitudes:</p> In\u00a0[\u00a0]: Copied! <pre>minamp, maxamp = cube.attrs[\"percentiles\"][1], cube.attrs[\"percentiles\"][-2]\n</pre> minamp, maxamp = cube.attrs[\"percentiles\"][1], cube.attrs[\"percentiles\"][-2] <p>...and we calculate the minimum and maximum X and Y coordinates using the <code>corners</code> array described above to set the figure extent and zoom in the area covered by the seismic cube:</p> In\u00a0[\u00a0]: Copied! <pre>xmin, xmax = corners[:, 0].min(), corners[:, 0].max()\nymin, ymax = corners[:, 1].min(), corners[:, 1].max()\n</pre> xmin, xmax = corners[:, 0].min(), corners[:, 0].max() ymin, ymax = corners[:, 1].min(), corners[:, 1].max() <p>We can plot <code>amp</code> now on top of the same twt grid we did above:</p> In\u00a0[\u00a0]: Copied! <pre>survey_limits = Polygon(\n    corners, fill=False, edgecolor=\"r\", linewidth=2, label=\"3D survey extent\"\n)\n\nf, ax = plt.subplots(nrows=2, figsize=(8, 10))\nax[0].pcolormesh(X, Y, Z, cmap=\"terrain_r\")\nax[0].add_patch(survey_limits)\nfor aa in ax:\n    hh = aa.pcolormesh(\n        amp.cdp_x, amp.cdp_y, amp.data, cmap=\"RdYlBu\", vmin=minamp, vmax=maxamp\n    )\n    aa.axis(\"equal\")\nax[0].legend()\nax[1].set_xlim(xmin, xmax)\nax[1].set_ylim(ymin, ymax)\nax[0].set_title(\"Top Hugin fm. and amplitude extraction on loaded seismic\")\nax[1].set_title(\"Amplitude extraction at Top Hugin (zoom)\")\ncax = f.add_axes([0.15, 0.16, 0.5, 0.02])\nf.colorbar(hh, cax=cax, orientation=\"horizontal\")\n</pre> survey_limits = Polygon(     corners, fill=False, edgecolor=\"r\", linewidth=2, label=\"3D survey extent\" )  f, ax = plt.subplots(nrows=2, figsize=(8, 10)) ax[0].pcolormesh(X, Y, Z, cmap=\"terrain_r\") ax[0].add_patch(survey_limits) for aa in ax:     hh = aa.pcolormesh(         amp.cdp_x, amp.cdp_y, amp.data, cmap=\"RdYlBu\", vmin=minamp, vmax=maxamp     )     aa.axis(\"equal\") ax[0].legend() ax[1].set_xlim(xmin, xmax) ax[1].set_ylim(ymin, ymax) ax[0].set_title(\"Top Hugin fm. and amplitude extraction on loaded seismic\") ax[1].set_title(\"Amplitude extraction at Top Hugin (zoom)\") cax = f.add_axes([0.15, 0.16, 0.5, 0.02]) f.colorbar(hh, cax=cax, orientation=\"horizontal\") <p>Another classic display is to superimpose the structure contours to the amplitudes. This is much faster and easier using survey coordinates (inlines and crosslines):</p> In\u00a0[\u00a0]: Copied! <pre>f, ax = plt.subplots(figsize=(12, 4))\namp.plot(cmap=\"RdYlBu\")\ncs = plt.contour(amp.xline, amp.iline, hrz_mapped.twt, levels=20, colors=\"grey\")\nplt.clabel(cs, fontsize=10, fmt=\"%.0f\")\nax.invert_xaxis()\n</pre> f, ax = plt.subplots(figsize=(12, 4)) amp.plot(cmap=\"RdYlBu\") cs = plt.contour(amp.xline, amp.iline, hrz_mapped.twt, levels=20, colors=\"grey\") plt.clabel(cs, fontsize=10, fmt=\"%.0f\") ax.invert_xaxis() In\u00a0[\u00a0]: Copied! <pre>opt = dict(\n    x=\"xline\",\n    y=\"twt\",\n    add_colorbar=True,\n    interpolation=\"spline16\",\n    robust=True,\n    yincrease=False,\n    cmap=\"Greys\",\n)\n\ninl_sel = [10130, 10100]\n\nf, ax = plt.subplots(nrows=2, figsize=(10, 6), sharey=True, constrained_layout=True)\nfor i, val in enumerate(inl_sel):\n    cube.data.sel(iline=val, twt=slice(2300, 3000)).plot.imshow(ax=ax[i], **opt)\n    x, t = hrz_mapped.sel(iline=val).xline, hrz_mapped.sel(iline=val).twt\n    ax[i].plot(x, t, color=\"r\")\n    ax[i].invert_xaxis()\n</pre> opt = dict(     x=\"xline\",     y=\"twt\",     add_colorbar=True,     interpolation=\"spline16\",     robust=True,     yincrease=False,     cmap=\"Greys\", )  inl_sel = [10130, 10100]  f, ax = plt.subplots(nrows=2, figsize=(10, 6), sharey=True, constrained_layout=True) for i, val in enumerate(inl_sel):     cube.data.sel(iline=val, twt=slice(2300, 3000)).plot.imshow(ax=ax[i], **opt)     x, t = hrz_mapped.sel(iline=val).xline, hrz_mapped.sel(iline=val).twt     ax[i].plot(x, t, color=\"r\")     ax[i].invert_xaxis() <p>We can also show an overlay of amplitudes and two-way-times along the same inlines:</p> In\u00a0[\u00a0]: Copied! <pre>inl_sel = [10130, 10100]\n\nf, ax = plt.subplots(nrows=2, figsize=(10, 6), sharey=True, constrained_layout=True)\n\nfor i, val in enumerate(inl_sel):\n    axz = ax[i].twinx()\n    x, t = amp.sel(iline=val).xline, amp.sel(iline=val).twt\n    a = amp.sel(iline=val).data\n    ax[i].plot(x, a, color=\"r\")\n    axz.plot(x, t, color=\"k\")\n    ax[i].invert_xaxis()\n    axz.invert_yaxis()\n    ax[i].set_ylabel(\"Amplitude\", color=\"r\")\n    plt.setp(ax[i].yaxis.get_majorticklabels(), color=\"r\")\n    axz.set_ylabel(\"TWT [ms]\")\n    ax[i].set_title(\"Amplitude and two-way-time at inline {}\".format(val))\n</pre> inl_sel = [10130, 10100]  f, ax = plt.subplots(nrows=2, figsize=(10, 6), sharey=True, constrained_layout=True)  for i, val in enumerate(inl_sel):     axz = ax[i].twinx()     x, t = amp.sel(iline=val).xline, amp.sel(iline=val).twt     a = amp.sel(iline=val).data     ax[i].plot(x, a, color=\"r\")     axz.plot(x, t, color=\"k\")     ax[i].invert_xaxis()     axz.invert_yaxis()     ax[i].set_ylabel(\"Amplitude\", color=\"r\")     plt.setp(ax[i].yaxis.get_majorticklabels(), color=\"r\")     axz.set_ylabel(\"TWT [ms]\")     ax[i].set_title(\"Amplitude and two-way-time at inline {}\".format(val)) <p>To create windows map extraction it is necessary to first mask the seisnc cube and then to collapse it along the chosen axis using a prefered method. In this case we are just going to calculate the mean amplitude in a 100ms window below our horizon.</p> In\u00a0[\u00a0]: Copied! <pre>mask_below = cube.where(cube.twt &lt; hrz_mapped.twt + 100)\nmask_above = cube.where(cube.twt &gt; hrz_mapped.twt)\nmasks = [mask_above, mask_below]\n</pre> mask_below = cube.where(cube.twt &lt; hrz_mapped.twt + 100) mask_above = cube.where(cube.twt &gt; hrz_mapped.twt) masks = [mask_above, mask_below] <p>Lets see what our masks look like</p> In\u00a0[\u00a0]: Copied! <pre>opt = dict(\n    x=\"xline\",\n    y=\"twt\",\n    add_colorbar=True,\n    interpolation=\"spline16\",\n    robust=True,\n    yincrease=False,\n    cmap=\"Greys\",\n)\n\ninl_sel = [10130, 10100]\n\nf, ax = plt.subplots(nrows=2, figsize=(10, 6), sharey=True, constrained_layout=True)\nfor i, val in enumerate(\n    inl_sel,\n):\n    masks[i].data.sel(iline=val, twt=slice(2300, 3000)).plot.imshow(ax=ax[i], **opt)\n    x, t = hrz_mapped.sel(iline=val).xline, hrz_mapped.sel(iline=val).twt\n    ax[i].plot(x, t, color=\"r\")\n    ax[i].invert_xaxis()\n</pre> opt = dict(     x=\"xline\",     y=\"twt\",     add_colorbar=True,     interpolation=\"spline16\",     robust=True,     yincrease=False,     cmap=\"Greys\", )  inl_sel = [10130, 10100]  f, ax = plt.subplots(nrows=2, figsize=(10, 6), sharey=True, constrained_layout=True) for i, val in enumerate(     inl_sel, ):     masks[i].data.sel(iline=val, twt=slice(2300, 3000)).plot.imshow(ax=ax[i], **opt)     x, t = hrz_mapped.sel(iline=val).xline, hrz_mapped.sel(iline=val).twt     ax[i].plot(x, t, color=\"r\")     ax[i].invert_xaxis() <p>And if we combine the masks.</p> In\u00a0[\u00a0]: Copied! <pre>opt = dict(\n    x=\"xline\",\n    y=\"twt\",\n    add_colorbar=True,\n    interpolation=\"spline16\",\n    robust=True,\n    yincrease=False,\n    cmap=\"Greys\",\n)\n\ninl_sel = [10130, 10100]\n\nf, ax = plt.subplots(nrows=1, figsize=(10, 3), sharey=True, constrained_layout=True)\n\nmasked_data = 0.5 * (mask_below + mask_above)\n\nmasked_data.data.sel(iline=val, twt=slice(2300, 3000)).plot.imshow(ax=ax, **opt)\nx, t = hrz_mapped.sel(iline=val).xline, hrz_mapped.sel(iline=val).twt\nax.plot(x, t, color=\"r\")\nax.invert_xaxis()\n</pre> opt = dict(     x=\"xline\",     y=\"twt\",     add_colorbar=True,     interpolation=\"spline16\",     robust=True,     yincrease=False,     cmap=\"Greys\", )  inl_sel = [10130, 10100]  f, ax = plt.subplots(nrows=1, figsize=(10, 3), sharey=True, constrained_layout=True)  masked_data = 0.5 * (mask_below + mask_above)  masked_data.data.sel(iline=val, twt=slice(2300, 3000)).plot.imshow(ax=ax, **opt) x, t = hrz_mapped.sel(iline=val).xline, hrz_mapped.sel(iline=val).twt ax.plot(x, t, color=\"r\") ax.invert_xaxis() <p>To get the horizon window extraction for sum of amplitudes we now need to sum along the time axis. Or we can use the <code>np.apply_along_axis</code> function to apply a custom function to our masked cube.</p> In\u00a0[\u00a0]: Copied! <pre>summed_amp = masked_data.sum(dim=\"twt\")\n\nf, ax = plt.subplots(figsize=(12, 4))\nsummed_amp.data.plot.imshow(\n    cmap=\"RdYlBu\",\n    interpolation=\"spline16\",\n)\ncs = plt.contour(amp.xline, amp.iline, hrz_mapped.twt, levels=20, colors=\"grey\")\nplt.clabel(cs, fontsize=10, fmt=\"%.0f\")\nax.invert_xaxis()\nax.set_title(\"Sum of amplitudes Top Hugin 0 to +100ms\")\n</pre> summed_amp = masked_data.sum(dim=\"twt\")  f, ax = plt.subplots(figsize=(12, 4)) summed_amp.data.plot.imshow(     cmap=\"RdYlBu\",     interpolation=\"spline16\", ) cs = plt.contour(amp.xline, amp.iline, hrz_mapped.twt, levels=20, colors=\"grey\") plt.clabel(cs, fontsize=10, fmt=\"%.0f\") ax.invert_xaxis() ax.set_title(\"Sum of amplitudes Top Hugin 0 to +100ms\")"},{"location":"examples/example_amplitude_extraction_displays.html#working-with-seismic-and-interpreted-horizons","title":"Working with seismic and interpreted horizons\u00b6","text":""},{"location":"examples/example_amplitude_extraction_displays.html#load-horizon-data","title":"Load horizon data\u00b6","text":"<p>First we load the horizon data to a Pandas DataFrame and take a look at the first few lines:</p>"},{"location":"examples/example_amplitude_extraction_displays.html#display-horizon-and-seismic-extents","title":"Display horizon and seismic extents\u00b6","text":"<p>We can build a plot to display the horizon in its entirety and overlay it with the extent of the seismic cube previously loaded.</p> <p>First we use segysak built-in <code>calc_corner_points()</code> method to calculate the corner points of the loaded cube and copy them to a numpy array to be used for the plot:</p>"},{"location":"examples/example_amplitude_extraction_displays.html#extracting-amplitudes-along-the-horizon","title":"Extracting amplitudes along the horizon\u00b6","text":"<p>This is where the magic of segysak comes in. We use <code>surface_from_points</code> to map the loaded horizon imported in tabular format to each seismic bin. The input horizon in this case is defined in geographical coordinates but it would also have worked if it was defined in inlines and crosslines:</p>"},{"location":"examples/example_amplitude_extraction_displays.html#display-horizon-in-section-view","title":"Display horizon in section view\u00b6","text":""},{"location":"examples/example_amplitude_extraction_displays.html#horizon-sculpting-and-windowed-map-extraction","title":"Horizon Sculpting and Windowed map Extraction\u00b6","text":""},{"location":"examples/example_extract_arbitrary_line.html","title":"Extract an arbitrary line from a 3D volume","text":"In\u00a0[\u00a0]: Copied! <pre>import warnings\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import warnings  warnings.filterwarnings(\"ignore\") In\u00a0[\u00a0]: Copied! <pre>from os import path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport wellpathpy as wpp\nimport xarray as xr\n</pre> from os import path import numpy as np import pandas as pd import matplotlib.pyplot as plt import wellpathpy as wpp import xarray as xr In\u00a0[\u00a0]: Copied! <pre>from segysak import __version__\n\nprint(__version__)\n</pre> from segysak import __version__  print(__version__) In\u00a0[\u00a0]: Copied! <pre>volve_3d_path = path.join(\"data\", \"volve10r12-full-twt-sub3d.sgy\")\nprint(f\"{volve_3d_path} exists: {path.exists(volve_3d_path)}\")\n</pre> volve_3d_path = path.join(\"data\", \"volve10r12-full-twt-sub3d.sgy\") print(f\"{volve_3d_path} exists: {path.exists(volve_3d_path)}\") In\u00a0[\u00a0]: Copied! <pre>from segysak.segy import (\n    segy_loader,\n    get_segy_texthead,\n    segy_header_scan,\n    segy_header_scrape,\n    well_known_byte_locs,\n)\n\nvolve_3d = segy_loader(volve_3d_path, **well_known_byte_locs(\"petrel_3d\"))\n</pre> from segysak.segy import (     segy_loader,     get_segy_texthead,     segy_header_scan,     segy_header_scrape,     well_known_byte_locs, )  volve_3d = segy_loader(volve_3d_path, **well_known_byte_locs(\"petrel_3d\")) In\u00a0[\u00a0]: Copied! <pre>arb_line_A = (\n    np.array([434300, 434600, 435500, 436300]),\n    np.array([6.4786e6, 6.4780e6, 6.4779e6, 6.4781e6]),\n)\narb_line_B = (\n    np.array([434000, 434600, 435500, 436500]),\n    np.array([6.4786e6, 6.4776e6, 6.4786e6, 6.4775e6]),\n)\n</pre> arb_line_A = (     np.array([434300, 434600, 435500, 436300]),     np.array([6.4786e6, 6.4780e6, 6.4779e6, 6.4781e6]), ) arb_line_B = (     np.array([434000, 434600, 435500, 436500]),     np.array([6.4786e6, 6.4776e6, 6.4786e6, 6.4775e6]), ) <p>Let's see how these lines are placed relative to the survey bounds. We can see A is fully enclosed whilst B has some segments outside.</p> In\u00a0[\u00a0]: Copied! <pre>ax = volve_3d.seis.plot_bounds()\nax.plot(arb_line_A[0], arb_line_A[1], \".-\", label=\"Arb Line A\")\nax.plot(arb_line_B[0], arb_line_B[1], \".-\", label=\"Arb Line B\")\nax.legend()\n</pre> ax = volve_3d.seis.plot_bounds() ax.plot(arb_line_A[0], arb_line_A[1], \".-\", label=\"Arb Line A\") ax.plot(arb_line_B[0], arb_line_B[1], \".-\", label=\"Arb Line B\") ax.legend() <p>Let's extract line A.</p> <p>We specify a <code>bin_spacing_hint</code> which is our desired bin spacing along the line. The function use this hint to calculate the closest binspacing that maintains uniform sampling.</p> <p>We have also specified the <code>method='linear'</code>, this is the default but you can specify and method that <code>DataArray.interp</code> accepts</p> In\u00a0[\u00a0]: Copied! <pre>from time import time\n\ntic = time()\nline_A = volve_3d.seis.interp_line(arb_line_A[0], arb_line_A[1], bin_spacing_hint=10)\ntoc = time()\nprint(f\"That took {toc-tic} seconds\")\n</pre> from time import time  tic = time() line_A = volve_3d.seis.interp_line(arb_line_A[0], arb_line_A[1], bin_spacing_hint=10) toc = time() print(f\"That took {toc-tic} seconds\") In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(8, 8))\nplt.imshow(line_A.to_array().squeeze().T, aspect=\"auto\", cmap=\"RdBu\", vmin=-10, vmax=10)\n</pre> plt.figure(figsize=(8, 8)) plt.imshow(line_A.to_array().squeeze().T, aspect=\"auto\", cmap=\"RdBu\", vmin=-10, vmax=10) <p>Now, let's extract line B. Note that blank traces are inserted where the line extends outside the survey bounds.</p> In\u00a0[\u00a0]: Copied! <pre>tic = time()\nline_B = volve_3d.seis.interp_line(arb_line_B[0], arb_line_B[1], bin_spacing_hint=10)\ntoc = time()\nprint(f\"That took {toc-tic} seconds\")\n</pre> tic = time() line_B = volve_3d.seis.interp_line(arb_line_B[0], arb_line_B[1], bin_spacing_hint=10) toc = time() print(f\"That took {toc-tic} seconds\") In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10, 8))\nplt.imshow(line_B.to_array().squeeze().T, aspect=\"auto\", cmap=\"RdBu\", vmin=-10, vmax=10)\n</pre> plt.figure(figsize=(10, 8)) plt.imshow(line_B.to_array().squeeze().T, aspect=\"auto\", cmap=\"RdBu\", vmin=-10, vmax=10) In\u00a0[\u00a0]: Copied! <pre>import shapefile\nfrom pprint import pprint\n</pre> import shapefile from pprint import pprint <p>Load the shapefile and get the list of points</p> In\u00a0[\u00a0]: Copied! <pre>sf = shapefile.Reader(path.join(\"data\", \"arbitrary_line.shp\"))\nline_petrel = sf.shapes()\nprint(f\"shapes are type {sf.shapeType} == POLYLINEZ\")\nprint(f\"There are {len(sf.shapes())} shapes in here\")\nprint(f\"The line has {len(sf.shape(0).points)} points\")\n\npoints_from_shapefile = [list(t) for t in list(zip(*sf.shape(0).points))]\npprint(points_from_shapefile)\n</pre> sf = shapefile.Reader(path.join(\"data\", \"arbitrary_line.shp\")) line_petrel = sf.shapes() print(f\"shapes are type {sf.shapeType} == POLYLINEZ\") print(f\"There are {len(sf.shapes())} shapes in here\") print(f\"The line has {len(sf.shape(0).points)} points\")  points_from_shapefile = [list(t) for t in list(zip(*sf.shape(0).points))] pprint(points_from_shapefile) <p>Load the segy containing the line that Petrel extracted along this geometry</p> In\u00a0[\u00a0]: Copied! <pre>line_extracted_by_petrel_path = path.join(\"data\", \"volve10r12-full-twt-arb.sgy\")\nprint(\n    f\"{line_extracted_by_petrel_path} exists: {path.exists(line_extracted_by_petrel_path)}\"\n)\nline_extracted_by_petrel = segy_loader(\n    line_extracted_by_petrel_path, **well_known_byte_locs(\"petrel_3d\")\n)\n</pre> line_extracted_by_petrel_path = path.join(\"data\", \"volve10r12-full-twt-arb.sgy\") print(     f\"{line_extracted_by_petrel_path} exists: {path.exists(line_extracted_by_petrel_path)}\" ) line_extracted_by_petrel = segy_loader(     line_extracted_by_petrel_path, **well_known_byte_locs(\"petrel_3d\") ) <p>Extract the line using segysak</p> In\u00a0[\u00a0]: Copied! <pre>tic = time()\nline_extracted_by_segysak = volve_3d.seis.interp_line(\n    *points_from_shapefile,\n    bin_spacing_hint=10,\n    line_method=\"linear\",\n    xysel_method=\"linear\",\n)\ntoc = time()\nprint(f\"That took {toc-tic} seconds\")\n</pre> tic = time() line_extracted_by_segysak = volve_3d.seis.interp_line(     *points_from_shapefile,     bin_spacing_hint=10,     line_method=\"linear\",     xysel_method=\"linear\", ) toc = time() print(f\"That took {toc-tic} seconds\") <p>Plot the extracted lines side by side</p> In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n\naxs[0].imshow(\n    line_extracted_by_segysak.to_array().squeeze().T,\n    aspect=\"auto\",\n    cmap=\"RdBu\",\n    vmin=-10,\n    vmax=10,\n)\naxs[0].set_title(\"segysak\")\naxs[1].imshow(\n    line_extracted_by_petrel.to_array().squeeze().T,\n    aspect=\"auto\",\n    cmap=\"RdBu\",\n    vmin=-10,\n    vmax=10,\n)\naxs[1].set_title(\"petrel\")\n</pre> fig, axs = plt.subplots(1, 2, figsize=(16, 8))  axs[0].imshow(     line_extracted_by_segysak.to_array().squeeze().T,     aspect=\"auto\",     cmap=\"RdBu\",     vmin=-10,     vmax=10, ) axs[0].set_title(\"segysak\") axs[1].imshow(     line_extracted_by_petrel.to_array().squeeze().T,     aspect=\"auto\",     cmap=\"RdBu\",     vmin=-10,     vmax=10, ) axs[1].set_title(\"petrel\") <p>Plot the geometry, trace header locations along with the volve 3d bound box, to make sure things line up.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10, 10))\nax = volve_3d.seis.plot_bounds(ax=plt.gca())\n\n# plot path\nax.plot(*points_from_shapefile)\nax.scatter(*points_from_shapefile)\n\n# plot trace positons from extracted lines based on header\nax.scatter(\n    line_extracted_by_segysak.cdp_x,\n    line_extracted_by_segysak.cdp_y,\n    marker=\"x\",\n    color=\"k\",\n)\nax.scatter(\n    line_extracted_by_petrel.cdp_x,\n    line_extracted_by_petrel.cdp_y,\n    marker=\"+\",\n    color=\"r\",\n)\n\nax.set_xlim(434500, 435500)\nax.set_ylim(6477800, 6478600)\nplt.legend(labels=[\"bounds\", \"geom\", \"corner\", \"segysak\", \"petrel\"])\n</pre> plt.figure(figsize=(10, 10)) ax = volve_3d.seis.plot_bounds(ax=plt.gca())  # plot path ax.plot(*points_from_shapefile) ax.scatter(*points_from_shapefile)  # plot trace positons from extracted lines based on header ax.scatter(     line_extracted_by_segysak.cdp_x,     line_extracted_by_segysak.cdp_y,     marker=\"x\",     color=\"k\", ) ax.scatter(     line_extracted_by_petrel.cdp_x,     line_extracted_by_petrel.cdp_y,     marker=\"+\",     color=\"r\", )  ax.set_xlim(434500, 435500) ax.set_ylim(6477800, 6478600) plt.legend(labels=[\"bounds\", \"geom\", \"corner\", \"segysak\", \"petrel\"]) In\u00a0[\u00a0]: Copied! <pre>f12_dev = pd.read_csv(\"data/well_f12_deviation.asc\", comment=\"#\", delim_whitespace=True)\nf12_dev_pos = wpp.deviation(*f12_dev[[\"MD\", \"INCL\", \"AZIM_GN\"]].values.T)\n\n# depth values in MD that we want to sample the seismic cube at\nnew_depths = np.arange(0, f12_dev[\"MD\"].max(), 1)\n\n# use minimum curvature and resample to 1m interval\nf12_dev_pos = f12_dev_pos.minimum_curvature().resample(new_depths)\n\n# adjust position of deviation to local coordinates and TVDSS\nf12_dev_pos.to_wellhead(\n    6478566.23,\n    435050.21,\n    inplace=True,\n)\nf12_dev_pos.to_tvdss(\n    54.9,\n    inplace=True,\n)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nvolve_3d.seis.plot_bounds(ax=ax)\nsc = ax.scatter(f12_dev_pos.easting, f12_dev_pos.northing, c=f12_dev_pos.depth, s=1)\nplt.colorbar(sc, label=\"F12 Depth\")\nax.set_aspect(\"equal\")\n</pre> f12_dev = pd.read_csv(\"data/well_f12_deviation.asc\", comment=\"#\", delim_whitespace=True) f12_dev_pos = wpp.deviation(*f12_dev[[\"MD\", \"INCL\", \"AZIM_GN\"]].values.T)  # depth values in MD that we want to sample the seismic cube at new_depths = np.arange(0, f12_dev[\"MD\"].max(), 1)  # use minimum curvature and resample to 1m interval f12_dev_pos = f12_dev_pos.minimum_curvature().resample(new_depths)  # adjust position of deviation to local coordinates and TVDSS f12_dev_pos.to_wellhead(     6478566.23,     435050.21,     inplace=True, ) f12_dev_pos.to_tvdss(     54.9,     inplace=True, )  fig, ax = plt.subplots(figsize=(10, 5)) volve_3d.seis.plot_bounds(ax=ax) sc = ax.scatter(f12_dev_pos.easting, f12_dev_pos.northing, c=f12_dev_pos.depth, s=1) plt.colorbar(sc, label=\"F12 Depth\") ax.set_aspect(\"equal\") <p>We can easily sample the seismic cube by converting the positional log to <code>iline</code> and <code>xline</code> using the Affine transform for our data. We also need to convert the TVDSS values of the data to TWT (in this case we will just use a constant velocity).</p> <p>In both instances we will create custom xarray.DataArray instances because this allows us to relate the coordinate systems of well samples (on the new dimension <code>well</code>) to the <code>iline</code> and <code>xline</code> dimensions of the cube.</p> In\u00a0[\u00a0]: Copied! <pre># need the inverse to go from xy to il/xl\naffine = volve_3d.seis.get_affine_transform().inverted()\nilxl = affine.transform(np.dstack([f12_dev_pos.easting, f12_dev_pos.northing])[0])\n\nf12_dev_ilxl = dict(\n    iline=xr.DataArray(ilxl[:, 0], dims=\"well\", coords={\"well\": range(ilxl.shape[0])}),\n    xline=xr.DataArray(ilxl[:, 1], dims=\"well\", coords={\"well\": range(ilxl.shape[0])}),\n)\n\ntwt = xr.DataArray(\n    -1.0 * f12_dev_pos.depth * 2000 / 2400,  # 2400 m/s to convert to TWT - and negate,\n    dims=\"well\",\n    coords={\"well\": range(ilxl.shape[0])},\n)\n\nf12_dev_ilxl\n</pre> # need the inverse to go from xy to il/xl affine = volve_3d.seis.get_affine_transform().inverted() ilxl = affine.transform(np.dstack([f12_dev_pos.easting, f12_dev_pos.northing])[0])  f12_dev_ilxl = dict(     iline=xr.DataArray(ilxl[:, 0], dims=\"well\", coords={\"well\": range(ilxl.shape[0])}),     xline=xr.DataArray(ilxl[:, 1], dims=\"well\", coords={\"well\": range(ilxl.shape[0])}), )  twt = xr.DataArray(     -1.0 * f12_dev_pos.depth * 2000 / 2400,  # 2400 m/s to convert to TWT - and negate,     dims=\"well\",     coords={\"well\": range(ilxl.shape[0])}, )  f12_dev_ilxl <p>The DataArrays with the new axes can be passed to interp which will perform interpolation for us on the new dimension <code>well</code>.</p> <p>We can also plot the well path on our seismic extracted along the well path.</p> In\u00a0[\u00a0]: Copied! <pre>sel = volve_3d.interp(**f12_dev_ilxl)\nfig, axs = plt.subplots(figsize=(20, 10))\nsel.data.T.plot(ax=axs, yincrease=False)\ntwt.plot(color=\"k\", ax=axs)\n</pre> sel = volve_3d.interp(**f12_dev_ilxl) fig, axs = plt.subplots(figsize=(20, 10)) sel.data.T.plot(ax=axs, yincrease=False) twt.plot(color=\"k\", ax=axs) <p>To extract the data along the well path we just need to interpolate using the additional <code>twt</code> DataArray.</p> In\u00a0[\u00a0]: Copied! <pre>well_seismic = volve_3d.interp(**f12_dev_ilxl, twt=twt)\nwell_seismic.data.plot()\n</pre> well_seismic = volve_3d.interp(**f12_dev_ilxl, twt=twt) well_seismic.data.plot() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/example_extract_arbitrary_line.html#extract-an-arbitrary-line-from-a-3d-volume","title":"Extract an arbitrary line from a 3D volume\u00b6","text":"<p>Arbitrary lines are often defined as piecewise lines on time/z slices or basemap views that draw a path through features of interest or for example between well locations.</p> <p>By extracting an arbitrary line we hope to end up with a uniformly sampled vertical section of data that traverses the path where the sampling interval is of the order of the bin interval of the dataset.</p>"},{"location":"examples/example_extract_arbitrary_line.html#load-small-3d-volume-from-volve","title":"Load Small 3D Volume from Volve\u00b6","text":""},{"location":"examples/example_extract_arbitrary_line.html#arbitrary-lines","title":"Arbitrary Lines\u00b6","text":"<p>We define a line as lists of cdp_x &amp; cdp_y points. These can be inside or outside of the survey, but obviously should intersect it in order to be useful.</p>"},{"location":"examples/example_extract_arbitrary_line.html#petrel-shapefile","title":"Petrel Shapefile\u00b6","text":"<p>We have an arbitrary line geometry defined over this small survey region stored in a shape file exported from Petrel.</p> <p>Let's load that and extract an arbitrary line using segysak. We also have the seismic data extracted along that line by Petrel, so we can see how that compares.</p>"},{"location":"examples/example_extract_arbitrary_line.html#well-paths","title":"Well Paths\u00b6","text":"<p>Well paths can also be treated as an arbitrary line. In this example we will use the Affine transform to convert well X and Y locations to the seismic local grid, and the Xarray <code>interp</code> method to extract a seismic trace along the well bore.</p> <p>First we have to load the well bore deviation and use <code>wellpathpy</code> to convert it to XYZ coordinates with a higher sampling rate.</p>"},{"location":"examples/example_extract_data_on_a_horizon.html","title":"Extract data at the intersection of a horizon and 3D volume","text":"In\u00a0[\u00a0]: Copied! <pre>import warnings\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import warnings  warnings.filterwarnings(\"ignore\") In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nfrom segysak.segy import (\n    segy_loader,\n    get_segy_texthead,\n    segy_header_scan,\n    segy_header_scrape,\n)\n\nfrom os import path\n</pre> %load_ext autoreload %autoreload 2  import matplotlib.pyplot as plt  %matplotlib inline  from segysak.segy import (     segy_loader,     get_segy_texthead,     segy_header_scan,     segy_header_scrape, )  from os import path In\u00a0[\u00a0]: Copied! <pre>volve_3d_path = path.join(\"data\", \"volve10r12-full-twt-sub3d.sgy\")\nprint(\"3D\", volve_3d_path, path.exists(volve_3d_path))\n</pre> volve_3d_path = path.join(\"data\", \"volve10r12-full-twt-sub3d.sgy\") print(\"3D\", volve_3d_path, path.exists(volve_3d_path)) In\u00a0[\u00a0]: Copied! <pre>get_segy_texthead(volve_3d_path)\n</pre> get_segy_texthead(volve_3d_path) In\u00a0[\u00a0]: Copied! <pre>from segysak.segy import well_known_byte_locs\n\nvolve_3d = segy_loader(volve_3d_path, **well_known_byte_locs(\"petrel_3d\"))\nvolve_3d.data\n</pre> from segysak.segy import well_known_byte_locs  volve_3d = segy_loader(volve_3d_path, **well_known_byte_locs(\"petrel_3d\")) volve_3d.data In\u00a0[\u00a0]: Copied! <pre>top_hugin_path = path.join(\"data\", \"hor_twt_hugin_fm_top.dat\")\nprint(\"Top Hugin\", top_hugin_path, path.exists(top_hugin_path))\n</pre> top_hugin_path = path.join(\"data\", \"hor_twt_hugin_fm_top.dat\") print(\"Top Hugin\", top_hugin_path, path.exists(top_hugin_path)) In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\ntop_hugin_df = pd.read_csv(top_hugin_path, names=[\"cdp_x\", \"cdp_y\", \"twt\"], sep=\" \")\ntop_hugin_df.head()\n</pre> import pandas as pd  top_hugin_df = pd.read_csv(top_hugin_path, names=[\"cdp_x\", \"cdp_y\", \"twt\"], sep=\" \") top_hugin_df.head() <p>Would be good to plot a seismic (iline,xline) section in Pyvista as well</p> In\u00a0[\u00a0]: Copied! <pre># import pyvista as pv\n\n# point_cloud = pv.PolyData(-1*top_hugin_df.to_numpy(), cmap='viridis')\n# point_cloud.plot(eye_dome_lighting=True)\n</pre> # import pyvista as pv  # point_cloud = pv.PolyData(-1*top_hugin_df.to_numpy(), cmap='viridis') # point_cloud.plot(eye_dome_lighting=True) <p>Alternativey we can use the points to output a <code>xarray.Dataset</code> which comes with coordinates for plotting already gridded up for Pyvista.</p> In\u00a0[\u00a0]: Copied! <pre>top_hugin_ds = volve_3d.seis.surface_from_points(\n    top_hugin_df, \"twt\", right=(\"cdp_x\", \"cdp_y\")\n)\ntop_hugin_ds\n</pre> top_hugin_ds = volve_3d.seis.surface_from_points(     top_hugin_df, \"twt\", right=(\"cdp_x\", \"cdp_y\") ) top_hugin_ds In\u00a0[\u00a0]: Copied! <pre># the twt values from the points now in a form we can relate to the xarray cube.\nplt.imshow(top_hugin_ds.twt)\n</pre> # the twt values from the points now in a form we can relate to the xarray cube. plt.imshow(top_hugin_ds.twt) In\u00a0[\u00a0]: Copied! <pre># point_cloud = pv.StructuredGrid(\n#     top_hugin_ds.cdp_x.values,\n#     top_hugin_ds.cdp_y.values,top_hugin_ds.twt.values,\n#     cmap='viridis')\n# point_cloud.plot(eye_dome_lighting=True)\n</pre> # point_cloud = pv.StructuredGrid( #     top_hugin_ds.cdp_x.values, #     top_hugin_ds.cdp_y.values,top_hugin_ds.twt.values, #     cmap='viridis') # point_cloud.plot(eye_dome_lighting=True) <p>Extracting horizon amplitudes requires us to interpolate the cube onto the 3D horizon.</p> In\u00a0[\u00a0]: Copied! <pre>top_hugin_amp = volve_3d.data.interp(\n    {\"iline\": top_hugin_ds.iline, \"xline\": top_hugin_ds.xline, \"twt\": top_hugin_ds.twt}\n)\n</pre> top_hugin_amp = volve_3d.data.interp(     {\"iline\": top_hugin_ds.iline, \"xline\": top_hugin_ds.xline, \"twt\": top_hugin_ds.twt} ) In\u00a0[\u00a0]: Copied! <pre>#\nfig = plt.figure(figsize=(15, 5))\ntop_hugin_amp.plot(cmap=\"bwr\")\ncs = plt.contour(\n    top_hugin_amp.xline, top_hugin_amp.iline, top_hugin_ds.twt, levels=20, colors=\"grey\"\n)\nplt.clabel(cs, fontsize=14, fmt=\"%.1f\")\n</pre> # fig = plt.figure(figsize=(15, 5)) top_hugin_amp.plot(cmap=\"bwr\") cs = plt.contour(     top_hugin_amp.xline, top_hugin_amp.iline, top_hugin_ds.twt, levels=20, colors=\"grey\" ) plt.clabel(cs, fontsize=14, fmt=\"%.1f\")"},{"location":"examples/example_extract_data_on_a_horizon.html#extract-data-at-the-intersection-of-a-horizon-and-3d-volume","title":"Extract data at the intersection of a horizon and 3D volume\u00b6","text":""},{"location":"examples/example_extract_data_on_a_horizon.html#load-small-3d-volume-from-volve","title":"Load Small 3D Volume from Volve\u00b6","text":""},{"location":"examples/example_extract_data_on_a_horizon.html#load-up-horizon-data","title":"Load up horizon data\u00b6","text":""},{"location":"examples/example_extract_data_on_a_horizon.html#horizon-amplitude-extraction","title":"Horizon Amplitude Extraction\u00b6","text":""},{"location":"examples/example_merge_surveys.html","title":"Merging Seismic Data Cubes","text":"<p>Often we receive seismic data cubes which we wish to merge that have different geometries. This workflow will guide you through the process of merging two such cubes.</p> In\u00a0[\u00a0]: Copied! <pre>from segysak import create3d_dataset\nfrom scipy.optimize import curve_fit\nimport numpy as np\nimport xarray as xr\nimport matplotlib.pyplot as plt\nfrom matplotlib.transforms import Affine2D\nfrom segysak import open_seisnc, create_seismic_dataset\n\nfrom dask.distributed import Client\n\n# start a dask client for processing\nclient = Client()\n</pre> from segysak import create3d_dataset from scipy.optimize import curve_fit import numpy as np import xarray as xr import matplotlib.pyplot as plt from matplotlib.transforms import Affine2D from segysak import open_seisnc, create_seismic_dataset  from dask.distributed import Client  # start a dask client for processing client = Client() <p>Creating some synthetic seismic surveys for us to play with. The online documentation uses a smaller example, but you can stress dask by commenting the dimension lines for smaller data and uncommenting the large desktop size volumes.</p> In\u00a0[\u00a0]: Copied! <pre># large desktop size merge\n# i1, i2, iN = 100, 750, 651\n# x1, x2, xN = 300, 1250, 951\n# t1, t2, tN = 0, 1848, 463\n# trans_x, trans_y = 10000, 2000\n\n# online example size merge\ni1, i2, iN = 650, 750, 101\nx1, x2, xN = 1150, 1250, 101\nt1, t2, tN = 0, 200, 201\ntrans_x, trans_y = 9350, 1680\n\niline = np.linspace(i1, i2, iN, dtype=int)\nxline = np.linspace(x1, x2, xN, dtype=int)\ntwt = np.linspace(t1, t2, tN, dtype=int)\n\naffine_survey1 = Affine2D()\naffine_survey1.rotate_deg(20).translate(trans_x, trans_y)\n\n# create an empty dataset and empty coordinates\nsurvey_1 = create_seismic_dataset(twt=twt, iline=iline, xline=xline)\nsurvey_1[\"cdp_x\"] = ((\"iline\", \"xline\"), np.empty((iN, xN)))\nsurvey_1[\"cdp_y\"] = ((\"iline\", \"xline\"), np.empty((iN, xN)))\n\nstacked_iline = survey_1.iline.broadcast_like(survey_1.cdp_x).stack(\n    {\"ravel\": (..., \"xline\")}\n)\nstacked_xline = survey_1.xline.broadcast_like(survey_1.cdp_x).stack(\n    {\"ravel\": (..., \"xline\")}\n)\n\npoints = np.dstack([stacked_iline, stacked_xline])\npoints_xy = affine_survey1.transform(points[0])\n# put xy back in stacked and unstack into survey_1\nsurvey_1[\"cdp_x\"] = stacked_iline.copy(data=points_xy[:, 0]).unstack()\nsurvey_1[\"cdp_y\"] = stacked_iline.copy(data=points_xy[:, 1]).unstack()\n\n# and the same for survey_2 but with a different affine to survey 1\naffine_survey2 = Affine2D()\naffine_survey2.rotate_deg(120).translate(11000, 3000)\n\nsurvey_2 = create_seismic_dataset(twt=twt, iline=iline, xline=xline)\nsurvey_2[\"cdp_x\"] = ((\"iline\", \"xline\"), np.empty((iN, xN)))\nsurvey_2[\"cdp_y\"] = ((\"iline\", \"xline\"), np.empty((iN, xN)))\n\nstacked_iline = survey_1.iline.broadcast_like(survey_1.cdp_x).stack(\n    {\"ravel\": (..., \"xline\")}\n)\nstacked_xline = survey_1.xline.broadcast_like(survey_1.cdp_x).stack(\n    {\"ravel\": (..., \"xline\")}\n)\n\npoints = np.dstack([stacked_iline, stacked_xline])\npoints_xy = affine_survey2.transform(points[0])\n# put xy back in stacked and unstack into survey_1\nsurvey_2[\"cdp_x\"] = stacked_iline.copy(data=points_xy[:, 0]).unstack()\nsurvey_2[\"cdp_y\"] = stacked_iline.copy(data=points_xy[:, 1]).unstack()\n</pre> # large desktop size merge # i1, i2, iN = 100, 750, 651 # x1, x2, xN = 300, 1250, 951 # t1, t2, tN = 0, 1848, 463 # trans_x, trans_y = 10000, 2000  # online example size merge i1, i2, iN = 650, 750, 101 x1, x2, xN = 1150, 1250, 101 t1, t2, tN = 0, 200, 201 trans_x, trans_y = 9350, 1680  iline = np.linspace(i1, i2, iN, dtype=int) xline = np.linspace(x1, x2, xN, dtype=int) twt = np.linspace(t1, t2, tN, dtype=int)  affine_survey1 = Affine2D() affine_survey1.rotate_deg(20).translate(trans_x, trans_y)  # create an empty dataset and empty coordinates survey_1 = create_seismic_dataset(twt=twt, iline=iline, xline=xline) survey_1[\"cdp_x\"] = ((\"iline\", \"xline\"), np.empty((iN, xN))) survey_1[\"cdp_y\"] = ((\"iline\", \"xline\"), np.empty((iN, xN)))  stacked_iline = survey_1.iline.broadcast_like(survey_1.cdp_x).stack(     {\"ravel\": (..., \"xline\")} ) stacked_xline = survey_1.xline.broadcast_like(survey_1.cdp_x).stack(     {\"ravel\": (..., \"xline\")} )  points = np.dstack([stacked_iline, stacked_xline]) points_xy = affine_survey1.transform(points[0]) # put xy back in stacked and unstack into survey_1 survey_1[\"cdp_x\"] = stacked_iline.copy(data=points_xy[:, 0]).unstack() survey_1[\"cdp_y\"] = stacked_iline.copy(data=points_xy[:, 1]).unstack()  # and the same for survey_2 but with a different affine to survey 1 affine_survey2 = Affine2D() affine_survey2.rotate_deg(120).translate(11000, 3000)  survey_2 = create_seismic_dataset(twt=twt, iline=iline, xline=xline) survey_2[\"cdp_x\"] = ((\"iline\", \"xline\"), np.empty((iN, xN))) survey_2[\"cdp_y\"] = ((\"iline\", \"xline\"), np.empty((iN, xN)))  stacked_iline = survey_1.iline.broadcast_like(survey_1.cdp_x).stack(     {\"ravel\": (..., \"xline\")} ) stacked_xline = survey_1.xline.broadcast_like(survey_1.cdp_x).stack(     {\"ravel\": (..., \"xline\")} )  points = np.dstack([stacked_iline, stacked_xline]) points_xy = affine_survey2.transform(points[0]) # put xy back in stacked and unstack into survey_1 survey_2[\"cdp_x\"] = stacked_iline.copy(data=points_xy[:, 0]).unstack() survey_2[\"cdp_y\"] = stacked_iline.copy(data=points_xy[:, 1]).unstack() <p>Let's check that there is a bit of overlap between the two surveys.</p> In\u00a0[\u00a0]: Copied! <pre># plotting every 10th line\n\nplt.figure(figsize=(10, 10))\nsurvey_1_plot = plt.plot(\n    survey_1.cdp_x.values[::10, ::10], survey_1.cdp_y.values[::10, ::10], color=\"grey\"\n)\nsurvey_2_plot = plt.plot(\n    survey_2.cdp_x.values[::10, ::10], survey_2.cdp_y.values[::10, ::10], color=\"blue\"\n)\n# plt.aspect(\"equal\")\nplt.legend([survey_1_plot[0], survey_2_plot[0]], [\"survey_1\", \"survey_2\"])\n</pre> # plotting every 10th line  plt.figure(figsize=(10, 10)) survey_1_plot = plt.plot(     survey_1.cdp_x.values[::10, ::10], survey_1.cdp_y.values[::10, ::10], color=\"grey\" ) survey_2_plot = plt.plot(     survey_2.cdp_x.values[::10, ::10], survey_2.cdp_y.values[::10, ::10], color=\"blue\" ) # plt.aspect(\"equal\") plt.legend([survey_1_plot[0], survey_2_plot[0]], [\"survey_1\", \"survey_2\"]) <p>Let's output these two datasets to disk so we can use lazy loading from seisnc. If the files are large, it's good practice to do this, because you will probably run out of memory.</p> <p>We are going to fill survey one with a values of 1 and survey 2 with a value of 2, just for simplicity in this example. We also tell the dtype to be <code>np.float32</code> to reduce memory usage.</p> In\u00a0[\u00a0]: Copied! <pre># save out with new geometry\nsurvey_1[\"data\"] = (\n    (\"iline\", \"xline\", \"twt\"),\n    np.full((iN, xN, tN), 1, dtype=np.float32),\n)\nsurvey_1.seisio.to_netcdf(\"data/survey_1.seisnc\")\ndel survey_1\nsurvey_2[\"data\"] = (\n    (\"iline\", \"xline\", \"twt\"),\n    np.full((iN, xN, tN), 2, dtype=np.float32),\n)\nsurvey_2.seisio.to_netcdf(\"data/survey_2.seisnc\")\ndel survey_2\n</pre> # save out with new geometry survey_1[\"data\"] = (     (\"iline\", \"xline\", \"twt\"),     np.full((iN, xN, tN), 1, dtype=np.float32), ) survey_1.seisio.to_netcdf(\"data/survey_1.seisnc\") del survey_1 survey_2[\"data\"] = (     (\"iline\", \"xline\", \"twt\"),     np.full((iN, xN, tN), 2, dtype=np.float32), ) survey_2.seisio.to_netcdf(\"data/survey_2.seisnc\") del survey_2 <p>Let us reimport the surveys we created but in a chunked (lazy) way.</p> In\u00a0[\u00a0]: Copied! <pre>survey_1 = open_seisnc(\"data/survey_1.seisnc\", chunks=dict(iline=10, xline=10, twt=100))\nsurvey_2 = open_seisnc(\"data/survey_2.seisnc\", chunks=dict(iline=10, xline=10, twt=100))\n</pre> survey_1 = open_seisnc(\"data/survey_1.seisnc\", chunks=dict(iline=10, xline=10, twt=100)) survey_2 = open_seisnc(\"data/survey_2.seisnc\", chunks=dict(iline=10, xline=10, twt=100)) <p>Check that the survey is chunked by looking at the printout for our datasets. Lazy and chunked data will have a <code>dask.array&lt;chunksize=</code> where the values are usually displayed.</p> In\u00a0[\u00a0]: Copied! <pre>survey_1\n</pre> survey_1 <p>It will help with our other functions if we load the cdp_x and cdp_y locations into memory. This can be done using the <code>load()</code> method.</p> In\u00a0[\u00a0]: Copied! <pre>survey_1[\"cdp_x\"] = survey_1[\"cdp_x\"].load()\nsurvey_1[\"cdp_y\"] = survey_1[\"cdp_y\"].load()\nsurvey_2[\"cdp_x\"] = survey_2[\"cdp_x\"].load()\nsurvey_2[\"cdp_y\"] = survey_2[\"cdp_y\"].load()\n</pre> survey_1[\"cdp_x\"] = survey_1[\"cdp_x\"].load() survey_1[\"cdp_y\"] = survey_1[\"cdp_y\"].load() survey_2[\"cdp_x\"] = survey_2[\"cdp_x\"].load() survey_2[\"cdp_y\"] = survey_2[\"cdp_y\"].load() In\u00a0[\u00a0]: Copied! <pre>survey_1\n</pre> survey_1 <p>We will also need an affine transform which converts from il/xl on survey 2 to il/xl on survey 1. This will relate the two grids to each other. Fortunately affine transforms can be added together which makes this pretty straight forward.</p> In\u00a0[\u00a0]: Copied! <pre>ilxl2_to_ilxl1 = (\n    survey_2.seis.get_affine_transform()\n    + survey_1.seis.get_affine_transform().inverted()\n)\n</pre> ilxl2_to_ilxl1 = (     survey_2.seis.get_affine_transform()     + survey_1.seis.get_affine_transform().inverted() ) <p>Let's check the transform to see if it makes sense. The colouring of our values by inline should be the same after the transform is applied. Cool.</p> In\u00a0[\u00a0]: Copied! <pre># plotting every 50th line\n\nn = 5\n\nfig, axs = plt.subplots(ncols=2, figsize=(20, 10))\nsurvey_1_plot = axs[0].scatter(\n    survey_1.cdp_x.values[::n, ::n],\n    survey_1.cdp_y.values[::n, ::n],\n    c=survey_1.iline.broadcast_like(survey_1.cdp_x).values[::n, ::n],\n    s=20,\n)\nsurvey_2_plot = axs[0].scatter(\n    survey_2.cdp_x.values[::n, ::n],\n    survey_2.cdp_y.values[::n, ::n],\n    c=survey_2.iline.broadcast_like(survey_2.cdp_x).values[::n, ::n],\n    s=20,\n)\nplt.colorbar(survey_1_plot, ax=axs[0])\naxs[0].set_aspect(\"equal\")\n\naxs[0].set_title(\"inline numbering before transform\")\n\nnew_ilxl = ilxl2_to_ilxl1.transform(\n    np.dstack(\n        [\n            survey_2.iline.broadcast_like(survey_2.cdp_x).values.ravel(),\n            survey_2.xline.broadcast_like(survey_2.cdp_x).values.ravel(),\n        ]\n    )[0]\n).reshape((survey_2.iline.size, survey_2.xline.size, 2))\n\nsurvey_2.seis.calc_corner_points()\ns2_corner_points_in_s1 = ilxl2_to_ilxl1.transform(survey_2.attrs[\"corner_points\"])\n\nprint(\"Min Iline:\", s2_corner_points_in_s1[:, 0].min(), survey_1.iline.min().values)\nmin_iline_combined = min(\n    s2_corner_points_in_s1[:, 0].min(), survey_1.iline.min().values\n)\nprint(\"Max Iline:\", s2_corner_points_in_s1[:, 0].max(), survey_1.iline.max().values)\nmax_iline_combined = max(\n    s2_corner_points_in_s1[:, 0].max(), survey_1.iline.max().values\n)\n\nprint(\"Min Xline:\", s2_corner_points_in_s1[:, 1].min(), survey_1.xline.min().values)\nmin_xline_combined = min(\n    s2_corner_points_in_s1[:, 1].min(), survey_1.xline.min().values\n)\nprint(\"Max Xline:\", s2_corner_points_in_s1[:, 1].max(), survey_1.xline.max().values)\nmax_xline_combined = max(\n    s2_corner_points_in_s1[:, 1].max(), survey_1.xline.max().values\n)\nprint(min_iline_combined, max_iline_combined, min_xline_combined, max_xline_combined)\n\nsurvey_1_plot = axs[1].scatter(\n    survey_1.cdp_x.values[::n, ::n],\n    survey_1.cdp_y.values[::n, ::n],\n    c=survey_1.iline.broadcast_like(survey_1.cdp_x).values[::n, ::n],\n    vmin=min_iline_combined,\n    vmax=max_iline_combined,\n    s=20,\n    cmap=\"hsv\",\n)\n\n\nsurvey_2_plot = axs[1].scatter(\n    survey_2.cdp_x.values[::n, ::n],\n    survey_2.cdp_y.values[::n, ::n],\n    c=new_ilxl[::n, ::n, 0],\n    vmin=min_iline_combined,\n    vmax=max_iline_combined,\n    s=20,\n    cmap=\"hsv\",\n)\nplt.colorbar(survey_1_plot, ax=axs[1])\n\naxs[1].set_title(\"inline numbering after transform\")\naxs[1].set_aspect(\"equal\")\n</pre> # plotting every 50th line  n = 5  fig, axs = plt.subplots(ncols=2, figsize=(20, 10)) survey_1_plot = axs[0].scatter(     survey_1.cdp_x.values[::n, ::n],     survey_1.cdp_y.values[::n, ::n],     c=survey_1.iline.broadcast_like(survey_1.cdp_x).values[::n, ::n],     s=20, ) survey_2_plot = axs[0].scatter(     survey_2.cdp_x.values[::n, ::n],     survey_2.cdp_y.values[::n, ::n],     c=survey_2.iline.broadcast_like(survey_2.cdp_x).values[::n, ::n],     s=20, ) plt.colorbar(survey_1_plot, ax=axs[0]) axs[0].set_aspect(\"equal\")  axs[0].set_title(\"inline numbering before transform\")  new_ilxl = ilxl2_to_ilxl1.transform(     np.dstack(         [             survey_2.iline.broadcast_like(survey_2.cdp_x).values.ravel(),             survey_2.xline.broadcast_like(survey_2.cdp_x).values.ravel(),         ]     )[0] ).reshape((survey_2.iline.size, survey_2.xline.size, 2))  survey_2.seis.calc_corner_points() s2_corner_points_in_s1 = ilxl2_to_ilxl1.transform(survey_2.attrs[\"corner_points\"])  print(\"Min Iline:\", s2_corner_points_in_s1[:, 0].min(), survey_1.iline.min().values) min_iline_combined = min(     s2_corner_points_in_s1[:, 0].min(), survey_1.iline.min().values ) print(\"Max Iline:\", s2_corner_points_in_s1[:, 0].max(), survey_1.iline.max().values) max_iline_combined = max(     s2_corner_points_in_s1[:, 0].max(), survey_1.iline.max().values )  print(\"Min Xline:\", s2_corner_points_in_s1[:, 1].min(), survey_1.xline.min().values) min_xline_combined = min(     s2_corner_points_in_s1[:, 1].min(), survey_1.xline.min().values ) print(\"Max Xline:\", s2_corner_points_in_s1[:, 1].max(), survey_1.xline.max().values) max_xline_combined = max(     s2_corner_points_in_s1[:, 1].max(), survey_1.xline.max().values ) print(min_iline_combined, max_iline_combined, min_xline_combined, max_xline_combined)  survey_1_plot = axs[1].scatter(     survey_1.cdp_x.values[::n, ::n],     survey_1.cdp_y.values[::n, ::n],     c=survey_1.iline.broadcast_like(survey_1.cdp_x).values[::n, ::n],     vmin=min_iline_combined,     vmax=max_iline_combined,     s=20,     cmap=\"hsv\", )   survey_2_plot = axs[1].scatter(     survey_2.cdp_x.values[::n, ::n],     survey_2.cdp_y.values[::n, ::n],     c=new_ilxl[::n, ::n, 0],     vmin=min_iline_combined,     vmax=max_iline_combined,     s=20,     cmap=\"hsv\", ) plt.colorbar(survey_1_plot, ax=axs[1])  axs[1].set_title(\"inline numbering after transform\") axs[1].set_aspect(\"equal\") <p>So we now have a transform that can convert ilxl from <code>survey_2</code> to ilxl from <code>survey_1</code> and we need to create a combined geometry for the merge. The dims need to cover the maximum range (inline, crossline) of two surveys. We are also going to use survey 1 as the base as we don't want to have to resample two cubes, although if you have a prefered geometry you could do that (custom affine transforms per survey required).</p> In\u00a0[\u00a0]: Copied! <pre># create new dims\niline_step = survey_1.iline.diff(\"iline\").mean().values\nxline_step = survey_1.xline.diff(\"xline\").mean().values\n\n# create the new dimensions\nnew_iline_dim = np.arange(\n    int(min_iline_combined // iline_step) * iline_step,\n    int(max_iline_combined) + iline_step * 2,\n    iline_step,\n    dtype=np.int32,\n)\nnew_xline_dim = np.arange(\n    int(min_xline_combined // xline_step) * xline_step,\n    int(max_xline_combined) + xline_step * 2,\n    xline_step,\n    dtype=np.int32,\n)\n\n# create a new empty dataset and a blank cdp_x for dims broadcasting\nsurvey_comb = create_seismic_dataset(\n    iline=new_iline_dim, xline=new_xline_dim, twt=survey_1.twt\n)\nsurvey_comb[\"cdp_x\"] = (\n    (\"iline\", \"xline\"),\n    np.empty((new_iline_dim.size, new_xline_dim.size)),\n)\n\n# calculate the x and y using the survey 1 affine which is our base grid and reshape to the dataset grid\nnew_cdp_xy = affine_survey1.transform(\n    np.dstack(\n        [\n            survey_comb.iline.broadcast_like(survey_comb.cdp_x).values.ravel(),\n            survey_comb.xline.broadcast_like(survey_comb.cdp_x).values.ravel(),\n        ]\n    )[0]\n)\nnew_cdp_xy_grid = new_cdp_xy.reshape((new_iline_dim.size, new_xline_dim.size, 2))\n\n# plot to check\nplt.figure(figsize=(10, 10))\n\nsurvey_comb_plot = plt.plot(\n    new_cdp_xy_grid[:, :, 0], new_cdp_xy_grid[:, :, 1], color=\"red\", alpha=0.5\n)\nsurvey_1_plot = plt.plot(\n    survey_1.cdp_x.values[::10, ::10], survey_1.cdp_y.values[::10, ::10], color=\"grey\"\n)\nsurvey_2_plot = plt.plot(\n    survey_2.cdp_x.values[::10, ::10], survey_2.cdp_y.values[::10, ::10], color=\"blue\"\n)\n\nplt.legend(\n    [survey_1_plot[0], survey_2_plot[0], survey_comb_plot[0]],\n    [\"survey_1\", \"survey_2\", \"survey_merged\"],\n)\n\n# put the new x and y in the empty dataset\nsurvey_comb[\"cdp_x\"] = ((\"iline\", \"xline\"), new_cdp_xy_grid[..., 0])\nsurvey_comb[\"cdp_y\"] = ((\"iline\", \"xline\"), new_cdp_xy_grid[..., 1])\nsurvey_comb = survey_comb.set_coords((\"cdp_x\", \"cdp_y\"))\n</pre> # create new dims iline_step = survey_1.iline.diff(\"iline\").mean().values xline_step = survey_1.xline.diff(\"xline\").mean().values  # create the new dimensions new_iline_dim = np.arange(     int(min_iline_combined // iline_step) * iline_step,     int(max_iline_combined) + iline_step * 2,     iline_step,     dtype=np.int32, ) new_xline_dim = np.arange(     int(min_xline_combined // xline_step) * xline_step,     int(max_xline_combined) + xline_step * 2,     xline_step,     dtype=np.int32, )  # create a new empty dataset and a blank cdp_x for dims broadcasting survey_comb = create_seismic_dataset(     iline=new_iline_dim, xline=new_xline_dim, twt=survey_1.twt ) survey_comb[\"cdp_x\"] = (     (\"iline\", \"xline\"),     np.empty((new_iline_dim.size, new_xline_dim.size)), )  # calculate the x and y using the survey 1 affine which is our base grid and reshape to the dataset grid new_cdp_xy = affine_survey1.transform(     np.dstack(         [             survey_comb.iline.broadcast_like(survey_comb.cdp_x).values.ravel(),             survey_comb.xline.broadcast_like(survey_comb.cdp_x).values.ravel(),         ]     )[0] ) new_cdp_xy_grid = new_cdp_xy.reshape((new_iline_dim.size, new_xline_dim.size, 2))  # plot to check plt.figure(figsize=(10, 10))  survey_comb_plot = plt.plot(     new_cdp_xy_grid[:, :, 0], new_cdp_xy_grid[:, :, 1], color=\"red\", alpha=0.5 ) survey_1_plot = plt.plot(     survey_1.cdp_x.values[::10, ::10], survey_1.cdp_y.values[::10, ::10], color=\"grey\" ) survey_2_plot = plt.plot(     survey_2.cdp_x.values[::10, ::10], survey_2.cdp_y.values[::10, ::10], color=\"blue\" )  plt.legend(     [survey_1_plot[0], survey_2_plot[0], survey_comb_plot[0]],     [\"survey_1\", \"survey_2\", \"survey_merged\"], )  # put the new x and y in the empty dataset survey_comb[\"cdp_x\"] = ((\"iline\", \"xline\"), new_cdp_xy_grid[..., 0]) survey_comb[\"cdp_y\"] = ((\"iline\", \"xline\"), new_cdp_xy_grid[..., 1]) survey_comb = survey_comb.set_coords((\"cdp_x\", \"cdp_y\")) In\u00a0[\u00a0]: Copied! <pre>survey_2_new_ilxl_loc = affine_survey2.inverted().transform(new_cdp_xy)\n</pre> survey_2_new_ilxl_loc = affine_survey2.inverted().transform(new_cdp_xy) <p>We then need to create a sampling DataArray. If we passed the iline and cross line locations from the previous cell unfortunately Xarray would broadcast the inline and xline values against each other. The simplest way is to create a new stacked flat dimension which we can unstack into a cube later.</p> <p>We also need to give it unique names so xarray doesn't confuse the new dimensions with the old dimensions.</p> In\u00a0[\u00a0]: Copied! <pre>flat = (\n    survey_comb.rename(  # use the combined survey\n        {\"iline\": \"new_iline\", \"xline\": \"new_xline\"}\n    )  # renaming inline and xline so they don't confuse xarray\n    .stack(\n        {\"flat\": (\"new_iline\", \"new_xline\")}\n    )  # and flatten the iline and xline axes using stack\n    .flat  # return just the flat coord object which is multi-index (so we can unstack later)\n)\nflat\n</pre> flat = (     survey_comb.rename(  # use the combined survey         {\"iline\": \"new_iline\", \"xline\": \"new_xline\"}     )  # renaming inline and xline so they don't confuse xarray     .stack(         {\"flat\": (\"new_iline\", \"new_xline\")}     )  # and flatten the iline and xline axes using stack     .flat  # return just the flat coord object which is multi-index (so we can unstack later) ) flat In\u00a0[\u00a0]: Copied! <pre># create a dictionary of new coordinates to sample to. We use flat here because it will easily allow us to\n# unstack the data after interpolation. It's necessary to use this flat dataset, otherwise xarray will try to\n# interpolate on the cartesian product of iline and xline, which isn't really what we want.\n\nresampling_data_arrays = dict(\n    iline=xr.DataArray(survey_2_new_ilxl_loc[:, 0], dims=\"flat\", coords={\"flat\": flat}),\n    xline=xr.DataArray(survey_2_new_ilxl_loc[:, 1], dims=\"flat\", coords={\"flat\": flat}),\n)\n\nresampling_data_arrays\n</pre> # create a dictionary of new coordinates to sample to. We use flat here because it will easily allow us to # unstack the data after interpolation. It's necessary to use this flat dataset, otherwise xarray will try to # interpolate on the cartesian product of iline and xline, which isn't really what we want.  resampling_data_arrays = dict(     iline=xr.DataArray(survey_2_new_ilxl_loc[:, 0], dims=\"flat\", coords={\"flat\": flat}),     xline=xr.DataArray(survey_2_new_ilxl_loc[:, 1], dims=\"flat\", coords={\"flat\": flat}), )  resampling_data_arrays <p>The resampling can take a while with larger cubes, so it is good to use dask and to output the cube to disk at this step. When using dask for processing it is better to output to disk regularly, as this will improve how your code runs and the overall memory usage.</p> <p>If you are executing this exmample locally, the task progress can be viewed by opening the dask client.</p> In\u00a0[\u00a0]: Copied! <pre>survey_2_resamp = survey_2.interp(**resampling_data_arrays)\nsurvey_2_resamp_newgeom = (\n    survey_2_resamp.drop_vars((\"iline\", \"xline\"))\n    .unstack(\"flat\")\n    .rename({\"new_iline\": \"iline\", \"new_xline\": \"xline\"})\n    .data\n)\nsurvey_2_resamp_newgeom.to_netcdf(\"data/survey_2_1.nc\", compute=True, engine=\"h5netcdf\")\n</pre> survey_2_resamp = survey_2.interp(**resampling_data_arrays) survey_2_resamp_newgeom = (     survey_2_resamp.drop_vars((\"iline\", \"xline\"))     .unstack(\"flat\")     .rename({\"new_iline\": \"iline\", \"new_xline\": \"xline\"})     .data ) survey_2_resamp_newgeom.to_netcdf(\"data/survey_2_1.nc\", compute=True, engine=\"h5netcdf\") In\u00a0[\u00a0]: Copied! <pre>survey_2_resamp_newgeom = xr.open_dataarray(\n    \"data/survey_2_1.nc\", chunks=dict(iline=10, xline=10, twt=100), engine=\"h5netcdf\"\n)\n</pre> survey_2_resamp_newgeom = xr.open_dataarray(     \"data/survey_2_1.nc\", chunks=dict(iline=10, xline=10, twt=100), engine=\"h5netcdf\" ) In\u00a0[\u00a0]: Copied! <pre>survey_2_resamp_newgeom.expand_dims({\"survey\": [2]})\n</pre> survey_2_resamp_newgeom.expand_dims({\"survey\": [2]}) In\u00a0[\u00a0]: Copied! <pre># concatenate survey with new dimension \"survey\".\nsurvey_comb[\"data\"] = xr.concat([survey_1.data, survey_2_resamp_newgeom], \"survey\")\n</pre> # concatenate survey with new dimension \"survey\". survey_comb[\"data\"] = xr.concat([survey_1.data, survey_2_resamp_newgeom], \"survey\") In\u00a0[\u00a0]: Copied! <pre>survey_comb\n</pre> survey_comb <p>We can check the merged surveys by looking at some plots. If we select just the first survey the values will be 1. If we select just the second survey the values will be 2. And if we take the mean along the survey dimension, then where the surveys overlap the values will be 1.5.</p> <p>For seismic data, a better form of conditioning and reduction might be required for merging traces together to ensure a smoother seam.</p> In\u00a0[\u00a0]: Copied! <pre>sel = survey_comb.sel(xline=1190)\n\nfig, axs = plt.subplots(ncols=3, figsize=(30, 10))\n\nsel.isel(survey=0).data.T.plot(ax=axs[0], yincrease=False, vmin=1, vmax=2)\nsel.isel(survey=1).data.T.plot(ax=axs[1], yincrease=False, vmin=1, vmax=2)\nsel.data.mean(\"survey\").T.plot(ax=axs[2], yincrease=False, vmin=1, vmax=2)\n</pre> sel = survey_comb.sel(xline=1190)  fig, axs = plt.subplots(ncols=3, figsize=(30, 10))  sel.isel(survey=0).data.T.plot(ax=axs[0], yincrease=False, vmin=1, vmax=2) sel.isel(survey=1).data.T.plot(ax=axs[1], yincrease=False, vmin=1, vmax=2) sel.data.mean(\"survey\").T.plot(ax=axs[2], yincrease=False, vmin=1, vmax=2) In\u00a0[\u00a0]: Copied! <pre>survey_comb.twt\n</pre> survey_comb.twt <p>And if we inspect a time slice.</p> In\u00a0[\u00a0]: Copied! <pre>sel = survey_comb.sel(twt=100)\n\nfig, axs = plt.subplots(ncols=3, figsize=(30, 10))\n\nsel.isel(survey=0).data.T.plot(ax=axs[0], yincrease=False, vmin=1, vmax=2)\nsel.isel(survey=1).data.T.plot(ax=axs[1], yincrease=False, vmin=1, vmax=2)\nsel.data.mean(\"survey\").T.plot(ax=axs[2], yincrease=False, vmin=1, vmax=2)\n</pre> sel = survey_comb.sel(twt=100)  fig, axs = plt.subplots(ncols=3, figsize=(30, 10))  sel.isel(survey=0).data.T.plot(ax=axs[0], yincrease=False, vmin=1, vmax=2) sel.isel(survey=1).data.T.plot(ax=axs[1], yincrease=False, vmin=1, vmax=2) sel.data.mean(\"survey\").T.plot(ax=axs[2], yincrease=False, vmin=1, vmax=2) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/example_merge_surveys.html#merging-seismic-data-cubes","title":"Merging Seismic Data Cubes\u00b6","text":""},{"location":"examples/example_merge_surveys.html#how-to-merge-two-different-geometries","title":"How to merge two different geometries\u00b6","text":""},{"location":"examples/example_merge_surveys.html#resampling-survey_2-to-survey_1-geometry","title":"Resampling <code>survey_2</code> to <code>survey_1</code> geometry\u00b6","text":"<p>Resampling one dataset to another requires us to tell Xarray/dask where we want the new traces to be. First we can convert the x and y coordinates of the combined survey to the il xl of survey 2 using the affine transform of survey 2. This transform works il/xl to x and y and therefore we need it inverted.</p>"},{"location":"examples/example_merge_surveys.html#combining-cubes","title":"Combining Cubes\u00b6","text":"<p>The last step is combining the cube is to load the two datasets to be combined and concatenate them together along a new axis. This will simplify reduction processes later.</p>"},{"location":"examples/example_segy_headers.html","title":"Working with SEG-Y headers","text":"In\u00a0[\u00a0]: Copied! <pre>import warnings\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import warnings  warnings.filterwarnings(\"ignore\") In\u00a0[\u00a0]: Copied! <pre>from segysak.segy import segy_header_scan\n\n# default just needs the file name\nscan = segy_header_scan(\"data/volve10r12-full-twt-sub3d.sgy\")\nscan\n</pre> from segysak.segy import segy_header_scan  # default just needs the file name scan = segy_header_scan(\"data/volve10r12-full-twt-sub3d.sgy\") scan <p>If you want to see the full DataFrame in a notebook, use the <code>pandas</code> options context manager.</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom IPython.display import display\n\nwith pd.option_context(\"display.max_rows\", 89):\n    display(scan)\n</pre> import pandas as pd from IPython.display import display  with pd.option_context(\"display.max_rows\", 89):     display(scan) <p>Often lots of header fields don't get filled, so lets filter by the standard deviation column <code>std</code>. In fact, there are so few here we don't need the context manager. As you can see, for <code>segy_loader</code> or <code>segy_converter</code> we will need to tell those functions that the byte location for iline and xline are 189 and 193 respectively, and the byte locations for cdp_x and cdp_y are either 73 and 77 or 181 and 185 which are identical pairs.</p> In\u00a0[\u00a0]: Copied! <pre># NIIIICCCEEEE...\nscan[scan[\"std\"] &gt; 0]\n</pre> # NIIIICCCEEEE... scan[scan[\"std\"] &gt; 0] In\u00a0[\u00a0]: Copied! <pre>from segysak.segy import segy_header_scrape\n\nscrape = segy_header_scrape(\"data/volve10r12-full-twt-sub3d.sgy\", partial_scan=10000)\nscrape\n</pre> from segysak.segy import segy_header_scrape  scrape = segy_header_scrape(\"data/volve10r12-full-twt-sub3d.sgy\", partial_scan=10000) scrape <p>We know from the scan that many of these fields were empty so lets go ahead and filter our scrape by using the standard deviation again and passing the index which is the same as our column names.</p> In\u00a0[\u00a0]: Copied! <pre>scrape = scrape[scan[scan[\"std\"] &gt; 0].index]\nscrape\n</pre> scrape = scrape[scan[scan[\"std\"] &gt; 0].index] scrape <p>We know from the scan that many of these fields were empty so lets go ahead and filter our scrape by using the standard deviation again and passing the index which is the same as our column names.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>plot = scrape.hist(bins=25, figsize=(20, 10))\n</pre> plot = scrape.hist(bins=25, figsize=(20, 10)) <p>We can also just plot up the geometry to check that everything looks ok, here the line numbering and coordinates seem to match up, great!</p> In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(nrows=2, figsize=(12, 10), sharex=True, sharey=True)\n\nscrape.plot(\n    kind=\"scatter\", x=\"CDP_X\", y=\"CDP_Y\", c=\"INLINE_3D\", ax=axs[0], cmap=\"gist_ncar\"\n)\nscrape.plot(\n    kind=\"scatter\", x=\"CDP_X\", y=\"CDP_Y\", c=\"CROSSLINE_3D\", ax=axs[1], cmap=\"gist_ncar\"\n)\nfor aa in axs:\n    aa.set_aspect(\"equal\", \"box\")\n</pre> fig, axs = plt.subplots(nrows=2, figsize=(12, 10), sharex=True, sharey=True)  scrape.plot(     kind=\"scatter\", x=\"CDP_X\", y=\"CDP_Y\", c=\"INLINE_3D\", ax=axs[0], cmap=\"gist_ncar\" ) scrape.plot(     kind=\"scatter\", x=\"CDP_X\", y=\"CDP_Y\", c=\"CROSSLINE_3D\", ax=axs[1], cmap=\"gist_ncar\" ) for aa in axs:     aa.set_aspect(\"equal\", \"box\")"},{"location":"examples/example_segy_headers.html#working-with-seg-y-headers","title":"Working with SEG-Y headers\u00b6","text":"<p>Headers in SEG-Y data are additional meta information associated with each trace. In SEG-Y these are not pooled in a common data block but interleaved with the seismic trace data so we need to do some work to extract it. segysak has two helper methods for extracting information from a SEG-Y file. These are <code>segy_header_scan</code> and <code>segy_header_scrape</code>. Both of these functions return <code>pandas.DataFrame</code> objects containing header or header related information which can be used in QC, analysis and plotting.</p>"},{"location":"examples/example_segy_headers.html#scanning-the-headers","title":"Scanning the headers\u00b6","text":"<p><code>segy_header_scan</code> is primarily designed to help quickly asscertain the byte locations of key header information for loading or converting the full SEG-Y file. It does this by just looking at the first N traces (1000 by default) and returns the byte location and statistics related to the file.</p>"},{"location":"examples/example_segy_headers.html#scraping-headers","title":"Scraping Headers\u00b6","text":"<p>Scraping the header works like a scan but instead of statistics we get a DataFrame of actual trace header values. You can reduce the size of the scan by using the partial_scan keyword if required. The index of the DataFrame is the trace index and the columns are the header fields.</p>"},{"location":"examples/example_segysak_basics.html","title":"SEGY-SAK Basics","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\n%matplotlib inline\n\nfrom segysak import create3d_dataset\n\n# dims iline, xline, vert\ndims = (10, 5, 1000)\n\nnew_seisnc = create3d_dataset(\n    dims,\n    first_iline=1,\n    iline_step=2,\n    first_xline=10,\n    xline_step=1,\n    first_sample=0,\n    sample_rate=1,\n    vert_domain=\"TWT\",\n)\nnew_seisnc\n</pre> import matplotlib.pyplot as plt  %matplotlib inline  from segysak import create3d_dataset  # dims iline, xline, vert dims = (10, 5, 1000)  new_seisnc = create3d_dataset(     dims,     first_iline=1,     iline_step=2,     first_xline=10,     xline_step=1,     first_sample=0,     sample_rate=1,     vert_domain=\"TWT\", ) new_seisnc In\u00a0[\u00a0]: Copied! <pre># select iline value 9\nxarray_selection = new_seisnc.sel(iline=9)\n# select xline value 12\nxarray_selection = new_seisnc.sel(xline=12)\n# select iline and xline intersection point\nxarray_selection = new_seisnc.sel(iline=9, xline=12)\n# key error\n# xarray_selection = new_seisnc.sel(twt=8.5)\n# select nearest twt slice\nxarray_selection = new_seisnc.sel(twt=8.5, method=\"nearest\")\n# select a range\nxarray_selection = new_seisnc.sel(iline=[9, 11, 13])\n# select a subcube\n# also slices can be used to select ranges as if they were indices!\nxarray_selection = new_seisnc.sel(iline=slice(9, 13), xline=[10, 11, 12])\n# index selection principles are similar\nxarray_selection = new_seisnc.sel(iline=slice(1, 4))\n\n# putting it altogether to extract a sub-cropped horizon slice at odd interval\nxarray_selection = new_seisnc.sel(\n    twt=8.5, iline=new_seisnc.iline[:4], xline=[10, 12], method=\"nearest\"\n)\nxarray_selection\n</pre> # select iline value 9 xarray_selection = new_seisnc.sel(iline=9) # select xline value 12 xarray_selection = new_seisnc.sel(xline=12) # select iline and xline intersection point xarray_selection = new_seisnc.sel(iline=9, xline=12) # key error # xarray_selection = new_seisnc.sel(twt=8.5) # select nearest twt slice xarray_selection = new_seisnc.sel(twt=8.5, method=\"nearest\") # select a range xarray_selection = new_seisnc.sel(iline=[9, 11, 13]) # select a subcube # also slices can be used to select ranges as if they were indices! xarray_selection = new_seisnc.sel(iline=slice(9, 13), xline=[10, 11, 12]) # index selection principles are similar xarray_selection = new_seisnc.sel(iline=slice(1, 4))  # putting it altogether to extract a sub-cropped horizon slice at odd interval xarray_selection = new_seisnc.sel(     twt=8.5, iline=new_seisnc.iline[:4], xline=[10, 12], method=\"nearest\" ) xarray_selection In\u00a0[\u00a0]: Copied! <pre># get the dims\ndims = new_seisnc.sizes\nprint(dims)\ndkeys = (\"xline\", \"iline\", \"twt\")\ndsize = [dims[key] for key in dkeys]\nprint(\"keys:\", dkeys, \"sizes:\", dsize)\n</pre> # get the dims dims = new_seisnc.sizes print(dims) dkeys = (\"xline\", \"iline\", \"twt\") dsize = [dims[key] for key in dkeys] print(\"keys:\", dkeys, \"sizes:\", dsize) In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\n# create some data and the dimension shapes\nxline_, iline_, twt_ = np.meshgrid(new_seisnc.iline, new_seisnc.xline, new_seisnc.twt)\ndata = np.sin(twt_ / 100) + 2 * iline_ * np.cos(xline_ / 20 + 10)\n\n# assign the data to dataset by passing in a tuple of the dimension keys and the new data\nnew_seisnc[\"data\"] = (dkeys, data)\n\nfig, axs = plt.subplots(ncols=3, figsize=(15, 5))\n\n# axes are wrong for seismic\nnew_seisnc.data.sel(iline=7).plot(ax=axs[0])\n\n# rotate the cube\nnew_seisnc.data.transpose(\"twt\", \"iline\", \"xline\").sel(iline=7).plot(\n    ax=axs[1], yincrease=False\n)\n\n# xline is the same?\nnew_seisnc.data.transpose(\"twt\", \"iline\", \"xline\").isel(xline=2).plot(\n    ax=axs[2], yincrease=False\n)\n</pre> import numpy as np  # create some data and the dimension shapes xline_, iline_, twt_ = np.meshgrid(new_seisnc.iline, new_seisnc.xline, new_seisnc.twt) data = np.sin(twt_ / 100) + 2 * iline_ * np.cos(xline_ / 20 + 10)  # assign the data to dataset by passing in a tuple of the dimension keys and the new data new_seisnc[\"data\"] = (dkeys, data)  fig, axs = plt.subplots(ncols=3, figsize=(15, 5))  # axes are wrong for seismic new_seisnc.data.sel(iline=7).plot(ax=axs[0])  # rotate the cube new_seisnc.data.transpose(\"twt\", \"iline\", \"xline\").sel(iline=7).plot(     ax=axs[1], yincrease=False )  # xline is the same? new_seisnc.data.transpose(\"twt\", \"iline\", \"xline\").isel(xline=2).plot(     ax=axs[2], yincrease=False ) <p>Accessing the coordinates or data as a <code>numpy</code> array.</p> In\u00a0[\u00a0]: Copied! <pre># Data can be dropped out of xarray but you then need to manage the coordinates and the dimension\n# labels\n\nnew_seisnc.iline.values\n</pre> # Data can be dropped out of xarray but you then need to manage the coordinates and the dimension # labels  new_seisnc.iline.values <p><code>xarray.Dataset</code> and <code>DataArray</code> have lots of great built in methods in addition to plot.</p> In\u00a0[\u00a0]: Copied! <pre>print(new_seisnc.data.mean())\n</pre> print(new_seisnc.data.mean()) In\u00a0[\u00a0]: Copied! <pre>print(new_seisnc.data.max())\n</pre> print(new_seisnc.data.max()) <p><code>numpy</code> functions also work on <code>xarray</code> objects but this returns a new <code>DataArray</code> not a <code>ndarray</code>.</p> In\u00a0[\u00a0]: Copied! <pre>print(np.sum(new_seisnc.data))\n</pre> print(np.sum(new_seisnc.data)) <p>With <code>xarray</code> you can apply operations along 1 or more dimensions to reduce the dataset. This could be useful for collapsing gathers for example by applying the mean along the <code>offset</code> dimension. Here we combine a <code>numpy</code> operation <code>abs</code> which returns an <code>DataArray</code> and then sum along the time dimension to create a grid without the time dimension. Along with using masks this is a fundamental building block for performing horizonal sculpting.</p> In\u00a0[\u00a0]: Copied! <pre>map_data = np.abs(new_seisnc.data).sum(dim=\"twt\")\nimg = map_data.plot()\n</pre> map_data = np.abs(new_seisnc.data).sum(dim=\"twt\") img = map_data.plot() <p>Sometimes we need to modify the dimensions because they were read wrong or to scale them. Modify your dimension from the seisnc and then put it back using <code>assign_coords</code>.</p> In\u00a0[\u00a0]: Copied! <pre>new_seisnc.assign_coords(iline=new_seisnc.iline * 10, twt=new_seisnc.twt + 1500)\n</pre> new_seisnc.assign_coords(iline=new_seisnc.iline * 10, twt=new_seisnc.twt + 1500)"},{"location":"examples/example_segysak_basics.html#segy-sak-basics","title":"SEGY-SAK Basics\u00b6","text":"<p>segysak offers a number of utilities to create and load seismic data using <code>xarray</code> and <code>segyio</code>. In general segysak uses <code>xarray.Dataset</code> to store the data and provides an interface to additional seismic specific functionality by adding the <code>.seis</code> and <code>.seisio</code> names-spaces to an <code>xarray.Dataset</code> (just <code>dataset</code> from now on). That sounds complicated but let us walk through some examples together.</p>"},{"location":"examples/example_segysak_basics.html#creating-empty-3d-geometry","title":"Creating empty 3D geometry\u00b6","text":"<p>In segysak we use the term <code>seisnc</code> to refer to a <code>dataset</code> which is compatible with segysak's functionality and which has the additional names spaces registered with <code>xarray</code>, for all intensive purposes it is an <code>xarray.Dataset</code> but with defined dimensions and coordinates and some extended functionality. The <code>seisnc</code> dimensions are defined depending on what type of seismic it is (2D, 3D, gathers, etc.)</p> <p>To create an empty 3D instance of <code>seisnc</code> use the <code>create3d_dataset</code>. The function creates a new <code>seisnc</code> based upon definitions for the dimensions, <code>iline</code> numbering, <code>xline</code> numbering and the vertical sampling.</p>"},{"location":"examples/example_segysak_basics.html#dimension-based-selection-and-transformation","title":"Dimension based selection and transformation\u00b6","text":"<p>As you can see from the print out of the previous cell, we have three dimensions in this dataset. They are <code>iline</code>, <code>xline</code> and <code>twt</code> (although the order, number and names might change depending on the make up of our volume). The ordering isn't import to <code>xarray</code> because it uses labels, and accessing data is done using these labels rather than indexing directly into the data like <code>numpy</code>. <code>xarray</code> also makes it further convenient by allowing us to select based on the dimension values using the <code>.sel</code> method with tools for selecting nearest or ranges as well. If necessary you can also select by index using the <code>.isel</code> method.</p>"},{"location":"examples/example_segysak_basics.html#coordinates-selection","title":"Coordinates Selection\u00b6","text":"<p>Usually for seismic the X and Y coordinates labelled <code>cdp_x</code> and <code>cdp_y</code> in seisnc are rotated and scaled relative to the grid geometry and now seisnc dimensions <code>iline</code>, <code>xline</code> and <code>twt</code>. For <code>xarray</code> this means you cannot use the <code>.sel</code> and <code>.isel</code> methods to select data for <code>cdp_x</code> and <code>cdp_y</code>. segysak is developing more natural interfaces to access data using X and Y coordinates and this is available through the <code>seisnc.seis</code> namespace, covered in other examples.</p>"},{"location":"examples/example_segysak_basics.html#adding-data-to-an-empty-seisnc","title":"Adding data to an empty seisnc\u00b6","text":"<p>Because <code>xarray</code> needs to understand the dimensions of any data you assign it must be explicitly communicated either via labels or creating an <code>xarray.DataArray</code> first.</p>"},{"location":"examples/example_segysak_basics.html#other-useful-methods","title":"Other Useful Methods\u00b6","text":""},{"location":"examples/example_segysak_dask.html","title":"Using dask","text":"In\u00a0[\u00a0]: Copied! <pre>import warnings\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import warnings  warnings.filterwarnings(\"ignore\") In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom segysak import open_seisnc, segy\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import numpy as np from segysak import open_seisnc, segy  import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>from dask.distributed import Client\n\nclient = Client()\nclient\n</pre> from dask.distributed import Client  client = Client() client <p>We can also scale the cluster to be a bit smaller.</p> In\u00a0[\u00a0]: Copied! <pre>client.cluster.scale(2, memory=\"0.5gb\")\nclient\n</pre> client.cluster.scale(2, memory=\"0.5gb\") client In\u00a0[\u00a0]: Copied! <pre>segy_file = \"data/volve10r12-full-twt-sub3d.sgy\"\nseisnc_file = \"data/volve10r12-full-twt-sub3d.seisnc\"\nsegy.segy_converter(\n    segy_file, seisnc_file, iline=189, xline=193, cdp_x=181, cdp_y=185, silent=True\n)\n</pre> segy_file = \"data/volve10r12-full-twt-sub3d.sgy\" seisnc_file = \"data/volve10r12-full-twt-sub3d.seisnc\" segy.segy_converter(     segy_file, seisnc_file, iline=189, xline=193, cdp_x=181, cdp_y=185, silent=True ) <p>By specifying the chunks argument to the <code>open_seisnc</code> command we can ask dask to fetch the data in chunks of size n. In this example the <code>iline</code> dimension will be chunked in groups of 100. The valid arguments to chunks depends on the dataset but any dimension can be used.</p> <p>Even though the seis of the dataset is <code>2.14GB</code> it hasn't yet been loaded into memory, not will <code>dask</code> load it entirely unless the operation demands it.</p> In\u00a0[\u00a0]: Copied! <pre>seisnc = open_seisnc(\"data/volve10r12-full-twt-sub3d.seisnc\", chunks={\"iline\": 100})\nseisnc.seis.humanbytes\n</pre> seisnc = open_seisnc(\"data/volve10r12-full-twt-sub3d.seisnc\", chunks={\"iline\": 100}) seisnc.seis.humanbytes <p>Lets see what our dataset looks like. See that the variables are <code>dask.array</code>. This means they are references to the on disk data. The dimensions must be loaded so <code>dask</code> knows how to manage your dataset.</p> In\u00a0[\u00a0]: Copied! <pre>seisnc\n</pre> seisnc In\u00a0[\u00a0]: Copied! <pre>mean = seisnc.data.mean()\nmean\n</pre> mean = seisnc.data.mean() mean <p>Whoa-oh, the mean is what? Yeah, <code>dask</code> won't calculate anything until you ask it to. This means you can string computations together into a task graph for lazy evaluation. To get the mean try this</p> In\u00a0[\u00a0]: Copied! <pre>mean.compute().values\n</pre> mean.compute().values In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))\n\niline = seisnc.sel(iline=10100).transpose(\"twt\", \"xline\").data\nxline = seisnc.sel(xline=2349).transpose(\"twt\", \"iline\").data\nzslice = seisnc.sel(twt=2900, method=\"nearest\").transpose(\"iline\", \"xline\").data\n\nq = iline.quantile([0, 0.001, 0.5, 0.999, 1]).values\nrq = np.max(np.abs([q[1], q[-2]]))\n\niline.plot(robust=True, ax=axs[0, 0], yincrease=False)\nxline.plot(robust=True, ax=axs[0, 1], yincrease=False)\nzslice.plot(robust=True, ax=axs[0, 2])\n\nimshow_kwargs = dict(\n    cmap=\"seismic\", aspect=\"auto\", vmin=-rq, vmax=rq, interpolation=\"bicubic\"\n)\n\naxs[1, 0].imshow(iline.values, **imshow_kwargs)\naxs[1, 0].set_title(\"iline\")\naxs[1, 1].imshow(xline.values, **imshow_kwargs)\naxs[1, 1].set_title(\"xline\")\naxs[1, 2].imshow(zslice.values, origin=\"lower\", **imshow_kwargs)\naxs[1, 2].set_title(\"twt\")\n</pre> fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))  iline = seisnc.sel(iline=10100).transpose(\"twt\", \"xline\").data xline = seisnc.sel(xline=2349).transpose(\"twt\", \"iline\").data zslice = seisnc.sel(twt=2900, method=\"nearest\").transpose(\"iline\", \"xline\").data  q = iline.quantile([0, 0.001, 0.5, 0.999, 1]).values rq = np.max(np.abs([q[1], q[-2]]))  iline.plot(robust=True, ax=axs[0, 0], yincrease=False) xline.plot(robust=True, ax=axs[0, 1], yincrease=False) zslice.plot(robust=True, ax=axs[0, 2])  imshow_kwargs = dict(     cmap=\"seismic\", aspect=\"auto\", vmin=-rq, vmax=rq, interpolation=\"bicubic\" )  axs[1, 0].imshow(iline.values, **imshow_kwargs) axs[1, 0].set_title(\"iline\") axs[1, 1].imshow(xline.values, **imshow_kwargs) axs[1, 1].set_title(\"xline\") axs[1, 2].imshow(zslice.values, origin=\"lower\", **imshow_kwargs) axs[1, 2].set_title(\"twt\")"},{"location":"examples/example_segysak_dask.html#using-dask","title":"Using dask\u00b6","text":"<p>dask is a Python package built upon the scientific stack to enable scalling of Python through interactive sessions to multi-core and multi-node.</p> <p>Of particular relevance to SEGY-SAK is that <code>xrray.Dataset</code> loads naturally into <code>dask</code>.</p>"},{"location":"examples/example_segysak_dask.html#imports-and-setup","title":"Imports and Setup\u00b6","text":"<p>Here we import the plotting tools, <code>numpy</code> and setup the <code>dask.Client</code> which will auto start a <code>localcluster</code>. Printing the client returns details about the dashboard link and resources.</p>"},{"location":"examples/example_segysak_dask.html#lazy-loading-from-seisnc-using-chunking","title":"Lazy loading from SEISNC using chunking\u00b6","text":"<p>If your data is in SEG-Y to use dask it must be converted to SEISNC. If you do this with the CLI it only need happen once.</p>"},{"location":"examples/example_segysak_dask.html#operations-on-seisnc-using-dask","title":"Operations on SEISNC using <code>dask</code>\u00b6","text":"<p>In this simple example we calculate the mean, of the entire cube. If you check the dashboard (when running this example yourself). You can see the task graph and task stream execution.</p>"},{"location":"examples/example_segysak_dask.html#plotting-with-dask","title":"Plotting with <code>dask</code>\u00b6","text":"<p>The lazy loading of data means we can plot what we want using <code>xarray</code> style slicing and <code>dask</code> will fetch only the data we need.</p>"},{"location":"examples/example_segysak_segy_vectorisation.html","title":"SEG-Y to Vector DataFrames and Back","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre>import pathlib\nfrom IPython.display import display\nfrom segysak.segy import segy_loader, well_known_byte_locs, segy_writer\n\nvolve_3d_path = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\")\nprint(\"3D\", volve_3d_path.exists())\n\nvolve_3d = segy_loader(volve_3d_path, **well_known_byte_locs(\"petrel_3d\"))\n</pre> import pathlib from IPython.display import display from segysak.segy import segy_loader, well_known_byte_locs, segy_writer  volve_3d_path = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\") print(\"3D\", volve_3d_path.exists())  volve_3d = segy_loader(volve_3d_path, **well_known_byte_locs(\"petrel_3d\")) In\u00a0[\u00a0]: Copied! <pre>volve_3d_df = volve_3d.to_dataframe()\ndisplay(volve_3d_df)\n</pre> volve_3d_df = volve_3d.to_dataframe() display(volve_3d_df) <p>We can remove the multi-index by resetting the index of the DataFrame. Vectorized workflows such as machine learning can then be easily applied to the DataFrame.</p> In\u00a0[\u00a0]: Copied! <pre>volve_3d_df_reindex = volve_3d_df.reset_index()\ndisplay(volve_3d_df_reindex)\n</pre> volve_3d_df_reindex = volve_3d_df.reset_index() display(volve_3d_df_reindex) In\u00a0[\u00a0]: Copied! <pre>volve_3d_df_multi = volve_3d_df_reindex.set_index([\"iline\", \"xline\", \"twt\"])\ndisplay(volve_3d_df_multi)\nvolve_3d_ds = volve_3d_df_multi.to_xarray()\ndisplay(volve_3d_ds)\n</pre> volve_3d_df_multi = volve_3d_df_reindex.set_index([\"iline\", \"xline\", \"twt\"]) display(volve_3d_df_multi) volve_3d_ds = volve_3d_df_multi.to_xarray() display(volve_3d_ds) <p>The resulting dataset requires some changes to make it compatible again for export to SEGY. Firstly, the attributes need to be set. The simplest way is to copy these from the original SEG-Y input. Otherwise they can be set manually. <code>segysak</code> specifically needs the <code>sample_rate</code> and the <code>coord_scalar</code> attributes.</p> In\u00a0[\u00a0]: Copied! <pre>volve_3d_ds.attrs = volve_3d.attrs\ndisplay(volve_3d_ds.attrs)\n</pre> volve_3d_ds.attrs = volve_3d.attrs display(volve_3d_ds.attrs) <p>The <code>cdp_x</code> and <code>cdp_y</code> positions must be reduced to 2D along the vertical axis \"twt\" and set as coordinates.</p> In\u00a0[\u00a0]: Copied! <pre>volve_3d_ds[\"cdp_x\"] = volve_3d_ds[\"cdp_x\"].mean(dim=[\"twt\"])\nvolve_3d_ds[\"cdp_y\"] = volve_3d_ds[\"cdp_y\"].mean(dim=[\"twt\"])\nvolve_3d_ds = volve_3d_ds.set_coords([\"cdp_x\", \"cdp_y\"])\nvolve_3d_ds\n</pre> volve_3d_ds[\"cdp_x\"] = volve_3d_ds[\"cdp_x\"].mean(dim=[\"twt\"]) volve_3d_ds[\"cdp_y\"] = volve_3d_ds[\"cdp_y\"].mean(dim=[\"twt\"]) volve_3d_ds = volve_3d_ds.set_coords([\"cdp_x\", \"cdp_y\"]) volve_3d_ds <p>Afterwards, use the <code>segy_writer</code> utility as normal to return to SEGY.</p> In\u00a0[\u00a0]: Copied! <pre>segy_writer(volve_3d_ds, \"test.segy\")\n</pre> segy_writer(volve_3d_ds, \"test.segy\")"},{"location":"examples/example_segysak_segy_vectorisation.html#seg-y-to-vector-dataframes-and-back","title":"SEG-Y to Vector DataFrames and Back\u00b6","text":"<p>The connection of segysak to <code>xarray</code> greatly simplifies the process of vectorising segy 3D data and returning it to SEGY. To do this, one can use the close relationship between <code>pandas</code> and <code>xarray</code>.</p>"},{"location":"examples/example_segysak_segy_vectorisation.html#loading-data","title":"Loading Data\u00b6","text":"<p>We start by loading data normally using the <code>segy_loader</code> utility. For this example we will use the Volve example sub-cube.</p>"},{"location":"examples/example_segysak_segy_vectorisation.html#vectorisation","title":"Vectorisation\u00b6","text":"<p>Once the data is loaded it can be converted to a <code>pandas.DataFrame</code> directly from the loaded <code>Dataset</code>. The Dataframe is multi-index and contains columns for each variable in the originally loaded dataset. This includes the seismic amplitude as <code>data</code> and the <code>cdp_x</code> and <code>cdp_y</code> locations. If you require smaller volumes from the input data, you can use xarray selection methods prior to conversion to a DataFrame.</p>"},{"location":"examples/example_segysak_segy_vectorisation.html#return-to-xarray","title":"Return to Xarray\u00b6","text":"<p>It is possible to return the DataFrame to the Dataset for output to SEGY. To do this the multi-index must be reset. Afterward, <code>pandas</code> provides the <code>to_xarray</code> method.</p>"},{"location":"examples/example_working_with_3d_gathers.html","title":"Working with 3D Gathers","text":"In\u00a0[\u00a0]: Copied! <pre>import pathlib\nfrom IPython.display import display\nimport pandas as pd\nimport xarray as xr\nimport numpy as np\nfrom segysak.segy import segy_loader, segy_header_scan\nimport matplotlib.pyplot as plt\n</pre> import pathlib from IPython.display import display import pandas as pd import xarray as xr import numpy as np from segysak.segy import segy_loader, segy_header_scan import matplotlib.pyplot as plt <p>This example uses a subset of the Penobscot 3D with data exported from the OpendTect project.</p> <p>First we scan the data to determine which byte locations contain the relevant information. We will need to provide a byte location for the offset variable so a 4th dimension can be created when loaded the data.</p> In\u00a0[\u00a0]: Copied! <pre>segy_file = pathlib.Path(\"data/3D_gathers_pstm_nmo.sgy\")\nwith pd.option_context(\"display.max_rows\", 100):\n    display(segy_header_scan(segy_file))\n</pre> segy_file = pathlib.Path(\"data/3D_gathers_pstm_nmo.sgy\") with pd.option_context(\"display.max_rows\", 100):     display(segy_header_scan(segy_file)) In\u00a0[\u00a0]: Copied! <pre>penobscot_3d_gath = segy_loader(\n    segy_file, iline=189, xline=193, cdp_x=181, cdp_y=185, offset=37\n)\n</pre> penobscot_3d_gath = segy_loader(     segy_file, iline=189, xline=193, cdp_x=181, cdp_y=185, offset=37 ) <p>Note that the loaded Dataset has four dimensions with the additional dimension labeled offset. There are 61 offsets in this dataset or 61 traces per inline and xline location.</p> In\u00a0[\u00a0]: Copied! <pre>display(penobscot_3d_gath)\nprint(penobscot_3d_gath.offset.values)\n</pre> display(penobscot_3d_gath) print(penobscot_3d_gath.offset.values) <p>Lets check that the data looks OK for a couple of offsets. We've only got a small dataset of 11x11 traces so the seismic will look at little odd at this scale.</p> In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(ncols=2, figsize=(20, 10))\n\npenobscot_3d_gath.isel(iline=5, offset=0).data.T.plot(\n    yincrease=False, ax=axs[0], vmax=5000\n)\npenobscot_3d_gath.isel(xline=5, offset=0).data.T.plot(\n    yincrease=False, ax=axs[1], vmax=5000\n)\n</pre> fig, axs = plt.subplots(ncols=2, figsize=(20, 10))  penobscot_3d_gath.isel(iline=5, offset=0).data.T.plot(     yincrease=False, ax=axs[0], vmax=5000 ) penobscot_3d_gath.isel(xline=5, offset=0).data.T.plot(     yincrease=False, ax=axs[1], vmax=5000 ) In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(figsize=(20, 10))\naxs.imshow(\n    penobscot_3d_gath.isel(iline=0)\n    .data.stack(stacked_offset=(\"xline\", \"offset\"))\n    .values,\n    vmin=-5000,\n    vmax=5000,\n    cmap=\"seismic\",\n    aspect=\"auto\",\n)\n</pre> fig, axs = plt.subplots(figsize=(20, 10)) axs.imshow(     penobscot_3d_gath.isel(iline=0)     .data.stack(stacked_offset=(\"xline\", \"offset\"))     .values,     vmin=-5000,     vmax=5000,     cmap=\"seismic\",     aspect=\"auto\", ) <p>One can easily create a common offset stack by reversing the stacked dimension arguments <code>\"offset\"</code> and <code>\"xline\"</code>.</p> In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(figsize=(20, 10))\naxs.imshow(\n    penobscot_3d_gath.isel(iline=0)\n    .data.stack(stacked_offset=(\"offset\", \"xline\"))\n    .values,\n    vmin=-5000,\n    vmax=5000,\n    cmap=\"seismic\",\n    aspect=\"auto\",\n)\n</pre> fig, axs = plt.subplots(figsize=(20, 10)) axs.imshow(     penobscot_3d_gath.isel(iline=0)     .data.stack(stacked_offset=(\"offset\", \"xline\"))     .values,     vmin=-5000,     vmax=5000,     cmap=\"seismic\",     aspect=\"auto\", ) In\u00a0[\u00a0]: Copied! <pre>arb_line = np.array([(733600, 733850), (4895180.0, 4895180.0)])\n\nax = penobscot_3d_gath.seis.plot_bounds()\nax.plot(arb_line[0, :], arb_line[1, :], label=\"arb_line\")\nplt.legend()\n</pre> arb_line = np.array([(733600, 733850), (4895180.0, 4895180.0)])  ax = penobscot_3d_gath.seis.plot_bounds() ax.plot(arb_line[0, :], arb_line[1, :], label=\"arb_line\") plt.legend() <p>Here we need to think carefully about the <code>bin_spacing_hint</code>. We also don't want to interpolate the gathers, so we use <code>xysel_method=\"nearest\"</code>.</p> In\u00a0[\u00a0]: Copied! <pre>penobscot_3d_gath_arb = penobscot_3d_gath.seis.interp_line(\n    arb_line[0, :], arb_line[1, :], bin_spacing_hint=30, xysel_method=\"nearest\"\n)\n</pre> penobscot_3d_gath_arb = penobscot_3d_gath.seis.interp_line(     arb_line[0, :], arb_line[1, :], bin_spacing_hint=30, xysel_method=\"nearest\" ) In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(figsize=(20, 10))\naxs.imshow(\n    penobscot_3d_gath_arb.data.stack(\n        stacked_offset=(\n            \"cdp\",\n            \"offset\",\n        )\n    ).values,\n    vmin=-5000,\n    vmax=5000,\n    cmap=\"seismic\",\n    aspect=\"auto\",\n)\n</pre> fig, axs = plt.subplots(figsize=(20, 10)) axs.imshow(     penobscot_3d_gath_arb.data.stack(         stacked_offset=(             \"cdp\",             \"offset\",         )     ).values,     vmin=-5000,     vmax=5000,     cmap=\"seismic\",     aspect=\"auto\", ) In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(ncols=2, figsize=(20, 10))\n\npenobscot_3d_gath.isel(iline=5, xline=0).data.plot(\n    yincrease=False, ax=axs[0], vmax=5000\n)\n\n# the mute relates the offset to an expected twt, let's just use a linear mute for this example\nmute = penobscot_3d_gath.offset * 0.6 + 300\n# and then we can plot it up\nmute.plot(ax=axs[0], color=\"k\")\n# apply the mute to the volume\npenobscot_3d_gath_muted = penobscot_3d_gath.where(penobscot_3d_gath.twt &gt; mute)\n\n# muted\npenobscot_3d_gath_muted.isel(iline=5, xline=0).data.plot(\n    yincrease=False, ax=axs[1], vmax=5000\n)\n</pre> fig, axs = plt.subplots(ncols=2, figsize=(20, 10))  penobscot_3d_gath.isel(iline=5, xline=0).data.plot(     yincrease=False, ax=axs[0], vmax=5000 )  # the mute relates the offset to an expected twt, let's just use a linear mute for this example mute = penobscot_3d_gath.offset * 0.6 + 300 # and then we can plot it up mute.plot(ax=axs[0], color=\"k\") # apply the mute to the volume penobscot_3d_gath_muted = penobscot_3d_gath.where(penobscot_3d_gath.twt &gt; mute)  # muted penobscot_3d_gath_muted.isel(iline=5, xline=0).data.plot(     yincrease=False, ax=axs[1], vmax=5000 ) <p>Stacking is the process of averaging the gathers for constant time to create a single trace per inline and crossline location.</p> In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(ncols=3, figsize=(20, 10))\n\nplot_kwargs = dict(vmax=5000, interpolation=\"bicubic\", yincrease=False)\n\n# compare with the zero offset trace and use imshow for interpolation\npenobscot_3d_gath.isel(iline=5, offset=0).data.T.plot.imshow(ax=axs[0], **plot_kwargs)\n\n# stack the no mute data\npenobscot_3d_gath.isel(iline=5).data.mean(\"offset\").T.plot.imshow(\n    ax=axs[1], **plot_kwargs\n)\n\n# stack the muted data\npenobscot_3d_gath_muted.isel(iline=5).data.mean(\"offset\").T.plot.imshow(\n    ax=axs[2], **plot_kwargs\n)\n</pre> fig, axs = plt.subplots(ncols=3, figsize=(20, 10))  plot_kwargs = dict(vmax=5000, interpolation=\"bicubic\", yincrease=False)  # compare with the zero offset trace and use imshow for interpolation penobscot_3d_gath.isel(iline=5, offset=0).data.T.plot.imshow(ax=axs[0], **plot_kwargs)  # stack the no mute data penobscot_3d_gath.isel(iline=5).data.mean(\"offset\").T.plot.imshow(     ax=axs[1], **plot_kwargs )  # stack the muted data penobscot_3d_gath_muted.isel(iline=5).data.mean(\"offset\").T.plot.imshow(     ax=axs[2], **plot_kwargs ) In\u00a0[\u00a0]: Copied! <pre>!pip list\n</pre> !pip list In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/example_working_with_3d_gathers.html#working-with-3d-gathers","title":"Working with 3D Gathers\u00b6","text":"<p>Gathers or pre-stack data are common in seismic interpretation and processing. In this example we will load gather data by finding and specifying the offset byte location. Learn different ways to plot and select offset data. As well as perform a simple mute and trace stack to reduce the offset dimension.</p>"},{"location":"examples/example_working_with_3d_gathers.html#plotting-gathers-sequentially","title":"Plotting Gathers Sequentially\u00b6","text":"<p>Plotting of gathers is often done in a stacked way, displaying sequential gathers along a common dimension, usually inline or crossline. Xarray provides the <code>stack</code> method which can be used to stack labelled dimensions together.</p>"},{"location":"examples/example_working_with_3d_gathers.html#arbitrary-line-extraction-on-gathers","title":"Arbitrary line extraction on Gathers\u00b6","text":"<p>Arbitrary line slicing of gathers based upon coordinates is also possible. Lets create a line that crosses the 3D.</p>"},{"location":"examples/example_working_with_3d_gathers.html#muting-and-stacking-gathers","title":"Muting and Stacking Gathers\u00b6","text":"<p>Using one of our gathers let's define a mute function before we stack the data.</p>"},{"location":"meta/faq.html","title":"FAQ","text":""},{"location":"meta/faq.html#i-have-a-hdf5-version-conflict-error","title":"I have a HDF5 version conflict error","text":"<p>If you are in a conda environment this can occur when conflicts arrise from the installed netCDF4 binaries and your system binaries. We suggest you try updating the library with your distribution package manager. Re-creating your conda environment or trying to reinstall the netCDF4 related packages.</p>"},{"location":"meta/faq.html#how-big-can-my-input-seg-y-file-be","title":"How big can my input SEG-Y file be","text":"<p>For practical purposes SEGY-SAK is a designed as a desktop focussed tool for files that fit into memory. Files on the order of 10s of Gb can be reliably loaded into memory these days. For files greater than the amount of memory available the <code>segy_convert</code> function should be used to convert directly to NETCDF4. Conversion of very large files can be slow but they can then be lazily loaded using <code>Xarray</code> and <code>dask</code>.</p>"},{"location":"meta/faq.html#why-dont-you-use-the-global-coordinates-for-dimensions","title":"Why don't you use the global coordinates for dimensions","text":"<p>Xarray requires that our dimensions be orthogonal to each other. Often seismic data is rotated relative to the global cartesian grid and therefore it is not orthogonal any more. To get around this users of seismic data regularly work with the local seismic grid defined by the inline, crossline and vertical directions. This local grid is linked to the global coordinate system through an affine transform.</p>"},{"location":"LICENSE.html","title":"LICENSE","text":"<pre><code>                GNU GENERAL PUBLIC LICENSE\n                   Version 3, 29 June 2007\n</code></pre> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/  Everyone is permitted to copy and distribute verbatim copies  of this license document, but changing it is not allowed.</p> <pre><code>                        Preamble\n</code></pre> <p>The GNU General Public License is a free, copyleft license for software and other kinds of works.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works.  By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.  We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors.  You can apply it to your programs, too.</p> <p>When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights.  Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.</p> <p>For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received.  You must make sure that they, too, receive or can get the source code.  And you must show them these terms so they know their rights.</p> <p>Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.</p> <p>For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software.  For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.</p> <p>Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so.  This is fundamentally incompatible with the aim of protecting users' freedom to change the software.  The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable.  Therefore, we have designed this version of the GPL to prohibit the practice for those products.  If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.</p> <p>Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary.  To prevent this, the GPL assures that patents cannot be used to render the program non-free.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p> <pre><code>                   TERMS AND CONDITIONS\n</code></pre> <ol> <li>Definitions.</li> </ol> <p>\"This License\" refers to version 3 of the GNU General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License.  Each licensee is addressed as \"you\".  \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy.  The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy.  Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies.  Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License.  If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p> <ol> <li>Source Code.</li> </ol> <p>The \"source code\" for a work means the preferred form of the work for making modifications to it.  \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form.  A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.  However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work.  For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p> <ol> <li>Basic Permissions.</li> </ol> <p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met.  This License explicitly affirms your unlimited permission to run the unmodified Program.  The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work.  This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force.  You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright.  Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below.  Sublicensing is not allowed; section 10 makes it unnecessary.</p> <ol> <li>Protecting Users' Legal Rights From Anti-Circumvention Law.</li> </ol> <p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p> <ol> <li>Conveying Verbatim Copies.</li> </ol> <p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p> <ol> <li>Conveying Modified Source Versions.</li> </ol> <p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <pre><code>a) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\n</code></pre> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.  Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p> <ol> <li>Conveying Non-Source Forms.</li> </ol> <p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <pre><code>a) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\n</code></pre> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling.  In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage.  For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product.  A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source.  The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information.  But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed.  Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p> <ol> <li>Additional Terms.</li> </ol> <p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law.  If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it.  (Additional permissions may be written to require their own removal in certain cases when you modify the work.)  You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <pre><code>a) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\n</code></pre> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10.  If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.  If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p> <ol> <li>Termination.</li> </ol> <p>You may not propagate or modify a covered work except as expressly provided under this License.  Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License.  If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p> <ol> <li>Acceptance Not Required for Having Copies.</li> </ol> <p>You are not required to accept this License in order to receive or run a copy of the Program.  Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance.  However, nothing other than this License grants you permission to propagate or modify any covered work.  These actions infringe copyright if you do not accept this License.  Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p> <ol> <li>Automatic Licensing of Downstream Recipients.</li> </ol> <p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License.  You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations.  If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.  For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p> <ol> <li>Patents.</li> </ol> <p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based.  The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version.  For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement).  To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients.  \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License.  You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p> <ol> <li>No Surrender of Others' Freedom.</li> </ol> <p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all.  For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p> <ol> <li>Use with the GNU Affero General Public License.</li> </ol> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work.  The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.</p> <ol> <li>Revised Versions of this License.</li> </ol> <p>The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number.  If the Program specifies that a certain numbered version of the GNU General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions.  However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p> <ol> <li>Disclaimer of Warranty.</li> </ol> <p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> <ol> <li>Limitation of Liability.</li> </ol> <p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p> <ol> <li>Interpretation of Sections 15 and 16.</li> </ol> <p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <pre><code>                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\n</code></pre> <p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program.  It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:</p> <pre><code>&lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\nThis program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; type `show c' for details.\n</code></pre> <p>The hypothetical commands <code>show w' and</code>show c' should show the appropriate parts of the General Public License.  Of course, your program's commands might be different; for a GUI interface, you would use an \"about box\".</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see https://www.gnu.org/licenses/.</p> <p>The GNU General Public License does not permit incorporating your program into proprietary programs.  If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library.  If this is what you want to do, use the GNU Lesser General Public License instead of this License.  But first, please read https://www.gnu.org/licenses/why-not-lgpl.html.</p>"},{"location":"code_of_conduct.html","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct.html#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code_of_conduct.html#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct.html#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct.html#scope","title":"Scope","text":"<p>This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct.html#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at segysak. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"code_of_conduct.html#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/\u00bc/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>"}]}