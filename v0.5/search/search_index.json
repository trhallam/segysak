{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"SEGY-SAK","text":"<p>SEGY-SAK: A library for loading and manipulating SEG-Y data with Python using Xarray</p> <p>SEGY-SAK can be use as a tool to handle SEG-Y files inside Python environment.</p> <p>By including  SEGY-SAK in your toolbox you will be able to load or transform the original binary SEG-Y data into more accessible and Python friendly formats. It leverages the work of  Segyio to simplify loading of common SEG-Y formats into <code>xarray.Dataset</code> objects for ease of use and to NetCDF4 files for better on disk  and large file performance using Dask. Tools to help users create new volumes and to return data to SEG-Y are also included.</p>"},{"location":"index.html#features","title":"Features","text":"<p>Here are some of the features of SEGY-SAK</p> <ul> <li> <p> Loading and Writing SEG-Y data</p> <p>Conveniently load or write all types of SEG-Y data into or from an easy to use Xarray Dataset.</p> <ul> <li>Any number of dimensions.</li> <li>Data with with missing traces.</li> </ul> <p> Example</p> </li> <li> <p> Interact with SEG-Y headers and text</p> <ul> <li>Scan or extract the trace headers.</li> <li>Extract the file text header.</li> </ul> <p> Example</p> </li> <li> <p> Select and Extract</p> <ul> <li>Label based slicing (iline, xline, sample)</li> <li>Arbitrary line slicing</li> <li>Horizon amplitude extraction and sculpting</li> <li>Well path extraction</li> </ul> <p> Slicing, arbitrary lines</p> <p> Horizon extraction and sculpting</p> </li> <li> <p> Seismic Geometry Tools</p> <ul> <li>Generate cube affine transform</li> <li>Fill missing trace X and Y coords.</li> </ul> </li> <li> <p> Scale with the Python stack</p> <ul> <li>Dask integration</li> <li>Lazy loading of SEG-Y by default.</li> </ul> <p> Dask example</p> </li> <li> <p> Help</p> <ul> <li>Questions on the discussion board</li> <li>Submit a bug</li> </ul> </li> </ul>"},{"location":"index.html#see-also","title":"See also","text":"<p>Fundamental Python libraries to SEGY-SAK are Segyio  and Xarray.</p> <p>Many of the examples in this documentation use a subset of the the full Volve dataset which was published by Equinor and you can read about it or get a copy of it here.</p>"},{"location":"index.html#license","title":"License","text":"<p>Segysak use the GPL-3 license.</p> <p>The GNU General Public License is a free, copyleft license for software and other kinds of works.</p>"},{"location":"index.html#citations","title":"Citations","text":"<p>If you use this software, please cite it.</p> APA LikeBibtex Text Only<pre><code>Hallam A. SEGY-SAK URL: https://trhallam.github.io/segysak/\n</code></pre> Text Only<pre><code>@misc{YourReferenceHere,\nauthor = {Hallam, Antony},\ntitle = {SEGY-SAK},\nurl = {https://trhallam.github.io/segysak/}\n}\n</code></pre>"},{"location":"LICENSE.html","title":"LICENSE","text":"<pre><code>                GNU GENERAL PUBLIC LICENSE\n                   Version 3, 29 June 2007\n</code></pre> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/  Everyone is permitted to copy and distribute verbatim copies  of this license document, but changing it is not allowed.</p> <pre><code>                        Preamble\n</code></pre> <p>The GNU General Public License is a free, copyleft license for software and other kinds of works.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works.  By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.  We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors.  You can apply it to your programs, too.</p> <p>When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights.  Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.</p> <p>For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received.  You must make sure that they, too, receive or can get the source code.  And you must show them these terms so they know their rights.</p> <p>Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.</p> <p>For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software.  For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.</p> <p>Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so.  This is fundamentally incompatible with the aim of protecting users' freedom to change the software.  The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable.  Therefore, we have designed this version of the GPL to prohibit the practice for those products.  If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.</p> <p>Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary.  To prevent this, the GPL assures that patents cannot be used to render the program non-free.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p> <pre><code>                   TERMS AND CONDITIONS\n</code></pre> <ol> <li>Definitions.</li> </ol> <p>\"This License\" refers to version 3 of the GNU General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License.  Each licensee is addressed as \"you\".  \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy.  The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy.  Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies.  Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License.  If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p> <ol> <li>Source Code.</li> </ol> <p>The \"source code\" for a work means the preferred form of the work for making modifications to it.  \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form.  A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.  However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work.  For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p> <ol> <li>Basic Permissions.</li> </ol> <p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met.  This License explicitly affirms your unlimited permission to run the unmodified Program.  The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work.  This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force.  You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright.  Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below.  Sublicensing is not allowed; section 10 makes it unnecessary.</p> <ol> <li>Protecting Users' Legal Rights From Anti-Circumvention Law.</li> </ol> <p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p> <ol> <li>Conveying Verbatim Copies.</li> </ol> <p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p> <ol> <li>Conveying Modified Source Versions.</li> </ol> <p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <pre><code>a) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\n</code></pre> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.  Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p> <ol> <li>Conveying Non-Source Forms.</li> </ol> <p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <pre><code>a) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\n</code></pre> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling.  In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage.  For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product.  A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source.  The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information.  But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed.  Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p> <ol> <li>Additional Terms.</li> </ol> <p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law.  If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it.  (Additional permissions may be written to require their own removal in certain cases when you modify the work.)  You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <pre><code>a) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\n</code></pre> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10.  If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.  If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p> <ol> <li>Termination.</li> </ol> <p>You may not propagate or modify a covered work except as expressly provided under this License.  Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License.  If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p> <ol> <li>Acceptance Not Required for Having Copies.</li> </ol> <p>You are not required to accept this License in order to receive or run a copy of the Program.  Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance.  However, nothing other than this License grants you permission to propagate or modify any covered work.  These actions infringe copyright if you do not accept this License.  Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p> <ol> <li>Automatic Licensing of Downstream Recipients.</li> </ol> <p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License.  You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations.  If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.  For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p> <ol> <li>Patents.</li> </ol> <p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based.  The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version.  For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement).  To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients.  \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License.  You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p> <ol> <li>No Surrender of Others' Freedom.</li> </ol> <p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all.  For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p> <ol> <li>Use with the GNU Affero General Public License.</li> </ol> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work.  The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.</p> <ol> <li>Revised Versions of this License.</li> </ol> <p>The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number.  If the Program specifies that a certain numbered version of the GNU General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions.  However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p> <ol> <li>Disclaimer of Warranty.</li> </ol> <p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> <ol> <li>Limitation of Liability.</li> </ol> <p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p> <ol> <li>Interpretation of Sections 15 and 16.</li> </ol> <p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <pre><code>                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\n</code></pre> <p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program.  It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:</p> <pre><code>&lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\nThis program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; type `show c' for details.\n</code></pre> <p>The hypothetical commands <code>show w' and</code>show c' should show the appropriate parts of the General Public License.  Of course, your program's commands might be different; for a GUI interface, you would use an \"about box\".</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see https://www.gnu.org/licenses/.</p> <p>The GNU General Public License does not permit incorporating your program into proprietary programs.  If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library.  If this is what you want to do, use the GNU Lesser General Public License instead of this License.  But first, please read https://www.gnu.org/licenses/why-not-lgpl.html.</p>"},{"location":"api.html","title":"API reference","text":"<p>This set of pages provides an auto-generated summary of the <code>segysak</code> package API. For more details and examples, refer to the relevant examples.</p>"},{"location":"contributing.html","title":"Contributing","text":"<p>Thank you for considering contributing to our project!</p> <p>This is a community-driven project, so it's people like you that make it useful and successful. These are some of the many ways to contribute:</p> <ul> <li>Submitting bug reports and feature requests</li> <li>Writing tutorials or examples</li> <li>Fixing typos and improving the documentation</li> <li>Writing code for everyone to use</li> </ul> <p>If you get stuck at any point you can create an issue on GitHub (look for the Issues tab in the repository) or contact us at one of the other channels mentioned below.</p> <p>For more information on contributing to open source projects, GitHub's own guide is a great starting point if you are new to version control.</p> <p>Also, checkout the Zen of Scientific Software Maintenance for some guiding principles on how to create high quality scientific software contributions.</p>"},{"location":"contributing.html#ground-rules","title":"Ground Rules","text":"<p>The goal is to maintain a diverse community that's pleasant for everyone. Please be considerate and respectful of others. Everyone must abide by our Code of Conduct and we encourage all to read it carefully.</p>"},{"location":"contributing.html#what-can-i-do","title":"What Can I Do?","text":"<ul> <li>Tackle any issue that you wish! Some issues are labelled as \"good first issues\" to   indicate that they are beginner friendly, meaning that they don't require extensive   knowledge of the project.</li> <li>Make a tutorial or example of how to do something.</li> <li>Provide feedback about how we can improve the project or about your particular use   case.</li> <li>Contribute code you already have. It doesn't need to be perfect! We will help you   clean things up, test it, etc.</li> </ul>"},{"location":"contributing.html#how-can-i-talk-to-you","title":"How Can I Talk to You?","text":"<p>Discussion often happens in the repository issues tracker and individual pull requests. For questions and general chat, please use the Discussions board.</p>"},{"location":"contributing.html#reporting-a-bug","title":"Reporting a Bug","text":"<p>Find the Issues tab on the top of the Github repository and click New Issue. You'll be prompted to choose between different types of issue, like bug reports and feature requests. Choose the one that best matches your need. The Issue will be populated with one of our templates. Please try to fillout the template with as much detail as you can.</p> <p>Tip</p> <p>Remember: the more information we have, the easier it will be for us to solve your problem.</p>"},{"location":"contributing.html#editing-the-documentation","title":"Editing the Documentation","text":"<p>If you're browsing the documentation and notice a typo or something that could be improved, please consider letting us know by creating an issue (see reporting_a_bug) or submitting a fix (even better).</p> <p>You can submit fixes to the documentation pages completely online without having to download and install anything:</p> <ul> <li>On each documentation page, there should be an \"edit on Github\" link at the very   top.</li> <li>Click on that link to open the respective source file (usually an <code>.rst</code> file in the   <code>doc</code> folder) on Github for editing online (you'll need a Github account).</li> <li>Make your desired changes.</li> <li>When you're done, scroll to the bottom of the page.</li> <li>Fill out the two fields under \"Commit changes\": the first is a short title describing   your fixes; the second is a more detailed description of the changes. Try to be as   detailed as possible and describe why you changed something.</li> <li>Click on the \"Commit changes\" button to open a   pull request (see below).</li> <li>We'll review your changes and then merge them in if everything is OK.</li> <li>Done</li> </ul> <p>Alternatively, you can make the changes offline to the files in the <code>doc</code> folder or the example scripts. See Contributing Code for instructions.</p>"},{"location":"contributing.html#contributing-code","title":"Contributing Code","text":"<p>Is this your first contribution? Please take a look at these resources to learn about git and pull requests don't hesitate to ask questions:</p> <ul> <li>How to Contribute to Open Source</li> <li>Aaron Meurer's tutorial on the git workflow</li> <li>How to Contribute to an Open Source Project on GitHub</li> </ul>"},{"location":"contributing.html#general-guidelines","title":"General guidelines","text":"<p>We follow the git pull request workflow  to make changes to our codebase. Every change made goes through a pull request, even our own, so that our continuous integration services have a change to check that the code is up to standards and passes all our tests. This way, the master branch is always stable.</p> <p>General guidelines for pull requests (PRs):</p> <ul> <li>Open an issue first describing what you want to do. If there is already an issue     that matches your PR, leave a comment there instead to let us know what you plan to     do.</li> <li>Each pull request should consist of a small and logical collection of changes.</li> <li>Larger changes should be broken down into smaller components and integrated     separately.</li> <li>Bug fixes should be submitted in separate PRs.</li> <li>Describe what your PR changes and why this is a good thing. Be as specific as you     can. The PR description is how we keep track of the changes made to the project over     time.</li> <li>Do not commit changes to files that are irrelevant to your feature or bugfix (eg:     <code>.gitignore</code>, IDE project files, etc).</li> <li>Write descriptive commit messages. Chris Beams has written a     guide on how to write good commit     messages.</li> <li>Be willing to accept criticism and work on improving your code; we don't want to break     other users' code, so care must be taken not to introduce bugs.</li> <li>Be aware that the pull request review process is not immediate, and is generally     proportional to the size of the pull request.</li> </ul>"},{"location":"contributing.html#setting-up-your-environment","title":"Setting up your environment","text":"<p>This project uses the Hatch python project tool. Hatch takes care of Python versions and dependencies (as defined in the <code>pyproject.toml</code>) and development environments.</p> <p>Follow the hatch instructions for installing <code>hatch</code>  on your system.</p> <p>To run tests on your system in the default Python version</p> Bash<pre><code>hatch test\n</code></pre> <p>This installs the virtual environment, installs the project in development mode and runs the test suite. Under the hood the command uses the <code>pytest</code> package. If you need to modify the testing you can use the <code>hatch-test</code> environment and <code>pytest</code> command directly.</p> Bash<pre><code>hatch run hatch-test.py3.12:pytest --help\n</code></pre>"},{"location":"contributing.html#code-style","title":"Code style","text":"<p>We use Black to format the code so we don't have to think about it. Black loosely follows the PEP8 guide but with a few differences. Regardless, you won't have to worry about formatting the code yourself.</p> <p>Don't worry if you forget to do it. Our continuous integration systems will warn us and you can make  a new commit with the formatted code.</p> <p>We also use flake8 and  pylint to check the quality of the code and quickly catch common errors.</p>"},{"location":"contributing.html#docstrings","title":"Docstrings","text":"<p>All docstrings should follow the Google Style Guide. All functions/classes/methods should have docstrings with a full description of all arguments and return values.</p> <p>While the maximum line length for code is automatically set by Black, docstrings must be formatted manually. To play nicely with Jupyter and IPython, keep docstrings limited to 79 characters per line. We don't have a good way of enforcing this automatically yet, so please do your best.</p>"},{"location":"contributing.html#testing-your-code","title":"Testing your code","text":"<p>Automated testing helps ensure that our code is as free of bugs as it can be. It also lets us know immediately if a change we make breaks any other part of the code.</p> <p>All of our test code and data are stored in the <code>tests</code> subpackage. We use the pytest framework to run the test suite.</p> <p>Please write tests for your code so that we can be sure that it won't break any of the existing functionality. Tests also help us be confident that we won't break your code in the future.</p> <p>If you're new to testing, see existing test files for examples of things to do. Don't let the tests keep you from submitting your contribution! If you're not sure how to do this or are having trouble, submit your pull request anyway. We will help you create the tests and sort out any kind of problem during code review.</p> <p>Run the tests and calculate test coverage using:</p> Bash<pre><code>hatch test\n</code></pre> <p>A coverage report can be generated with</p> Bash<pre><code>hatch test -c\n</code></pre> <p>The coverage report will let you know which lines of code are touched by the tests. Strive to get 100% coverage for the lines you changed. It's OK if you can't or don't know how to test something. Leave a comment in the PR and we'll help you out.</p>"},{"location":"contributing.html#documentation","title":"Documentation","text":"<p>Most documentation sources are in the <code>docs</code> folder. We use Mkdocs to build the web pages from these sources.</p> <p>To serve a live copy of the documentation locally, Mkdocs provides a local web-server and auto-building tool</p> Bash<pre><code>hatch run docs:serve\n</code></pre> <p>To build the HTML files as a static site</p> Bash<pre><code>hatch run docs:build\n</code></pre>"},{"location":"contributing.html#examples","title":"Examples","text":"<p>Examples are written as Jupyter Notebooks and converted to <code>py</code> files using Jupytext. Each example is executed prior to building the documentation so that they are kept relevant and up-to-date.</p> <p>To contribute an example, start by writing a notebook that uses either the data in the examples folder, or a small (less than 50Mb) dataset you can contribute to the project.</p> <p>Then using Jupytext sync your notebook with a Percent style <code>py</code> file using the Jupytext menu. This <code>py</code> file will form the basis of your pull request.</p> <p>When the file has been checked, the example can be added to the documentation to ensure  it is rendered in the final documentation output.</p> <p>Other examples not suitable for the documentation are welcome and can be submitted as notebooks (without execution) in the <code>examples/notebooks</code> folder of the repository.</p> <p>You can start a Jupyter session in the project directory using hatch</p> Bash<pre><code>hatch run docs:jupyter lab\n</code></pre>"},{"location":"contributing.html#code-review","title":"Code Review","text":"<p>After you've submitted a pull request, you should expect to hear at least a comment within a couple of days. We may suggest some changes or improvements or alternatives.</p> <p>Some things that will increase the chance that your pull request is accepted quickly:</p> <ul> <li>Write a good and detailed description of what the PR does.</li> <li>Write tests for the code you wrote/modified.</li> <li>Readable code is better than clever code (even with comments).</li> <li>Write documentation for your code (docstrings) and leave comments explaining the   reason behind non-obvious things.</li> <li>Include an example of new features in the gallery or tutorials.</li> <li>Follow the PEP8_ style guide for code and the   <code>Google Docstring Guide</code>   for documentation.</li> </ul> <p>Pull requests will automatically have tests run by Github Actions. This includes running both the unit tests as well as code linters. Github will show the status of these checks on the pull request. Try to get them all passing (green). If you have any trouble, leave a comment in the PR or get in touch.</p>"},{"location":"contributing.html#attribution","title":"Attribution","text":"<p>This contributing document is largely based upon the work by the Fatiando a Terra project.</p>"},{"location":"examples_about.html","title":"About","text":"<p>The examples in this section are configured as working Juptyer Notebooks. They can be downloaded directly by using the  icon at the top of the page of each example.</p> <p>The data used for these examples is available from the Github repository  examples/data directory.</p> <p>Tip</p> <p>If the examples aren't enough and you are still a little lost, please head over to the  Discussions board to browse previous questions or ask a new one.</p>"},{"location":"history.html","title":"History","text":""},{"location":"history.html#history","title":"History","text":"<p>Segysak was originally conceived out of a need for a better interface to SEG-Y data in Python. The groundwork was layed by Tony Hallam but development really began during the Transform 2020 Software Underground Hackathon held online across the world due to the cancellation of of the EAGE Annual in June of that year. Significant contributions during the hackathon were made by Steve Purves, Gijs Straathof, Fabio Contreras and Alessandro Amato del Monte.</p> <p>Significant updates were made at Transform 2021.  Multiple new and advanced examples were released. A 2 hour video tutorial and notebook as a demonstration of key functionality and an introduction to Xarry for seismic applications streamed.  Experimental ZGY support was introduced.</p>"},{"location":"installation.html","title":"Installation","text":"<p>SEGY-SAK can be installed by using <code>pip</code> from PyPi and from source.</p>"},{"location":"installation.html#python-package-index-via-pip","title":"Python Package Index via <code>pip</code>","text":"<p>From the command line run the <code>pip</code> package manager</p> Bash<pre><code>python -m pip install segysak\n</code></pre>"},{"location":"installation.html#install-from-source","title":"Install from source","text":"<p>Clone the SEGY-SAK Github repository and in the top level directory run</p> Bash<pre><code>python -m pip install .\n</code></pre> <p>To run the tests install the test dependencies and run <code>pytest</code></p> Bash<pre><code>python -m pip install .[test]\npytest\n</code></pre> <p>Info</p> <p>If you wish to develop SEGY-SAK please follow the contributing guide.</p>"},{"location":"quickstart.html","title":"Quickstart","text":""},{"location":"quickstart.html#open-a-seg-y-file","title":"Open a SEG-Y File","text":"<p>SEG-Y data is opened using a lazy backend with an Xarray backend engine. The limited subset of the trace headers are  scanned and loaded into the Dataset eagerly to build the Dataset geometry but the trace data is accessed as required to save memory. Additional trace headers can be included by specifying a name and byte position in the <code>extra_byte_fields</code> argument.</p> <p>Note</p> <p>Not all SEG-Y data writes trace headers to the same byte positions. Check your data to ensure you have specified the correct trace header byte locations for the dimensions and extra byte fields you want.</p> 3D3D Gathers2D Line Python<pre><code>import xarray as xr\nds = xr.open_dataset(\n    segyfile_path,\n    dim_byte_fields={'iline':189, 'xline':193},\n    extra_byte_fields={'cdp_x':181, 'cdp_y':185},\n)\nds.segysak.scale_coords() # if required\n</code></pre> Python<pre><code>import xarray as xr\nds = xr.open_dataset(\n    segyfile_path,\n    dim_byte_fields={'iline':189, 'xline':193, 'offset':37},\n    extra_byte_fields={'cdp_x':181, 'cdp_y':185},\n)\nds.segysak.scale_coords() # if required\n</code></pre> Python<pre><code>import xarray as xr\nds = xr.open_dataset(\n    segyfile_path,\n    dim_byte_fields={'cdp':22},\n    extra_byte_fields={'cdp_x':181, 'cdp_y':185},\n)\nds.segysak.scale_coords() # if required\n</code></pre>"},{"location":"quickstart.html#check-the-text-header","title":"Check the text header","text":"<p>The text header can be accessed directly without loading the data and often contains information about which byte locations to specify to <code>open_dataset</code>.</p> Python<pre><code>from segysak.segy import get_segy_texthead\n\nget_segy_texthead(segyfile_path)\n</code></pre>"},{"location":"quickstart.html#scan-or-extract-the-trace-headers","title":"Scan or extract the trace headers","text":"<p>The trace headers contain information about each SEG-Y file trace, including scalars, parameters and coordinates. The information available in your trace headers depends upon the file and how or where it was originally created.</p> <p>To discover what information exists in your trace headers you can either scan a few traces for a summary or extract the entire content as a Pandas DataFrame.</p> ScanScrape/Extract Python<pre><code>from segysak.segy import segy_header_scan\nscan = segy_header_scan(segyfile_path)\n</code></pre> Python<pre><code>from segysak.segy import segy_header_scrape\nscrape = segy_header_scrape(segyfile_path)\n</code></pre>"},{"location":"quickstart.html#disabling-tqdm-progress-bars","title":"Disabling TQDM progress bars","text":"<p>If you wish to disable loading/progress bars for the package. This can be done globally for the session.</p> Python<pre><code>from segysak.progress import Progress\nProgress.set_defaults(disable=True)\n</code></pre>"},{"location":"quickstart.html#more-details","title":"More Details","text":"<p>For more detailed information and examples. Check the examples section or the package reference.</p>"},{"location":"seisnc-standard.html","title":"SEISNC conventions for <code>xarray</code>","text":"<p>The SEGY-SAK Xarray seismic specification termed SEISNC can be used by SEGY-SAK to create and manage Xarray Datasets with details and attributes common to seismic data. Xarray uses NetCDF4 as its standard representation and this fits neatly with regularly  sampled seismic data (e.g. 3D data). Moreover, unlike SEG-Y, <code>xarray</code> compatible formats fit neatly into the Python scientific stack providing more performant operations like lazy loading, easy slicing,  compatibility with multi-core and multi-node operations using <code>dask</code> as well  as important features such as labelled axes and coordinates.</p> <p>This specification is not meant to be prescriptive but outlines some basic requirements for <code>xarray</code> datasets to work effectively with SEGY-SAK functionality.</p>"},{"location":"seisnc-standard.html#3d-data-dimensions","title":"3D data dimensions","text":"<p>SEGY-SAK uses the convention labels of <code>iline</code>, <code>xline</code> to describe the bins of 3D data. The vertical dimension is labelled <code>samples</code> and it is up to the user to understand the domain (usually TWT or depth). A typical <code>xarray</code> dataset created by SEGY-SAK will return for example</p> Python<pre><code>&gt;&gt;&gt; seisnc_3d = xr.open_dataset('test3d.sgy', dim_byte_fields={\"iline\":189, \"xline\":193})\n&gt;&gt;&gt; seisnc_3d.sizes\nFrozen({'iline': 61, 'xline': 202, 'samples': 850})\n</code></pre> <p>Frozen standard names are supplied via the <code>HorDimKeyField</code>.</p>"},{"location":"seisnc-standard.html#2d-data-dimensions","title":"2D data dimensions","text":"<p>For 2D data SEGY-SAK uses the dimension label <code>cdp</code> for CDP locations. This allows the package to distinguish between 2D and 3D data to allow automation on saving and convenience wrappers. The same vertical dimensions apply as for 3D. A typical <code>xarray</code> in 2D format would return</p> Python<pre><code>&gt;&gt;&gt; seisnc_2d = xr.open_dataset('test3d.sgy', dim_byte_fields={\"cdp\": 21})\n&gt;&gt;&gt; seisnc_2d.sizes\nFrozen({'cdp': 61, 'twt': 850})\n</code></pre> <p>Frozen standard names are supplied via the <code>HorDimKeyField</code>.</p>"},{"location":"seisnc-standard.html#gathers-pre-stack-data-extra-dimensions","title":"Gathers / Pre-stack data / Extra dimensions","text":"<p>SEGY-SAK supports an arbitrary number of extra dimensions which might be written to SEG-Y. The most common types of data with extra dimensions are gathers also known pre-stack data. Often this data is either exported as angle or offset dependent gathers. SEGY-SAK provides standard names for these dimensions.</p> <p>Frozen standard names are supplied via the <code>DimKeyField</code>.</p>"},{"location":"seisnc-standard.html#map-coordinates","title":"Map Coordinates","text":"<p>Map coordinates describe the physical location on earth of the seismic data. Rarely do these coordinates align nicely with a regular grid due to rotation of the seismic bin grid relative to the map grid. Therefore it is typically not feasible to use map coordinates for the Dataset dimensions.</p> <p>Instead, X and Y map coordinates must be loading into the Dataset variables on regular dimensions such as <code>iline</code>/<code>xline</code> or <code>cdp</code>. SEGY-SAK by convention labels the X and Y map coordinate variables <code>cdp_x</code> and <code>cdp_y</code>. Other variable names can be used if users use the <code>ds.segysak.set_coords()</code> method.</p> Python<pre><code>&gt;&gt;&gt; seisnc_3d = xr.open_dataset(\n   'test3d.sgy', dim_byte_fields={\"iline\":189, \"xline\":193}, extra_byte_fields={\"cdp_x\": 73, \"cdp_y\": 77}\n)\n&gt;&gt;&gt; seisnc_3d.sizes\nFrozen({'iline': 61, 'xline': 202, 'samples': 850})\n&gt;&gt;&gt; seisnc_3d.data_vars\nData variables:\n    cdp_x    (iline, xline) float64 99kB 4.364e+05 4.364e+05 ... 4.341e+05\n    cdp_y    (iline, xline) float64 99kB 6.477e+06 6.477e+06 ... 6.479e+06\n    data     (iline, xline, samples) float32 42MB ...\n</code></pre> <p>Frozen standard names are supplied via the <code>CoordKeyField</code>.</p>"},{"location":"seisnc-standard.html#attributes","title":"Attributes","text":"<p>Xarray Datasets have an attributes dictionary available from <code>ds.attrs</code>.</p> <p>This works well for simple attributes but SEGY-SAK found a need for more complex variable types not supported by standard NetCDF4 as well as a need for a protected attribute area. To meet this need, SEGY-SAK implements it's own <code>attrs</code> as a JSON encoded string under <code>ds.attrs['seisnc']</code>. Access to the variables in this string are via the <code>segysak</code> Dataset accessor (an extension for Xarray) <code>ds.segysak.attrs</code>.</p> <p>Any number of attributes can be added to a <code>siesnc</code> file. Currently the following attributes are extracted or reserved for use by SEGY-SAK. Attributes marked by  are depreciated.</p> Attribute Name type Purpose <code>ns</code> int number of samples per trace <code>ds</code> float sample interval <code>text</code> str ebcidc header as ascii text <code>measurement_sys</code> vertical units of the data <code>d3_domain</code>  vertical domain of the data <code>epsg</code> str data epsg code <code>corner_points</code> tuple  corner points of the dataset in grid coordinates <code>corner_points_xy</code> tuple corner points of the dataset in xy <code>source_file</code> str name of the file the dataset was created from <code>srd</code> float seismic reference datum of the data in vertical units <code>measurement_sys</code> and <code>d3_domain</code> <code>datatype</code> str  the data type e.g. amplitude, velocity, attribute <code>percentiles</code> tuple  this is an array of approximate percentile values created during scanning from SEG-Y. Primarily this is useful for plotting by limiting the dynamic range of the display. The percentiles are in percent 0, 0.1, 10, 50, 90, 99.9 &amp; <code>coord_scalar</code> int the SEG-Y coordinate scalar <code>coord_scaled</code> A boolean that indicates if the coordinates have been scaled or not. <code>dimensions</code> A mapping of <code>DimKeyField</code> and <code>HorKeyField</code> to Dataset dimensions. <code>vert_domain</code> A member of <code>VerticalKeyDim</code>. <p>Frozen standard names are supplied via the <code>AttrKeyField</code>.</p>"},{"location":"seisnc-standard.html#keyfields","title":"KeyFields","text":"<p>Keyfields represent standard names for variables, dimensions and attributes that SEGY-SAK uses internally to access relevant data. It is helpful to stick to these conventions when working with SEGY-SAK. Alternatively it is possible to set a mapping to your chosen names using the <code>set_dimensions</code>, and <code>set_coords</code> methods in the accessor module <code>segysak</code>.</p> <p>The frozen <code>dataclass</code> are prefixed by <code>_</code> while instantiated dataclasses for general use have no prefix.</p>"},{"location":"seisnc-standard.html#frozen-base-classes","title":"Frozen base classes","text":"<p>Info</p> <p>These classes are referenced here to show the standard seisnc variable names. Use the instantiated classes if including these keyfields in your code.</p> <p>Enumerations of dataset keys.</p>"},{"location":"seisnc-standard.html#segysak._keyfield._HorDimKeyField","title":"<code>_HorDimKeyField</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetaDataClass</code></p> <p>Horizontal Dimensions of seismic data.</p> Source code in <code>segysak/_keyfield.py</code> Python<pre><code>@dataclass(order=True, frozen=True)\nclass _HorDimKeyField(MetaDataClass):\n    \"\"\"Horizontal Dimensions of seismic data.\"\"\"\n\n    cdp: str = \"cdp\"\n    iline: str = \"iline\"\n    xline: str = \"xline\"\n</code></pre>"},{"location":"seisnc-standard.html#segysak._keyfield._VerDimKeyField","title":"<code>_VerDimKeyField</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetaDataClass</code></p> <p>Vertical Dimensions of seismic data.</p> Source code in <code>segysak/_keyfield.py</code> Python<pre><code>@dataclass(order=True, frozen=True)\nclass _VerDimKeyField(MetaDataClass):\n    \"\"\"Vertical Dimensions of seismic data.\"\"\"\n\n    depth: str = \"depth\"\n    twt: str = \"twt\"\n    samples: str = \"samples\"\n</code></pre>"},{"location":"seisnc-standard.html#segysak._keyfield._DimKeyField","title":"<code>_DimKeyField</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_HorDimKeyField</code>, <code>_VerDimKeyField</code></p> <p>Dimension keys to use for naming.</p> Source code in <code>segysak/_keyfield.py</code> Python<pre><code>@dataclass(order=True, frozen=True)\nclass _DimKeyField(_HorDimKeyField, _VerDimKeyField):\n    \"\"\"Dimension keys to use for naming.\"\"\"\n\n    offset: str = \"offset\"\n    angle: str = \"angle\"\n</code></pre>"},{"location":"seisnc-standard.html#segysak._keyfield._CoordKeyField","title":"<code>_CoordKeyField</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetaDataClass</code></p> <p>Coordinate Keys to use for naming key components of the Dataset</p> Source code in <code>segysak/_keyfield.py</code> Python<pre><code>@dataclass(order=True, frozen=True)\nclass _CoordKeyField(MetaDataClass):\n    \"\"\"Coordinate Keys to use for naming key components of the Dataset\"\"\"\n\n    cdp_x: str = \"cdp_x\"\n    cdp_y: str = \"cdp_y\"\n    lat: str = \"lat\"\n    long: str = \"lon\"\n</code></pre>"},{"location":"seisnc-standard.html#segysak._keyfield._VariableKeyField","title":"<code>_VariableKeyField</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetaDataClass</code></p> Source code in <code>segysak/_keyfield.py</code> Python<pre><code>@dataclass(frozen=True)\nclass _VariableKeyField(MetaDataClass):\n    data: str = \"data\"\n</code></pre>"},{"location":"seisnc-standard.html#segysak._keyfield._AttrKeyField","title":"<code>_AttrKeyField</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetaDataClass</code></p> Source code in <code>segysak/_keyfield.py</code> Python<pre><code>@dataclass(frozen=True)\nclass _AttrKeyField(MetaDataClass):\n    ns: str = \"ns\"\n    sample_rate: str = \"sample_rate\"\n    text: str = \"text\"\n    measurement_system: str = \"measurement_system\"\n    d3_domain: str = \"d3_domain\"\n    epsg: str = \"epsg\"\n    corner_points: str = \"corner_points\"\n    corner_points_xy: str = \"corner_points_xy\"\n    source_file: str = \"source_file\"\n    srd: str = \"srd\"\n    datatype: str = \"datatype\"\n    percentiles: str = \"percentiles\"\n    coord_scalar: str = \"coord_scalar\"\n    coord_scaled: str = \"coord_scaled\"\n    dimensions: str = \"dimensions\"\n    vert_dimension: str = \"vert_dimension\"\n    vert_domain: str = \"vert_domain\"\n</code></pre>"},{"location":"seisnc-standard.html#segysak._keyfield._VerticalKeyDim","title":"<code>_VerticalKeyDim</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetaDataClass</code></p> Source code in <code>segysak/_keyfield.py</code> Python<pre><code>@dataclass(frozen=True)\nclass _VerticalKeyDim(MetaDataClass):\n    TWT: str = \"twt\"\n    DEPTH: str = \"depth\"\n    samples: str = \"samples\"\n</code></pre>"},{"location":"seisnc-standard.html#instantiated-classes","title":"Instantiated classes","text":"Python<pre><code>from segysak import (\n   HorDimKeyField,\n   VerDimKeyField,\n   DimKeyField,\n   CoordKeyField,\n   VariableKeyField,\n   AttrKeyField,\n   VerticalKeyDim,\n)\n</code></pre>"},{"location":"seisnc-standard.html#segysak.HorDimKeyField","title":"<code>HorDimKeyField = _HorDimKeyField()</code>  <code>module-attribute</code>","text":""},{"location":"seisnc-standard.html#segysak.VerDimKeyField","title":"<code>VerDimKeyField = _VerDimKeyField()</code>  <code>module-attribute</code>","text":""},{"location":"seisnc-standard.html#segysak.DimKeyField","title":"<code>DimKeyField = _DimKeyField()</code>  <code>module-attribute</code>","text":""},{"location":"seisnc-standard.html#segysak.CoordKeyField","title":"<code>CoordKeyField = _CoordKeyField()</code>  <code>module-attribute</code>","text":""},{"location":"seisnc-standard.html#segysak.VariableKeyField","title":"<code>VariableKeyField = _VariableKeyField()</code>  <code>module-attribute</code>","text":""},{"location":"seisnc-standard.html#segysak.AttrKeyField","title":"<code>AttrKeyField = _AttrKeyField()</code>  <code>module-attribute</code>","text":""},{"location":"seisnc-standard.html#segysak.VerticalKeyDim","title":"<code>VerticalKeyDim = _VerticalKeyDim()</code>  <code>module-attribute</code>","text":""},{"location":"seisnc.html","title":"Xarray SEISNC","text":"<p>SEISNC is set of conventions around loading data into Xarray datasets. The  standards assist some of the SEGY-SAK methods that  support operations targetting seismic data but also ensure the loaded seismic data is compatible with other exporters (e.g. NetCDF4, Zarr, ZGY).</p> <p>The standards are not prescriptive, but SEGY-SAK functionality may not work properly if they are not followed.</p>"},{"location":"tutorial.html","title":"Tutorial","text":"<p>Warning</p> <p>The content of this tutorial has some relevance, but there are significant changes to the functionality of SEGY-SAK from <code>v0.5</code> onwards. Please refer to the current documentation for the latest methods, examples and reference.</p> <p>A notebook tutorial and accompanying YouTube video were created and recorded for Transform 2021. The tutorial covers much of the example material and more with deeper explanations for integrating with Xarray.</p> <p>Checkout the tutorial repository(https://github.com/trhallam/segysak-t21-tutorial) or launch an interactive session on binder via  </p>"},{"location":"upgrading.html","title":"Upgrading to v0.5","text":"<p>SEGY-SAK underwent major redevelopment from <code>v0.4.x</code> to <code>v0.5</code>. The changes were necessary to introduce improvements to the overall SEGY-SAK experience and to bring SEGY-SAK closer to the Xarray ecosystem. This section offers a guide for upgrading your existing use of SEGY-SAK to the new API and discusses some of the advantages of the new approach.</p> <p>Tip</p> <p>If you have feedback or encounter issues with the new functionality of SEGY-SAK, the old functionality remains in place but is depreciated (with small changes) for backwards compatability. The old API is problematic to maintain, and will be removed in future releases.</p> <p>Please submit any problems to the Github Issue Tracker.</p>"},{"location":"upgrading.html#replacing-segy_loader","title":"Replacing <code>segy_loader</code>","text":"<p>The <code>segy_loader</code> method SEGY-SAK implemented was designed prior to the implementation of custom  backends for Xarray. The introduction of this new API from Xarray offered an opportunity to significantly improve the way SEGY-SAK handled and interacted with SEG-Y files. The new approach introduces</p> <ul> <li>lazy loading of SEG-Y trace data. (Headers still greedy for geometry).</li> <li>Better integration with other Xarray methods including streaming output to NetCDF, Zarr, ZGY or Xarray other supported formats.</li> <li>Better large file support (reduced memory footprint and lazy loading of data).</li> <li>Speed improvements for header scanning and loading.</li> <li>Greater maintainability and usability through a geometry agnostic loading approach.</li> <li>Support <code>open_mfdataset</code> such as in cases where gathers are split into inline files.</li> </ul> <code>v0.5</code><code>v0.4.x</code> Python<pre><code>import xarray as xr\nsegy = xr.open_dataset(\n    segy_file,\n    dim_byte_fields={\"iline\":189, \"xline\":193},\n    extra_byte_fields={\"cdp_x\":181, \"cdp_y\":185}\n)\n</code></pre> Python<pre><code>from segysak.segy import segy_loader\nsegy = segy_loader(\n    segy_file,\n    iline=189, xline=193,\n    cdpx=181, cdpy=185\n)\n</code></pre> <p>Upon loading the SEG-Y data with the new loading tool you will notice some incompatibility with the older SEISNC standard. Read more about replacing the <code>.seis</code> accessor and the changes to the  SEISNC reference.</p> <p>Coordinate scaling is no longer applied by default to loaded SEG-Y data. The user can scale the data (if the coordinate scalar was set in the SEG-Y headers) or provide the scalar manually.</p> Python<pre><code>segy.segysak.scale_coords()\n# or \nsegy.segysak.scale_coords(coord_scalar=-100)\n</code></pre>"},{"location":"upgrading.html#replacing-segy_writer","title":"Replacing <code>segy_writer</code>","text":"<p>The <code>segy_writer</code> functionality has similarly been updated to better align with Xarray. The new methodology introduces</p> <ul> <li>Native support for streamed writing via Xarray chunks. Specifically, chunking is support for all dimensions including trace samples.</li> <li>Access via the Xarray <code>.seisio</code> accessor.</li> <li>Dimensionality agnostic.</li> <li>Dead trace skipping on write.</li> </ul> <p>A vertical dimension is required and it's key can be specified using the <code>vert_dimension</code> keyword argument. Arbitrary key names for the vertical dimension are possible. Each orthogonal dimension of the trace data to be exported is then specified as an additional keyword argument (e.g. <code>iline</code> and <code>xline</code> in this case), with 1 or more horizontal dimensions required. Additional variables can be exported for each trace to the headers using the <code>trace_header_map</code> diction and byte location mapping.</p> <code>v0.5</code><code>v0.4.x</code> Python<pre><code>dataset.seisio.to_segy(\n    \"out.segy\",\n    trace_header_map={\"cdp_x\":73, \"cdp_y\":77},\n    iline=189, xline=193,\n    vert_dimension='samples'\n)\n</code></pre> Python<pre><code>from segysak.segy import segy_header_scrape\nsegy_writer(\n    dataset,\n    \"out.segy\",\n    trace_header_map=dict(iline=5, xline=21)\n)\n</code></pre>"},{"location":"upgrading.html#replacing-dsseisioto_netcdf-and-open_seisnc","title":"Replacing <code>ds.seisio.to_netcdf</code> and <code>open_seisnc</code>","text":"<p>Changes to the SEISNC format have negated the need for a special implementation of <code>to_netcdf</code> and improves SEGY-SAKs compatibility with other Xarray supported backends.</p> <code>v0.5</code><code>v0.4.x</code> Python<pre><code>dataset.to_netcdf('outfile.nc')\n</code></pre> Python<pre><code>dataset.seisio.to_netcdf('outfile.nc')\n</code></pre> <p>Similarly, <code>open_seisnc</code> is now depreciated. Datasets saved with the standard <code>ds.to_netcdf</code> method can now be opened using Xarray directly.</p> <code>v0.5</code><code>v0.4.x</code> Python<pre><code>dataset = xr.open_dataset('outfile_new.nc')\n</code></pre> Python<pre><code>from segysak import open_seisnc\ndataset = open_seisnc('outfile_old.nc')\n</code></pre>"},{"location":"upgrading.html#replacing-dsseis","title":"Replacing <code>ds.seis</code>","text":"<p>SEGY-SAK implemented a custom accessor namespace in <code>&lt;=0.4.x</code> called <code>seis</code>. This has been depreciated in favour of a <code>segysak</code> namespace. The <code>segysak</code> interface removes a lot of legacy implementation that tied SEGY-SAK uses to specific dimension names and keywords. While SEGY-SAK still supports that approach, the user has flexibility and SEGY-SAK now strives to be geometry/convention agnostic, relying on the user to understand their data. Doing this simplifies development and improves the generalisability of SEGY-SAK. </p> <p>Tip</p> <p>Data loaded using SEGY-SAK conventions  still requires minimal input from the user on dimension and  coordinate names.</p> <p>Major changes:</p> <ul> <li>The <code>segysak</code> accessor is available on DataArray and Dataset objects.</li> <li>Set and get dimension and coordinate names using <code>ds.segysak.set_dimensions</code>, <code>ds.segysak.get_dimensions</code>, <code>ds.segysak.set_coords</code> and <code>ds.segysak.get_coords</code>.</li> <li>Store seisnc attributes on DataArray and Dataset objects. This allows multiple DataArrays to be combined (e.g. time and depth data) into a single Dataset.</li> <li>Affine transform updated to use LSQ solver.</li> <li>Fixes/removes bugs with <code>fill_cdpna</code> and <code>percentiles</code>.</li> <li>The arguments and returned values have changed for some accessor methods. Refer to the reference for more details.</li> </ul> <p>Example with <code>calc_corner_points</code></p> <code>v0.5</code><code>v0.4.x</code> Python<pre><code>corner_points = dataset.seysak.calc_corner_points()\n</code></pre> Python<pre><code>dataset.seis.calc_corner_points()\ncorner_points = dataset.attrs['corner_points_xy']\n</code></pre>"},{"location":"upgrading.html#changes-to-seisnc","title":"Changes to SEISNC","text":"<p>SEGY-SAK no longer assumes any details about the data units or vertical dimension. As such, the vertical dimension is now loaded as <code>samples</code> by default.</p> <p>Previously, SEG-Y attributes would be stored in the Xarray <code>Dataset.attrs</code> dictionary. This caused ambiguity where attributes were more suited to a sub-array of the Dataset and created unnecessary  complexity for storing objects that were not compatible with NetCDF  (e.g. lists and custom attributes like the text header). To overcome these difficulties SEGY-SAK has introduced a custom attribute to  Xarray called <code>seisnc</code> which is stored in <code>Dataset.attrs['seisnc']</code>  or <code>DataArray.attrs['seisnc']</code> as a JSON string. The use of JSON  allows for the storage of more complicated objects natively in NetCDF.</p> <p>SEGY-SAK accessors take care or serial/deserialisation of the data to the JSON string. Any JSON compatible object is supported (lists, dictionaries, int, float, strings). The SEISNC attributes are set and accessed using the accessor namespace.</p> Python<pre><code>&gt; ds.seisnc['an_attribute'] = 'my attr'\n&gt; ds.seisnc['an_attribute']\n'my attr' \n&gt; ds.seisnc.attrs\n{'an_attribute':'my_attr'}\n&gt; ds.attrs\n{'seisnc':'{\"an_attribute\":\"my_attr\"}'}\n</code></pre> <p>Certain keys are still reserved for the SEISNC standard. However, some will be depreciated in future versions.</p>"},{"location":"upgrading.html#changes-to-the-old-api","title":"Changes to the old API","text":"<p>The only breaking change to the old API in <code>v0.5</code> is the removal of the silent argument from all functions. The old API made use of <code>tqdm</code> to post information about slow loading processes. This functionality is carried forward, but control of <code>tqdm</code> is handled by a new library class <code>Progress</code>. This all <code>tqdm</code> arguments to be set at a session level.</p> <p>For instance, to disable progress reporting for all subsequent SEGY-SAK commands.</p> <code>v0.5</code><code>v0.4.x</code> Python<pre><code>from segysak.progress import Progress\nfrom segysak.segy import segy_header_scrape\nProgress.set_defaults(disable=True)\n# silent keyword argument replaced by Progress.set_defaults(disable=True) ^^^\nscrape = segy_header_scrape(\"data/volve10r12-full-twt-sub3d.sgy\")\n</code></pre> Python<pre><code>from segysak.segy import segy_header_scrape\nscrape = segy_header_scrape(\n    \"data/volve10r12-full-twt-sub3d.sgy\",\n    silent=True\n)\n</code></pre>"},{"location":"upgrading.html#removing-zgy","title":"Removing ZGY","text":"<p>SEGY-SAK has removed ZGY tools which have been migrated to and improved in the  PyZGY package. Please use the ZGY tools and new Xarray backend provided by PyZGY.</p> <code>v0.5</code><code>v0.4.x</code> <p>Until a new release is created, please install PyZgy from Github</p> Bash<pre><code>pip install git+https://github.com/equinor/pyzgy.git\n</code></pre> Python<pre><code># open a zgy file\nzgy = xr.open_dataset('zgyfile.zgy')\n\n# write to a zgy file\nzgy.pyzgy.to_zgy('out_zgyfile.zgy')\n</code></pre> Python<pre><code>from segysak.openzgy import zgy_loader, zgy_writer\nds_zgy = zgy_loader(\"zgyfile.zgy\")\n\nzgy_writer(ds, \"out_zgyfile.zgy\")\n</code></pre>"},{"location":"why-segysak.html","title":"Overview: Why SEGY-SAK?","text":"<p>The objective of SEGY-SAK was to bring together the usefulness of <code>segyio</code> and <code>xarray</code> to improve accessibility of seismic data for geoscientists using Python. Key objectives for this project are to make loading and exporting SEG-Y easier by offering common simple interfaces to <code>segyio</code> and to then load data into an <code>xarray</code> format either directly into memory or by streaming large files to disk.</p> <p>Currently SEGY-SAK can load 2D, 2D gathers, 3D and 3D gathers into a common format stable format called <code>seisnc</code> that you can use faithfully in your downstream applications. The <code>seisnc</code> format is outlined in the standard section of the documentation and is the basis for all of the other functionality provided by SEGY-SAK.</p> <p>Beyond that, the <code>segysak</code> package has enhancements to <code>xarray</code> that reduce complexity for common seismic related tasks to extracting information from seismic such as arbitrary line, well-deviation or horizon extractions. We also offer a long list of notebook examples to help new and experienced Python users alike traverse the intricacies of Xarray and demonstrate how to do common tasks with <code>segysak</code>.</p> <p>Finally, the use of Xarray and the NetCDF4 format allows you to scale your problems using Dask. Dask can lazily load and process data across multiple cores and even distributed memory allowing you to apply your Python code to large seismic volumes. This process also means you can plot slices from your volumes in Python without loading the full dataset into memory.</p>"},{"location":"api/geometry.html","title":"Survey Geometry","text":"<p>Functions and utilities related to SEG-Y/Seismic Geometry</p>"},{"location":"api/geometry.html#segysak.geometry.fit_plane","title":"<code>fit_plane(x, y, z, p0=(1.0, 1.0, 1.0))</code>","text":"<p>Calculate the plane function coefficients for input data and return a partial plane function.</p> Source code in <code>segysak/geometry.py</code> Python<pre><code>def fit_plane(x: np.array, y: np.array, z: np.array, p0=(1.0, 1.0, 1.0)) -&gt; Callable:\n    \"\"\"Calculate the plane function coefficients for input data and return a partial plane function.\"\"\"\n\n    fit, _ = curve_fit(plane, (x, y), z, p0=p0)\n    func = lambda xy: plane(xy, *fit)\n    return func\n</code></pre>"},{"location":"api/geometry.html#segysak.geometry.get_uniform_spacing","title":"<code>get_uniform_spacing(points, extra=None, bin_spacing_hint=10, method='linear')</code>","text":"<p>Interpolate the cdp_x, cdp_y arrays uniformly while staying close to the requested bin spacing</p> <p>Assumes no gaps in the points.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>array</code> <p>cdp_x, cdp_y point pairs [M, 2] defining the path segments.</p> required <code>extra</code> <code>List[array]</code> <p>a list of 1D arrays [M] to also interpolate along the path.</p> <code>None</code> <code>bin_spacing_hint</code> <code>float</code> <p>A bin spacing to stay close to, in cdp world units. Default: 10</p> <code>10</code> <code>method</code> <code>str</code> <p>The scipy interp1d interpolation method between points.</p> <code>'linear'</code> <p>Returns:</p> Type Description <code>Tuple[array, Union[List[array], None]]</code> <p>Interpolated points, Interpolated extra vars: Uniform sampling using the bin_spacing hint.</p> Source code in <code>segysak/geometry.py</code> Python<pre><code>def get_uniform_spacing(\n    points: np.array,\n    extra: List[np.array] = None,\n    bin_spacing_hint: float = 10,\n    method: str = \"linear\",\n) -&gt; Tuple[np.array, Union[List[np.array], None]]:\n    \"\"\"Interpolate the cdp_x, cdp_y arrays uniformly while staying close to the\n    requested bin spacing\n\n    Assumes no gaps in the points.\n\n    Args:\n        points: cdp_x, cdp_y point pairs [M, 2] defining the path segments.\n        extra: a list of 1D arrays [M] to also interpolate along the path.\n        bin_spacing_hint: A bin spacing to stay close to, in cdp world units. Default: 10\n        method: The scipy interp1d interpolation method between points.\n\n    Returns:\n        Interpolated points, Interpolated extra vars: Uniform sampling using the bin_spacing hint.\n    \"\"\"\n    points = np.asarray(points)\n    assert len(points.shape) == 2\n    assert points.shape[1] == 2\n\n    segment_lengths = np.insert(np.linalg.norm(np.diff(points, axis=0), axis=1), 0, 0.0)\n    segments_cum_lengths = np.cumsum(segment_lengths)\n    path_length = segments_cum_lengths[-1]\n\n    num_pts = int(path_length / bin_spacing_hint) + 1\n    uniform_sampled_path = np.linspace(0, path_length, num_pts)\n\n    cdp_x_i = interp1d(segments_cum_lengths, points[:, 0], kind=method)(\n        uniform_sampled_path\n    )\n    cdp_y_i = interp1d(segments_cum_lengths, points[:, 1], kind=method)(\n        uniform_sampled_path\n    )\n\n    if extra is not None:\n        extras_i = [\n            interp1d(segments_cum_lengths, ex, kind=method)(uniform_sampled_path)\n            for ex in extra\n        ]\n    else:\n        extras_i = None\n\n    return np.column_stack((cdp_x_i, cdp_y_i)), extras_i\n</code></pre>"},{"location":"api/geometry.html#segysak.geometry.lsq_affine_transform","title":"<code>lsq_affine_transform(x, y, zero_small_values=True, estimate_error=False)</code>","text":"<p>Calculate the Affine transform from the least squared solver.</p> <p>Note, this is not an exact solution as there can be numeric error, but it is more robust than exact methods.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>The input coordinates as pairs [M, 2]. E.g. [[iline, xline], ...]</p> required <code>y</code> <code>array</code> <p>The output coordinates as pairs [M, 2]. E.g. [[cdp_x, cdp_y], ...]</p> required <code>zero_small_values</code> <code>bool</code> <p>Set small values in the LSQ solution to zero.</p> <code>True</code> <code>estimate_error</code> <code>bool</code> <p>Optionally use the transform to return a tuple of (mean, max) error for estimated transform.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Affine2D, Tuple[float, float]]</code> <p>transform, error: Returns the matplotlib Affine2D object and optionally an error estimate tuple.</p> Source code in <code>segysak/geometry.py</code> Python<pre><code>def lsq_affine_transform(\n    x: np.array,\n    y: np.array,\n    zero_small_values: bool = True,\n    estimate_error: bool = False,\n) -&gt; Tuple[Affine2D, Tuple[float, float]]:\n    \"\"\"Calculate the Affine transform from the least squared solver.\n\n    Note, this is not an exact solution as there can be numeric error, but it is more robust than exact methods.\n\n    Args:\n        x: The input coordinates as pairs [M, 2]. E.g. [[iline, xline], ...]\n        y: The output coordinates as pairs [M, 2]. E.g. [[cdp_x, cdp_y], ...]\n        zero_small_values: Set small values in the LSQ solution to zero.\n        estimate_error: Optionally use the transform to return a tuple of (mean, max) error for estimated transform.\n\n    Returns:\n        transform, error: Returns the matplotlib Affine2D object and optionally an error estimate tuple.\n    \"\"\"\n    # https://stackoverflow.com/questions/20546182/how-to-perform-coordinates-affine-transformation-using-python-part-2\n    n_points = x.shape[0]\n\n    # Pad the data with ones, so that our transformation can do translations too\n\n    pad = lambda x: np.hstack([x, np.ones((n_points, 1))])\n    X = pad(x)\n    Y = pad(y)\n\n    # Solve the least squares problem X * A = Y\n    # to find our transformation matrix A\n    A, res, rank, s = np.linalg.lstsq(X, Y, rcond=None)\n\n    # set really small values to zero\n    if zero_small_values:\n        A[np.abs(A) &lt; 1e-10] = 0\n\n    transform = Affine2D.from_values(*A[:, :2].ravel())\n\n    if estimate_error:\n        error = transform.transform(x) - y\n        return transform, (error.mean(), error.max())\n    else:\n        return transform, None\n</code></pre>"},{"location":"api/geometry.html#segysak.geometry.orthogonal_point_affine_transform","title":"<code>orthogonal_point_affine_transform(x, y, estimate_error=False)</code>","text":"<p>Calculate an affine transform using orthogonal points. This assumes an orthogonal survey. If you have a skewed goemetry, use <code>lsq_affine_transform</code>.</p> Text Only<pre><code>^ (2, 2)\n|\n|\n|\n|\n|_\n|_|____________&gt;\n(0, 0)         (1, 1)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tuple</code> <p>The input coordinates as pairs [3, 2]. E.g. [[iline, xline], ...]</p> required <code>y</code> <code>array</code> <p>The output coordinates as pairs [3, 2]. E.g. [[cdp_x, cdp_y], ...]</p> required <code>estimate_error</code> <code>bool</code> <p>Optionally use the transform to return a tuple of (mean, max) error for estimated transform.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Affine2D, Tuple[float, float]]</code> <p>transform, error: Returns the matplotlib Affine2D object and optionally an error estimate tuple.</p> Source code in <code>segysak/geometry.py</code> Python<pre><code>def orthogonal_point_affine_transform(\n    x: Tuple, y: np.array, estimate_error: bool = False\n) -&gt; Tuple[Affine2D, Tuple[float, float]]:\n    \"\"\"Calculate an affine transform using orthogonal points. This assumes an orthogonal survey. If you have a\n    skewed goemetry, use [`lsq_affine_transform`][segysak.geometry.lsq_affine_transform].\n\n    ```\n    ^ (2, 2)\n    |\n    |\n    |\n    |\n    |_\n    |_|____________&gt;\n    (0, 0)         (1, 1)\n    ```\n\n    Args:\n        x: The input coordinates as pairs [3, 2]. E.g. [[iline, xline], ...]\n        y: The output coordinates as pairs [3, 2]. E.g. [[cdp_x, cdp_y], ...]\n        estimate_error: Optionally use the transform to return a tuple of (mean, max) error for estimated transform.\n\n    Returns:\n        transform, error: Returns the matplotlib Affine2D object and optionally an error estimate tuple.\n    \"\"\"\n    # direct solve for affine transform via equation substitution\n    # https://cdn.sstatic.net/Sites/math/img/site-background-image.png?v=09a720444763\n    # ints for iline xline will often overflow\n    (x0, y0), (x1, y1), (x2, y2) = x\n    (x0p, y0p), (x1p, y1p), (x2p, y2p) = y\n\n    a = (x1p * y0 - x2p * y0 - x0p * y1 + x2p * y1 + x0p * y2 - x1p * y2) / (\n        x1 * y0 - x2 * y0 - x0 * y1 + x2 * y1 + x0 * y2 - x1 * y2\n    )\n    c = (x1p * x0 - x2p * x0 - x0p * x1 + x2p * x1 + x0p * x2 - x1p * x2) / (\n        -x1 * y0 + x2 * y0 + x0 * y1 - x2 * y1 - x0 * y2 + x1 * y2\n    )\n    b = (y1p * y0 - y2p * y0 - y0p * y1 + y2p * y1 + y0p * y2 - y1p * y2) / (\n        x1 * y0 - x2 * y0 - x0 * y1 + x2 * y1 + x0 * y2 - x1 * y2\n    )\n    d = (y1p * x0 - y2p * x0 - y0p * x1 + y2p * x1 + y0p * x2 - y1p * x2) / (\n        -x1 * y0 + x2 * y0 + x0 * y1 - x2 * y1 - x0 * y2 + x1 * y2\n    )\n    e = (\n        x2p * x1 * y0\n        - x1p * x2 * y0\n        - x2p * x0 * y1\n        + x0p * x2 * y1\n        + x1p * x0 * y2\n        - x0p * x1 * y2\n    ) / (x1 * y0 - x2 * y0 - x0 * y1 + x2 * y1 + x0 * y2 - x1 * y2)\n    f = (\n        y2p * x1 * y0\n        - y1p * x2 * y0\n        - y2p * x0 * y1\n        + y0p * x2 * y1\n        + y1p * x0 * y2\n        - y0p * x1 * y2\n    ) / (x1 * y0 - x2 * y0 - x0 * y1 + x2 * y1 + x0 * y2 - x1 * y2)\n    values = (v if ~np.isnan(v) else 0.0 for v in (a, b, c, d, e, f))\n    transform = Affine2D.from_values(*values)\n\n    if estimate_error:\n        error = transform.transform(x) - y\n        return transform, (error.mean(), error.max())\n    else:\n        return transform, None\n</code></pre>"},{"location":"api/geometry.html#segysak.geometry.plane","title":"<code>plane(xy, a, b, c)</code>","text":"<p>Function of a plane for linear fitting using curve_fit</p> Source code in <code>segysak/geometry.py</code> Python<pre><code>def plane(xy: Tuple[float, float], a: float, b: float, c: float) -&gt; float:\n    \"\"\"Function of a plane for linear fitting using curve_fit\"\"\"\n    x, y = xy\n    return a * x + b * y + c\n</code></pre>"},{"location":"api/segy_inspecting.html","title":"Inspecting SEG-Y","text":""},{"location":"api/segy_inspecting.html#segysak.segy.TraceHeaders","title":"<code>TraceHeaders</code>","text":"<p>A convenience class for accessing and iterating over a SEG-Y files trace headers. This class should be used with a context manager.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with TraceHeaders(segy_file, bytes_filter=bytes_filter, **segyio_kwargs) as headers:\n        ntraces = headers.ntraces\n        df = headers.to_dataframe(selection=slice(0, 100)))\n</code></pre> Source code in <code>segysak/segy/_segy_headers.py</code> Python<pre><code>class TraceHeaders:\n    \"\"\"A convenience class for accessing and iterating over a SEG-Y files trace\n    headers. This class should be used with a context manager.\n\n    Examples:\n\n        &gt;&gt;&gt; with TraceHeaders(segy_file, bytes_filter=bytes_filter, **segyio_kwargs) as headers:\n                ntraces = headers.ntraces\n                df = headers.to_dataframe(selection=slice(0, 100)))\n\n    \"\"\"\n\n    def __init__(\n        self,\n        segy_file: Union[str, os.PathLike],\n        bytes_filter: Union[List[int], None] = None,\n        tracefield_filter: Union[List[str], None] = None,\n        **segyio_kwargs: Any,\n    ):\n\n        check_tracefield(bytes_filter)\n        check_tracefield_names(tracefield_filter)\n\n        self.filter = self._combine_filters(bytes_filter, tracefield_filter)\n\n        self.bytes_filter = bytes_filter\n\n        self.segy_file = segy_file\n        _segyio_kwargs = segyio_kwargs.copy()\n        _segyio_kwargs.update({\"ignore_geometry\": True})\n        self.fh = segyio.open(self.segy_file, \"r\", **_segyio_kwargs)\n        self.ntraces = self.fh.tracecount\n\n    def _combine_filters(\n        self,\n        bytes_filter: Union[List[int], None],\n        tracefield_filter: Union[List[str], None],\n    ) -&gt; List[segyio.tracefield.TraceField]:\n\n        filter_list = []\n        if bytes_filter is not None:\n            filter_list += [\n                segyio.tracefield.TraceField(byte_loc) for byte_loc in bytes_filter\n            ]\n\n        if tracefield_filter is not None:\n            filter_list += [\n                segyio.tracefield.TraceField(segyio.tracefield.keys[key])\n                for key in tracefield_filter\n            ]\n\n        if filter_list:\n            filter_list = list(set(filter_list))\n        else:\n            filter_list = [\n                segyio.tracefield.TraceField(byte)\n                for byte in segyio.tracefield.keys.values()\n            ]\n\n        return filter_list\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.fh.close()\n\n    def __iter__(self) -&gt; Generator[Dict[str, Any], None, None]:\n        return self[:]\n\n    def __getitem__(\n        self, i: Union[int, slice]\n    ) -&gt; Generator[Dict[str, Any], None, None]:\n        silent = Progress._segysak_tqdm_kwargs[\"disable\"]\n        if isinstance(i, int):\n            silent = True\n            n = 1\n        else:\n            silent = False\n            n = len(range(*i.indices(self.ntraces)))\n\n        with Progress(unit=\" traces\", total=n) as pbar:\n            for header in self.fh.header[i]:\n                pbar.update(1)\n                yield {key: header[key] for key in self.filter}\n\n    def to_dataframe(self, selection: Union[int, slice, None] = None) -&gt; pd.DataFrame:\n        \"\"\"Return the Trace Headers as a DataFrame\n\n        Args:\n            selection: A subset of trace headers will be returned based on trace numbering.\n        \"\"\"\n        if isinstance(selection, int):\n            index = pd.Index(range(i, i + 1))\n        elif isinstance(selection, slice):\n            index = pd.Index(range(*selection.indices(self.ntraces)))\n        else:\n            index = pd.Index(range(self.ntraces))\n            selection = slice(None, None, None)\n\n        columns = tuple(str(f) for f in self.filter)\n\n        head_df = pd.DataFrame(index=index, columns=columns)\n        # This is slightly faster than building from dicts\n        head_df.iloc[:, :] = np.vstack([list(h.values()) for h in self[selection]])\n\n        # fix bad values\n        # head_df = head_df.replace(to_replace=-2147483648, value=np.nan)\n        # convert numeric\n        for col in head_df:\n            head_df[col] = pd.to_numeric(head_df[col], downcast=\"integer\")\n\n        return head_df\n</code></pre>"},{"location":"api/segy_inspecting.html#segysak.segy.TraceHeaders.to_dataframe","title":"<code>to_dataframe(selection=None)</code>","text":"<p>Return the Trace Headers as a DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>selection</code> <code>Union[int, slice, None]</code> <p>A subset of trace headers will be returned based on trace numbering.</p> <code>None</code> Source code in <code>segysak/segy/_segy_headers.py</code> Python<pre><code>def to_dataframe(self, selection: Union[int, slice, None] = None) -&gt; pd.DataFrame:\n    \"\"\"Return the Trace Headers as a DataFrame\n\n    Args:\n        selection: A subset of trace headers will be returned based on trace numbering.\n    \"\"\"\n    if isinstance(selection, int):\n        index = pd.Index(range(i, i + 1))\n    elif isinstance(selection, slice):\n        index = pd.Index(range(*selection.indices(self.ntraces)))\n    else:\n        index = pd.Index(range(self.ntraces))\n        selection = slice(None, None, None)\n\n    columns = tuple(str(f) for f in self.filter)\n\n    head_df = pd.DataFrame(index=index, columns=columns)\n    # This is slightly faster than building from dicts\n    head_df.iloc[:, :] = np.vstack([list(h.values()) for h in self[selection]])\n\n    # fix bad values\n    # head_df = head_df.replace(to_replace=-2147483648, value=np.nan)\n    # convert numeric\n    for col in head_df:\n        head_df[col] = pd.to_numeric(head_df[col], downcast=\"integer\")\n\n    return head_df\n</code></pre>"},{"location":"api/segy_inspecting.html#segysak.segy.segy_header_scrape","title":"<code>segy_header_scrape(segy_file, partial_scan=None, bytes_filter=None, chunk=100000, **segyio_kwargs)</code>","text":"<p>Scape all data from segy trace headers</p> <p>Parameters:</p> Name Type Description Default <code>segy_file</code> <code>Union[str, PathLike]</code> <p>SEG-Y File path</p> required <code>partial_scan</code> <code>Union[int, None]</code> <p>Setting partial scan to a positive int will scan only that many traces. Defaults to None.</p> <code>None</code> <code>bytes_filter</code> <code>Union[List[int], None]</code> <p>List of byte locations to load exclusively.</p> <code>None</code> <code>chunk</code> <code>int</code> <p>Number of traces to read in one go.</p> <code>100000</code> <code>segyio_kwargs</code> <code>Any</code> <p>Arguments passed to segyio.open</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: Raw header information in table for scanned traces.</p> Source code in <code>segysak/segy/_segy_headers.py</code> Python<pre><code>def segy_header_scrape(\n    segy_file: Union[str, os.PathLike],\n    partial_scan: Union[int, None] = None,\n    bytes_filter: Union[List[int], None] = None,\n    chunk: int = 100_000,\n    **segyio_kwargs: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Scape all data from segy trace headers\n\n    Args:\n        segy_file: SEG-Y File path\n        partial_scan: Setting partial scan to a positive int will scan only\n            that many traces. Defaults to None.\n        bytes_filter: List of byte locations to load exclusively.\n        chunk: Number of traces to read in one go.\n        segyio_kwargs: Arguments passed to segyio.open\n\n    Returns:\n        pandas.DataFrame: Raw header information in table for scanned traces.\n    \"\"\"\n    with TraceHeaders(segy_file, bytes_filter=bytes_filter, **segyio_kwargs) as headers:\n        if partial_scan is not None:\n            ntraces = partial_scan\n        else:\n            ntraces = headers.ntraces\n\n        chunks = ntraces // chunk + min(ntraces % chunk, 1)\n        _dfs = []\n        with Progress(unit=\" trace-chunks\", total=chunks) as pbar:\n            for chk in range(0, chunks):\n                chk_slc = slice(chk * chunk, min((chk + 1) * chunk, ntraces), None)\n                _dfs.append(headers.to_dataframe(selection=chk_slc))\n                pbar.update(1)\n\n    head_df = pd.concat(_dfs)\n    return head_df\n</code></pre>"},{"location":"api/segy_inspecting.html#segysak.segy.segy_bin_scrape","title":"<code>segy_bin_scrape(segy_file, **segyio_kwargs)</code>","text":"<p>Scrape binary header</p> <p>Parameters:</p> Name Type Description Default <code>segy_file</code> <code>Union[str, PathLike]</code> <p>SEG-Y file path</p> required <code>segyio_kwargs</code> <code>Any</code> <p>Arguments passed to segyio.open</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Binary header key value pairs</p> Source code in <code>segysak/segy/_segy_headers.py</code> Python<pre><code>def segy_bin_scrape(segy_file: Union[str, os.PathLike], **segyio_kwargs: Any) -&gt; Dict:\n    \"\"\"Scrape binary header\n\n    Args:\n        segy_file: SEG-Y file path\n        segyio_kwargs: Arguments passed to segyio.open\n\n    Returns:\n        Binary header key value pairs\n    \"\"\"\n    bk = _active_binfield_segyio()\n    segyio_kwargs[\"ignore_geometry\"] = True\n    with segyio.open(segy_file, \"r\", **segyio_kwargs) as segyf:\n        return {key: segyf.bin[item] for key, item in bk.items()}\n</code></pre>"},{"location":"api/segy_inspecting.html#segysak.segy.segy_header_scan","title":"<code>segy_header_scan(segy_file, max_traces_scan=1000, **segyio_kwargs)</code>","text":"<p>Perform a scan of the segy file headers and return ranges.</p> <p>To get the complete raw header values see <code>segy_header_scrape</code></p> <p>Parameters:</p> Name Type Description Default <code>segy_file</code> <code>Union[str, PathLike]</code> <p>SEG-Y file path</p> required <code>max_traces_scan</code> <code>int</code> <p>Number of traces to scan. For scan all traces set to &lt;= 0. Defaults to 1000.</p> <code>1000</code> <code>segyio_kwargs</code> <code>Any</code> <p>Arguments passed to segyio.open</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Uses pandas describe to return statistics of your headers.</p> Source code in <code>segysak/segy/_segy_headers.py</code> Python<pre><code>def segy_header_scan(\n    segy_file: Union[str, os.PathLike],\n    max_traces_scan: int = 1000,\n    **segyio_kwargs: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Perform a scan of the segy file headers and return ranges.\n\n    To get the complete raw header values see `segy_header_scrape`\n\n    Args:\n        segy_file: SEG-Y file path\n        max_traces_scan: Number of traces to scan. For scan all traces set to &lt;= 0. Defaults to 1000.\n        segyio_kwargs: Arguments passed to segyio.open\n\n    Returns:\n        Uses pandas describe to return statistics of your headers.\n    \"\"\"\n    if max_traces_scan &lt;= 0:\n        max_traces_scan = None\n    else:\n        if not isinstance(max_traces_scan, int):\n            raise ValueError(\"max_traces_scan must be int\")\n\n    head_df = segy_header_scrape(segy_file, max_traces_scan, **segyio_kwargs)\n\n    header_keys = head_df.describe().T\n    pre_cols = list(header_keys.columns)\n    header_keys[\"byte_loc\"] = [segyio.tracefield.keys[key] for key in header_keys.index]\n    header_keys = header_keys[[\"byte_loc\"] + pre_cols]\n    header_keys.nscan = head_df.shape[0]\n    return header_keys\n</code></pre>"},{"location":"api/segy_inspecting.html#segysak.segy.get_segy_texthead","title":"<code>get_segy_texthead(segy_file, ext_headers=False, no_richstr=False, **segyio_kwargs)</code>","text":"<p>Return the ebcidc header as a Python string. New lines are separated by the <code>\\n</code> char.</p> <p>Parameters:</p> Name Type Description Default <code>segy_file</code> <code>Union[str, PathLike]</code> <p>Segy File Path</p> required <code>ext_headers</code> <code>bool</code> <p>Return EBCIDC and extended headers in list. Defaults to False</p> <code>False</code> <code>no_richstr</code> <code>bool</code> <p>Defaults to False. If true the returned string will not be updated for pretty HTML printing.</p> <code>False</code> <code>segyio_kwargs</code> <code>Dict[str, Any]</code> <p>Key word arguments to pass to segyio.open</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>text</code> <code>str</code> <p>Returns the EBCIDC text as a formatted paragraph.</p> Source code in <code>segysak/segy/_segy_text.py</code> Python<pre><code>def get_segy_texthead(\n    segy_file: Union[str, os.PathLike],\n    ext_headers: bool = False,\n    no_richstr: bool = False,\n    **segyio_kwargs: Dict[str, Any],\n) -&gt; str:\n    \"\"\"Return the ebcidc header as a Python string. New lines are separated by the `\\\\n` char.\n\n    Args:\n        segy_file: Segy File Path\n        ext_headers: Return EBCIDC and extended headers in list. Defaults to False\n        no_richstr: Defaults to False. If true the returned string\n            will not be updated for pretty HTML printing.\n        segyio_kwargs: Key word arguments to pass to segyio.open\n\n    Returns:\n        text: Returns the EBCIDC text as a formatted paragraph.\n    \"\"\"\n\n    with open(segy_file, mode=\"rb\") as f:\n        f.seek(0, 0)  # Locate our position to first byte of file\n        data = f.read(3200)  # Read the first 3200 byte from our position\n\n    if _isascii(data) and ext_headers == False:\n        encoding = \"ascii\"\n    elif ext_headers == False:\n        encoding = \"cp500\"  # text is ebcidc\n    else:\n        encoding = \"ebcidc\"\n\n    if encoding in [\"ascii\", \"cp500\"]:\n        lines = []\n        # doing it this way ensure we split the bytes appropriately across the 40 lines.\n        for i in range(0, 3200, 80):\n            lines.append(data[i : i + 80].decode(\"cp500\"))\n            text = \"\\n\".join(lines)\n    else:\n        segyio_kwargs[\"ignore_geometry\"] = True\n        try:  # pray that the encoding is ebcidc\n            with segyio.open(segy_file, \"r\", **segyio_kwargs) as segyf:\n                text = segyf.text[0].decode(\"ascii\", \"replace\")\n                text = _text_fixes(text)\n                text = segyio.tools.wrap(text)\n                if segyf.ext_headers and ext_headers:\n                    text2 = segyf.text[1].decode(\"ascii\", \"replace\")\n                    text = [text, text2]\n        except UnicodeDecodeError as err:\n            print(err)\n            print(\"The segy text header could not be decoded.\")\n\n    text = _text_fixes(text)\n\n    if no_richstr:\n        return text\n    else:\n        return _upgrade_txt_richstr(text)\n</code></pre>"},{"location":"api/segy_inspecting.html#segysak.segy.header_as_dimensions","title":"<code>header_as_dimensions(head_df, dims)</code>","text":"<p>Convert dim_kwargs to a dictionary of dimensions. Also useful for checking geometry is correct and unique for each trace in a segy file header.</p> <p>Parameters:</p> Name Type Description Default <code>head_df</code> <code>DataFrame</code> <p>The header DataFrame from <code>segy_header_scrape</code>.</p> required <code>dims</code> <code>tuple</code> <p>Dimension names as per head_df column names.</p> required <p>Returns:</p> Name Type Description <code>dims</code> <code>Dict[str, array]</code> <p>Dimension name and label pairs.</p> Source code in <code>segysak/segy/_segy_headers.py</code> Python<pre><code>def header_as_dimensions(head_df: pd.DataFrame, dims: tuple) -&gt; Dict[str, np.array]:\n    \"\"\"Convert dim_kwargs to a dictionary of dimensions. Also useful for checking\n    geometry is correct and unique for each trace in a segy file header.\n\n    Args:\n        head_df: The header DataFrame from `segy_header_scrape`.\n        dims: Dimension names as per head_df column names.\n\n    Returns:\n        dims: Dimension name and label pairs.\n    \"\"\"\n    unique_dims = dict()\n    for dim in dims:\n        # get unique values of dimension and sort them ascending\n        as_unique = head_df[dim].unique()\n        unique_dims[dim] = np.sort(as_unique)\n\n    if head_df[list(dims)].shape != head_df[list(dims)].drop_duplicates().shape:\n        raise ValueError(\n            \"The selected dimensions results in multiple traces per \"\n            \"dimension location, add additional dimensions or use \"\n            \"trace numbering byte location to load as 2D.\"\n        )\n\n    return unique_dims\n</code></pre>"},{"location":"api/segy_loading.html","title":"Depreciated Loading SEG-Y","text":"<p>Warning</p> <p>These methods have been depreciated in favour of the direct Xarray backend engine interface. Examples in this documentation have been updated to use the backend engine or consult the Backend Engine reference.</p>"},{"location":"api/segy_loading.html#segysak.segy.segy_loader","title":"<code>segy_loader(segyfile, cdp=None, iline=None, xline=None, cdp_x=None, cdp_y=None, offset=None, vert_domain='TWT', data_type='AMP', ix_crop=None, cdp_crop=None, xy_crop=None, z_crop=None, return_geometry=False, extra_byte_fields=None, head_df=None, **segyio_kwargs)</code>","text":"<p>Load SEG-Y file into xarray.Dataset</p> <p>The output dataset has the following structure     Dimensions:         cdp/iline - CDP or Inline axis         xline - Xline axis         twt/depth - The vertical axis         offset - Offset/Angle Axis     Coordinates:         iline - The inline numbering         xline - The xline numbering         cdp_x - Eastings         cdp_y - Northings         cdp - Trace Number for 2d     Variables         data - The data volume     Attributes:         ns - number of samples vertical         sample_rate - sample rate in ms/m         test - text header         measurement_system : m/ft         source_file : segy source         srd : seismic reference datum         percentiles : data amplitude percentiles         coord_scalar : from trace headers</p> <p>Parameters:</p> Name Type Description Default <code>segyfile</code> <code>str</code> <p>Input segy file path</p> required <code>cdp</code> <code>int</code> <p>The CDP byte location, usually 21.</p> <code>None</code> <code>iline</code> <code>int</code> <p>Inline byte location, usually 189</p> <code>None</code> <code>xline</code> <code>int</code> <p>Cross-line byte location, usually 193</p> <code>None</code> <code>cdp_x</code> <code>int</code> <p>UTMX byte location, usually 181</p> <code>None</code> <code>cdp_y</code> <code>int</code> <p>UTMY byte location, usually 185</p> <code>None</code> <code>offset</code> <code>int</code> <p>Offset/angle byte location</p> <code>None</code> <code>vert_domain</code> <code>str</code> <p>Vertical sampling domain. One of ['TWT', 'DEPTH']. Defaults to 'TWT'.</p> <code>'TWT'</code> <code>data_type</code> <code>str</code> <p>Data type ['AMP', 'VEL']. Defaults to 'AMP'.</p> <code>'AMP'</code> <code>ix_crop</code> <code>list</code> <p>List of minimum and maximum inline and crossline to output. Has the form '[min_il, max_il, min_xl, max_xl]'. Ignored for 2D data.</p> <code>None</code> <code>cdp_crop</code> <code>list</code> <p>List of minimum and maximum cmp values to output. Has the form '[min_cmp, max_cmp]'. Ignored for 3D data.</p> <code>None</code> <code>xy_crop</code> <code>list</code> <p>List of minimum and maximum cdp_x and cdp_y to output. Has the form '[min_x, max_x, min_y, max_y]'. Ignored for 2D data.</p> <code>None</code> <code>z_crop</code> <code>list</code> <p>List of minimum and maximum vertical samples to output. Has the form '[min, max]'.</p> <code>None</code> <code>return_geometry</code> <code>bool</code> <p>If true returns an xarray.dataset which doesn't contain data but mirrors the input volume header information.</p> <code>False</code> <code>extra_byte_fields</code> <code>list / mapping</code> <p>A list of int or mapping of byte fields that should be returned as variables in the dataset.</p> <code>None</code> <code>head_df</code> <code>DataFrame</code> <p>The DataFrame output from <code>segy_header_scrape</code>. This DataFrame can be filtered by the user to load select trace sets. Trace loading is based upon the DataFrame index.</p> <code>None</code> <code>**segyio_kwargs</code> <p>Extra keyword arguments for segyio.open</p> <code>{}</code> <p>Returns:</p> Type Description <p>xarray.Dataset: If ncfile keyword is specified returns open handle to disk netcdf4, otherwise the data in memory. If return_geometry is True does not load trace data and returns headers in geometry.</p> Source code in <code>segysak/segy/_segy_loader.py</code> Python<pre><code>def segy_loader(\n    segyfile,\n    cdp=None,\n    iline=None,\n    xline=None,\n    cdp_x=None,\n    cdp_y=None,\n    offset=None,\n    vert_domain=\"TWT\",\n    data_type=\"AMP\",\n    ix_crop=None,\n    cdp_crop=None,\n    xy_crop=None,\n    z_crop=None,\n    return_geometry=False,\n    extra_byte_fields=None,\n    head_df=None,\n    **segyio_kwargs,\n):\n    \"\"\"Load SEG-Y file into xarray.Dataset\n\n    The output dataset has the following structure\n        Dimensions:\n            cdp/iline - CDP or Inline axis\n            xline - Xline axis\n            twt/depth - The vertical axis\n            offset - Offset/Angle Axis\n        Coordinates:\n            iline - The inline numbering\n            xline - The xline numbering\n            cdp_x - Eastings\n            cdp_y - Northings\n            cdp - Trace Number for 2d\n        Variables\n            data - The data volume\n        Attributes:\n            ns - number of samples vertical\n            sample_rate - sample rate in ms/m\n            test - text header\n            measurement_system : m/ft\n            source_file : segy source\n            srd : seismic reference datum\n            percentiles : data amplitude percentiles\n            coord_scalar : from trace headers\n\n    Args:\n        segyfile (str): Input segy file path\n        cdp (int, optional): The CDP byte location, usually 21.\n        iline (int, optional): Inline byte location, usually 189\n        xline (int, optional): Cross-line byte location, usually 193\n        cdp_x (int, optional): UTMX byte location, usually 181\n        cdp_y (int, optional): UTMY byte location, usually 185\n        offset (int, optional): Offset/angle byte location\n        vert_domain (str, optional): Vertical sampling domain. One of ['TWT', 'DEPTH']. Defaults to 'TWT'.\n        data_type (str, optional): Data type ['AMP', 'VEL']. Defaults to 'AMP'.\n        ix_crop (list, optional): List of minimum and maximum inline and crossline to output.\n            Has the form '[min_il, max_il, min_xl, max_xl]'. Ignored for 2D data.\n        cdp_crop (list, optional): List of minimum and maximum cmp values to output.\n            Has the form '[min_cmp, max_cmp]'. Ignored for 3D data.\n        xy_crop (list, optional): List of minimum and maximum cdp_x and cdp_y to output.\n            Has the form '[min_x, max_x, min_y, max_y]'. Ignored for 2D data.\n        z_crop (list, optional): List of minimum and maximum vertical samples to output.\n            Has the form '[min, max]'.\n        return_geometry (bool, optional): If true returns an xarray.dataset which doesn't contain data but mirrors\n            the input volume header information.\n        extra_byte_fields (list/mapping): A list of int or mapping of byte fields that should be returned as variables in the dataset.\n        head_df (pandas.DataFrame): The DataFrame output from `segy_header_scrape`. This DataFrame can be filtered by the user\n            to load select trace sets. Trace loading is based upon the DataFrame index.\n        **segyio_kwargs: Extra keyword arguments for segyio.open\n\n    Returns:\n        xarray.Dataset: If ncfile keyword is specified returns open handle to disk netcdf4,\n            otherwise the data in memory. If return_geometry is True does not load trace data and\n            returns headers in geometry.\n    \"\"\"\n    warn(\n        \"segy_loader will be removed in v0.6, please use the Xarray engine ds = xr.open_dataset(segy_file) method instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    extra_byte_fields = _loader_converter_checks(cdp, iline, xline, extra_byte_fields)\n\n    head_df, head_bin, head_loc = _loader_converter_header_handling(\n        segyfile,\n        cdp=cdp,\n        iline=iline,\n        xline=xline,\n        cdp_x=cdp_x,\n        cdp_y=cdp_y,\n        offset=offset,\n        vert_domain=vert_domain,\n        data_type=data_type,\n        ix_crop=ix_crop,\n        cdp_crop=cdp_crop,\n        xy_crop=xy_crop,\n        z_crop=z_crop,\n        return_geometry=return_geometry,\n        silent=Progress.silent(),\n        extra_byte_fields=extra_byte_fields,\n        head_df=head_df,\n        **segyio_kwargs,\n    )\n\n    byte_loc = [\n        bytel\n        for bytel in [cdp, iline, xline, cdp_x, cdp_y, offset]\n        if bytel is not None\n    ]\n    check_tracefield(byte_loc)\n\n    common_args = (segyfile, head_df, head_bin, head_loc)\n\n    common_kwargs = dict(\n        zcrop=z_crop,\n        vert_domain=vert_domain,\n        data_type=data_type,\n        return_geometry=return_geometry,\n        silent=Progress.silent(),\n    )\n\n    # 3d data needs iline and xline\n    if all(v is not None for v in (head_loc.iline, head_loc.xline)):\n        print(\"Loading as 3D\")\n        ds = _3dsegy_loader(\n            *common_args,\n            **common_kwargs,\n            **segyio_kwargs,\n        )\n        indexer = [\"il_index\", \"xl_index\"]\n        dims = (\n            DimensionKeyField.threed_head\n            if offset is None\n            else DimensionKeyField.threed_ps_head\n        )\n        is3d2d = True\n\n    # 2d data\n    elif head_loc.cdp is not None:\n        print(\"Loading as 2D\")\n        ds = _2dsegy_loader(*common_args, **common_kwargs, **segyio_kwargs)\n        indexer = [\"cdp_index\"]\n        dims = (\n            DimensionKeyField.twod_head\n            if offset is None\n            else DimensionKeyField.twod_ps_head\n        )\n        is3d2d = True\n\n    # fallbak to just a 2d array of traces\n    else:\n        ds = _2dsegy_loader(*common_args, **common_kwargs, **segyio_kwargs)\n        indexer = []\n        dims = DimensionKeyField.cdp_2d\n        is3d2d = False\n\n    indexer = indexer + [\"off_index\"] if offset is not None else indexer\n\n    ds = _loader_converter_write_headers(\n        ds, head_df, indexer, dims, extra_byte_fields, is3d2d=is3d2d\n    )\n\n    # ds.seis.get_corner_points()\n    return ds\n</code></pre>"},{"location":"api/segy_loading.html#segysak.segy.segy_freeloader","title":"<code>segy_freeloader(segyfile, vert_domain='TWT', data_type='AMP', return_geometry=False, extra_byte_fields=None, head_df=None, segyio_kwargs=None, **dim_kwargs)</code>","text":"<p>Freeform loader for SEG-Y data. This loader allows you to load SEG-Y into an xarray.Dataset using an arbitrary number of header locations to create othogonal dimensions. This is an eager loader and will transfer the entire SEG-Y and requested header information to memory.</p> <p>From the dimension header locations specified the freeloader will try to create a Dataset where each trace is assigned to a dimension.</p> <p>Parameters:</p> Name Type Description Default <code>segyfile</code> <code>string</code> <p>The SEG-Y file/path.</p> required <code>vert_domain</code> <code>str</code> <p>One of ('TWT', 'DEPTH'). Defaults to 'TWT'.</p> <code>'TWT'</code> <code>data_type</code> <code>str</code> <p>Defaults to \"AMP\".</p> <code>'AMP'</code> <code>return_geometry</code> <code>bool</code> <p>If true, just returned the empty dataset based upon the calcuated header geometry. Defaults to False.</p> <code>False</code> <code>extra_byte_fields</code> <code>dict</code> <p>Additional header information to load into the Dataset. Defaults to None.</p> <code>None</code> <code>head_df</code> <code>DataFrame</code> <p>The DataFrame output from <code>segy_header_scrape</code>. This DataFrame can be filtered by the user to load select trace sets. Trace loading is based upon the DataFrame index.</p> <code>None</code> <code>segyio_kwargs</code> <code>dict</code> <p>Extra keyword arguments for segyio.open</p> <code>None</code> <code>**dim_kwargs</code> <p>Dimension names and byte location pairs.</p> <code>{}</code> Source code in <code>segysak/segy/_segy_loader.py</code> Python<pre><code>def segy_freeloader(\n    segyfile,\n    vert_domain=\"TWT\",\n    data_type=\"AMP\",\n    return_geometry=False,\n    extra_byte_fields=None,\n    head_df=None,\n    segyio_kwargs=None,\n    **dim_kwargs,\n):\n    \"\"\"Freeform loader for SEG-Y data. This loader allows you to load SEG-Y into\n    an xarray.Dataset using an arbitrary number of header locations to create\n    othogonal dimensions. This is an eager loader and will transfer the entire\n    SEG-Y and requested header information to memory.\n\n    From the dimension header locations specified the freeloader will try to\n    create a Dataset where each trace is assigned to a dimension.\n\n    Args:\n        segyfile (string): The SEG-Y file/path.\n        vert_domain (str, optional): One of ('TWT', 'DEPTH'). Defaults to 'TWT'.\n        data_type (str, optional): Defaults to \"AMP\".\n        return_geometry (bool, optional): If true, just returned the empty\n            dataset based upon the calcuated header geometry. Defaults to False.\n        extra_byte_fields (dict, optional): Additional header information to\n            load into the Dataset. Defaults to None.\n        head_df (pandas.DataFrame): The DataFrame output from `segy_header_scrape`.\n            This DataFrame can be filtered by the user\n            to load select trace sets. Trace loading is based upon the DataFrame index.\n        segyio_kwargs (dict, optional): Extra keyword arguments for segyio.open\n        **dim_kwargs: Dimension names and byte location pairs.\n    \"\"\"\n    warn(\n        \"segy_loader will be removed in v0.6, please use the Xarray engine ds = xr.open_dataset(segy_file) method instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    if segyio_kwargs is None:\n        segyio_kwargs = dict()\n\n    if head_df is None:\n        # Start by scraping the headers.\n        head_df = segy_header_scrape(segyfile, **segyio_kwargs)\n\n    head_bin = segy_bin_scrape(segyfile, **segyio_kwargs)\n\n    # get vertical sample ranges\n    n0 = 0\n    nsamp = head_bin[\"Samples\"]\n    ns0 = head_df.DelayRecordingTime.min()\n\n    # binary header translation\n    nsamp = head_bin[\"Samples\"]\n    sample_rate = head_bin[\"Interval\"] / 1000.0\n    msys = _SEGY_MEASUREMENT_SYSTEM[head_bin[\"MeasurementSystem\"]]\n    vert_samples = np.arange(ns0, ns0 + sample_rate * nsamp, sample_rate, dtype=int)\n\n    # creating dimensions and new dataset\n    dims = dict()\n    dim_index_names = list()\n    dim_fields = list()\n    for dim in dim_kwargs:\n        trace_field = str(segyio.TraceField(dim_kwargs[dim]))\n        if trace_field == \"Unknown Enum\":\n            raise ValueError(f\"{dim}:{dim_kwargs[dim]} was not a valid byte header\")\n        dim_fields.append(trace_field)\n        as_unique = head_df[trace_field].unique()\n        dims[dim] = np.sort(as_unique)\n        d_map = {dval: i for i, dval in enumerate(as_unique)}\n        index_name = f\"{dim}_index\"\n        dim_index_names.append(index_name)\n        head_df.loc[:, index_name] = head_df[trace_field].map(d_map)\n\n    if (\n        head_df[dim_index_names].shape\n        != head_df[dim_index_names].drop_duplicates().shape\n    ):\n        raise ValueError(\n            \"The selected dimensions results in multiple traces per \"\n            \"dimension location, add additional dimensions or use \"\n            \"trace numbering to load as 2D.\"\n        )\n\n    builder, domain = _dataset_coordinate_helper(vert_samples, vert_domain, **dims)\n    ds = create_seismic_dataset(**builder)\n\n    # getting attributes\n    text = get_segy_texthead(segyfile, **segyio_kwargs)\n    ds.attrs[AttrKeyField.text] = text\n    ds.attrs[AttrKeyField.source_file] = pathlib.Path(segyfile).name\n    ds.attrs[AttrKeyField.measurement_system] = msys\n    ds.attrs[AttrKeyField.sample_rate] = sample_rate\n\n    # map extra byte fields into ds\n    if extra_byte_fields is not None:\n        to_add = list()\n        for name, byte in extra_byte_fields.items():\n            trace_field = str(segyio.TraceField(byte))\n            if trace_field == \"Unknown Enum\":\n                raise ValueError(f\"{name}:{byte} was not a valid byte header\")\n            to_add.append(trace_field)\n        to_add = to_add + dim_fields\n        extras = head_df[to_add].set_index(dim_fields).to_xarray()\n        extras = extras.rename_dims(\n            {b: a for a, b in zip(dim_kwargs, dim_fields) if a != b}\n        )\n        for name, xtr in zip(extra_byte_fields, to_add):\n            ds[name] = extras[xtr]\n\n    if return_geometry:\n        # return geometry -&gt; e.g. don't process segy traces\n        return ds\n\n    segyio_kwargs.update(dict(ignore_geometry=True))\n\n    with segyio.open(segyfile, \"r\", **segyio_kwargs) as segyf:\n\n        segyf.mmap()\n        shape = [ds.sizes[d] for d in dim_kwargs] + [vert_samples.size]\n        volume = np.zeros(shape, dtype=np.float32)\n\n        # this can probably be done as a block - leaving for now just incase sorting becomes an issue\n        indexes = tuple([head_df[idx].values for idx in dim_index_names])\n        volume[indexes] = segyf.trace.raw[:][head_df.index.values]\n\n    percentiles = np.percentile(volume, PERCENTILES)\n    ds[VariableKeyField.data] = (\n        list(dim_kwargs) + [VerticalKeyDim[domain]],\n        volume,\n    )\n    ds.attrs[AttrKeyField.percentiles] = list(percentiles)\n\n    return ds\n</code></pre>"},{"location":"api/segy_loading.html#segysak.segy.segy_converter","title":"<code>segy_converter(segyfile, ncfile, cdp=None, iline=None, xline=None, cdp_x=None, cdp_y=None, offset=None, vert_domain='TWT', data_type='AMP', ix_crop=None, cdp_crop=None, xy_crop=None, z_crop=None, return_geometry=False, extra_byte_fields=None, **segyio_kwargs)</code>","text":"<p>Convert SEG-Y data to NetCDF4 File</p> <p>The output ncfile has the following structure     Dimensions:         cdp/iline - CDP or Inline axis         xline - Xline axis         twt/depth - The vertical axis         offset - Offset/Angle Axis     Coordinates:         iline - The inline numbering         xline - The xline numbering         cdp_x - Eastings         cdp_y - Northings         cdp - Trace Number for 2d     Variables         data - The data volume     Attributes:         ns - number of samples vertical         sample_rate - sample rate in ms/m         test - text header         measurement_system : m/ft         source_file : segy source         srd : seismic reference datum         percentiles : data amplitude percentiles         coord_scalar : from trace headers</p> <p>Parameters:</p> Name Type Description Default <code>segyfile</code> <code>str</code> <p>Input segy file path</p> required <code>ncfile</code> <code>str</code> <p>Output SEISNC file path. If none the loaded data will be returned in memory as an xarray.Dataset.</p> required <code>cdp</code> <code>int</code> <p>The CDP byte location, usually 21.</p> <code>None</code> <code>iline</code> <code>int</code> <p>Inline byte location, usually 189</p> <code>None</code> <code>xline</code> <code>int</code> <p>Cross-line byte location, usually 193</p> <code>None</code> <code>cdp_x</code> <code>int</code> <p>UTMX byte location, usually 181</p> <code>None</code> <code>cdp_y</code> <code>int</code> <p>UTMY byte location, usually 185</p> <code>None</code> <code>offset</code> <code>int</code> <p>Offset/angle byte location</p> <code>None</code> <code>vert_domain</code> <code>str</code> <p>Vertical sampling domain. One of ['TWT', 'DEPTH']. Defaults to 'TWT'.</p> <code>'TWT'</code> <code>data_type</code> <code>str</code> <p>Data type ['AMP', 'VEL']. Defaults to 'AMP'.</p> <code>'AMP'</code> <code>ix_crop</code> <code>list</code> <p>List of minimum and maximum inline and crossline to output. Has the form '[min_il, max_il, min_xl, max_xl]'. Ignored for 2D data.</p> <code>None</code> <code>cdp_crop</code> <code>list</code> <p>List of minimum and maximum cmp values to output. Has the form '[min_cmp, max_cmp]'. Ignored for 3D data.</p> <code>None</code> <code>xy_crop</code> <code>list</code> <p>List of minimum and maximum cdp_x and cdp_y to output. Has the form '[min_x, max_x, min_y, max_y]'. Ignored for 2D data.</p> <code>None</code> <code>z_crop</code> <code>list</code> <p>List of minimum and maximum vertical samples to output. Has the form '[min, max]'.</p> <code>None</code> <code>return_geometry</code> <code>bool</code> <p>If true returns an xarray.dataset which doesn't contain data but mirrors the input volume header information.</p> <code>False</code> <code>silent</code> <code>bool</code> <p>Disable progress bar.</p> required <code>extra_byte_fields</code> <code>list / mapping</code> <p>A list of int or mapping of byte fields that should be returned as variables in the dataset.a</p> <code>None</code> <code>**segyio_kwargs</code> <p>Extra keyword arguments for segyio.open</p> <code>{}</code> Source code in <code>segysak/segy/_segy_loader.py</code> Python<pre><code>def segy_converter(\n    segyfile,\n    ncfile,\n    cdp=None,\n    iline=None,\n    xline=None,\n    cdp_x=None,\n    cdp_y=None,\n    offset=None,\n    vert_domain=\"TWT\",\n    data_type=\"AMP\",\n    ix_crop=None,\n    cdp_crop=None,\n    xy_crop=None,\n    z_crop=None,\n    return_geometry=False,\n    extra_byte_fields=None,\n    **segyio_kwargs,\n):\n    \"\"\"Convert SEG-Y data to NetCDF4 File\n\n    The output ncfile has the following structure\n        Dimensions:\n            cdp/iline - CDP or Inline axis\n            xline - Xline axis\n            twt/depth - The vertical axis\n            offset - Offset/Angle Axis\n        Coordinates:\n            iline - The inline numbering\n            xline - The xline numbering\n            cdp_x - Eastings\n            cdp_y - Northings\n            cdp - Trace Number for 2d\n        Variables\n            data - The data volume\n        Attributes:\n            ns - number of samples vertical\n            sample_rate - sample rate in ms/m\n            test - text header\n            measurement_system : m/ft\n            source_file : segy source\n            srd : seismic reference datum\n            percentiles : data amplitude percentiles\n            coord_scalar : from trace headers\n\n    Args:\n        segyfile (str): Input segy file path\n        ncfile (str): Output SEISNC file path. If none the loaded data will be\n            returned in memory as an xarray.Dataset.\n        cdp (int, optional): The CDP byte location, usually 21.\n        iline (int, optional): Inline byte location, usually 189\n        xline (int, optional): Cross-line byte location, usually 193\n        cdp_x (int, optional): UTMX byte location, usually 181\n        cdp_y (int, optional): UTMY byte location, usually 185\n        offset (int, optional): Offset/angle byte location\n        vert_domain (str, optional): Vertical sampling domain. One of ['TWT', 'DEPTH']. Defaults to 'TWT'.\n        data_type (str, optional): Data type ['AMP', 'VEL']. Defaults to 'AMP'.\n        ix_crop (list, optional): List of minimum and maximum inline and crossline to output.\n            Has the form '[min_il, max_il, min_xl, max_xl]'. Ignored for 2D data.\n        cdp_crop (list, optional): List of minimum and maximum cmp values to output.\n            Has the form '[min_cmp, max_cmp]'. Ignored for 3D data.\n        xy_crop (list, optional): List of minimum and maximum cdp_x and cdp_y to output.\n            Has the form '[min_x, max_x, min_y, max_y]'. Ignored for 2D data.\n        z_crop (list, optional): List of minimum and maximum vertical samples to output.\n            Has the form '[min, max]'.\n        return_geometry (bool, optional): If true returns an xarray.dataset which doesn't contain data but mirrors\n            the input volume header information.\n        silent (bool): Disable progress bar.\n        extra_byte_fields (list/mapping): A list of int or mapping of byte fields that should be returned as variables in the dataset.a\n        **segyio_kwargs: Extra keyword arguments for segyio.open\n\n    \"\"\"\n    warn(\n        \"segy_converter will be removed in v0.6, please use Xarray engine ds = xr.open_dataset(file) and then accessor ds.seisio.to_segy method instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    # Input sanity checks\n    extra_byte_fields = _loader_converter_checks(cdp, iline, xline, extra_byte_fields)\n    byte_loc = [\n        bytel\n        for bytel in [cdp, iline, xline, cdp_x, cdp_y, offset]\n        if bytel is not None\n    ]\n    check_tracefield(byte_loc)\n\n    head_df, head_bin, head_loc = _loader_converter_header_handling(\n        segyfile,\n        cdp=cdp,\n        iline=iline,\n        xline=xline,\n        cdp_x=cdp_x,\n        cdp_y=cdp_y,\n        offset=offset,\n        vert_domain=vert_domain,\n        data_type=data_type,\n        ix_crop=ix_crop,\n        cdp_crop=cdp_crop,\n        xy_crop=xy_crop,\n        z_crop=z_crop,\n        return_geometry=return_geometry,\n        silent=Progress.silent(),\n        extra_byte_fields=extra_byte_fields,\n        optimised_load=True,\n        **segyio_kwargs,\n    )\n    print(\"header_loaded\")\n    common_args = (segyfile, head_df, head_bin, head_loc)\n\n    common_kwargs = dict(\n        zcrop=z_crop,\n        ncfile=ncfile,\n        vert_domain=vert_domain,\n        data_type=data_type,\n        return_geometry=return_geometry,\n        silent=Progress.silent(),\n    )\n\n    # 3d data needs iline and xline\n    if iline is not None and xline is not None:\n\n        print(\"is_3d\")\n        ds = _3dsegy_loader(\n            *common_args,\n            **common_kwargs,\n            **segyio_kwargs,\n        )\n        indexer = [\"il_index\", \"xl_index\"]\n        dims = (\n            DimensionKeyField.threed_head\n            if offset is None\n            else DimensionKeyField.threed_ps_head\n        )\n        is3d2d = True\n\n    # 2d data\n    elif cdp is not None:\n        ds = _2dsegy_loader(*common_args, **common_kwargs, **segyio_kwargs)\n        indexer = [\"cdp_index\"]\n        dims = (\n            DimensionKeyField.twod_head\n            if offset is None\n            else DimensionKeyField.twod_ps_head\n        )\n        is3d2d = True\n\n    # fallbak to just a 2d array of traces\n    else:\n        ds = _2dsegy_loader(*common_args, **common_kwargs, **segyio_kwargs)\n        indexer = []\n        dims = DimensionKeyField.cdp_2d\n        is3d2d = False\n\n    indexer = indexer + [\"off_index\"] if offset is not None else indexer\n\n    ds = _loader_converter_write_headers(\n        ds, head_df, indexer, dims, extra_byte_fields, is3d2d=is3d2d\n    )\n    new_vars = {key: ds[key] for key in extra_byte_fields}\n    ds.close()\n    del ds\n\n    with h5netcdf.File(ncfile, \"a\") as seisnc:\n        for var, darray in new_vars.items():\n            seisnc_var = seisnc.create_variable(\n                var, dimensions=darray.dims, dtype=darray.dtype\n            )\n            seisnc_var[...] = darray[...]\n            seisnc.flush()\n\n    return None\n</code></pre>"},{"location":"api/segy_loading.html#segysak.segy.well_known_byte_locs","title":"<code>well_known_byte_locs(name)</code>","text":"<p>Return common bytes position kwargs_dict for segy_loader and segy_converter.</p> <p>Returns a dict containing the byte locations for well known SEG-Y variants in the wild.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Takes one of keys from KNOWN_BYTES</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of SEG-Y byte positions.</p> <p>Example:</p> <p>Use the output of this function to unpack arguments into <code>segy_loader</code></p> <p>seismic = segy_loader(filepath, **well_known_byte_locs('petrel_3d'))</p> Source code in <code>segysak/segy/_segy_loader.py</code> Python<pre><code>def well_known_byte_locs(name):\n    \"\"\"Return common bytes position kwargs_dict for segy_loader and segy_converter.\n\n    Returns a dict containing the byte locations for well known SEG-Y variants in the wild.\n\n    Args:\n        name (str): Takes one of keys from KNOWN_BYTES\n\n    Returns:\n        dict: A dictionary of SEG-Y byte positions.\n\n    Example:\n\n    Use the output of this function to unpack arguments into ``segy_loader``\n\n    &gt;&gt;&gt; seismic = segy_loader(filepath, **well_known_byte_locs('petrel_3d'))\n\n    \"\"\"\n    try:\n        return KNOWN_BYTES[name]\n    except KeyError:\n        raise ValueError(\n            f\"No byte locatons for {name}, select from {list(KNOWN_BYTES.keys())}\"\n        )\n</code></pre>"},{"location":"api/segy_text.html","title":"SEG-Y text header operations","text":"<p>Use these functions to view or modify existing SEGY text headers or create new segysak compatable text headers.</p>"},{"location":"api/segy_text.html#segysak.segy.get_segy_texthead","title":"<code>get_segy_texthead(segy_file, ext_headers=False, no_richstr=False, **segyio_kwargs)</code>","text":"<p>Return the ebcidc header as a Python string. New lines are separated by the <code>\\n</code> char.</p> <p>Parameters:</p> Name Type Description Default <code>segy_file</code> <code>Union[str, PathLike]</code> <p>Segy File Path</p> required <code>ext_headers</code> <code>bool</code> <p>Return EBCIDC and extended headers in list. Defaults to False</p> <code>False</code> <code>no_richstr</code> <code>bool</code> <p>Defaults to False. If true the returned string will not be updated for pretty HTML printing.</p> <code>False</code> <code>segyio_kwargs</code> <code>Dict[str, Any]</code> <p>Key word arguments to pass to segyio.open</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>text</code> <code>str</code> <p>Returns the EBCIDC text as a formatted paragraph.</p> Source code in <code>segysak/segy/_segy_text.py</code> Python<pre><code>def get_segy_texthead(\n    segy_file: Union[str, os.PathLike],\n    ext_headers: bool = False,\n    no_richstr: bool = False,\n    **segyio_kwargs: Dict[str, Any],\n) -&gt; str:\n    \"\"\"Return the ebcidc header as a Python string. New lines are separated by the `\\\\n` char.\n\n    Args:\n        segy_file: Segy File Path\n        ext_headers: Return EBCIDC and extended headers in list. Defaults to False\n        no_richstr: Defaults to False. If true the returned string\n            will not be updated for pretty HTML printing.\n        segyio_kwargs: Key word arguments to pass to segyio.open\n\n    Returns:\n        text: Returns the EBCIDC text as a formatted paragraph.\n    \"\"\"\n\n    with open(segy_file, mode=\"rb\") as f:\n        f.seek(0, 0)  # Locate our position to first byte of file\n        data = f.read(3200)  # Read the first 3200 byte from our position\n\n    if _isascii(data) and ext_headers == False:\n        encoding = \"ascii\"\n    elif ext_headers == False:\n        encoding = \"cp500\"  # text is ebcidc\n    else:\n        encoding = \"ebcidc\"\n\n    if encoding in [\"ascii\", \"cp500\"]:\n        lines = []\n        # doing it this way ensure we split the bytes appropriately across the 40 lines.\n        for i in range(0, 3200, 80):\n            lines.append(data[i : i + 80].decode(\"cp500\"))\n            text = \"\\n\".join(lines)\n    else:\n        segyio_kwargs[\"ignore_geometry\"] = True\n        try:  # pray that the encoding is ebcidc\n            with segyio.open(segy_file, \"r\", **segyio_kwargs) as segyf:\n                text = segyf.text[0].decode(\"ascii\", \"replace\")\n                text = _text_fixes(text)\n                text = segyio.tools.wrap(text)\n                if segyf.ext_headers and ext_headers:\n                    text2 = segyf.text[1].decode(\"ascii\", \"replace\")\n                    text = [text, text2]\n        except UnicodeDecodeError as err:\n            print(err)\n            print(\"The segy text header could not be decoded.\")\n\n    text = _text_fixes(text)\n\n    if no_richstr:\n        return text\n    else:\n        return _upgrade_txt_richstr(text)\n</code></pre>"},{"location":"api/segy_text.html#segysak.segy.put_segy_texthead","title":"<code>put_segy_texthead(segy_file, ebcidc, line_counter=True, **segyio_kwargs)</code>","text":"<p>Puts a text header (ebcidc) into a SEG-Y file.</p> <p>Parameters:</p> Name Type Description Default <code>segy_file</code> <code>Union[str, PathLike]</code> <p>The path to the file to update.</p> required <code>ebcidc</code> <code>Union[str, List[str], Dict[int, str], ByteString]</code> <p>A standard string, new lines will be preserved. A list or lines to add. A dict with numeric keys for line numbers e.g. {1: 'line 1'}. A pre-encoded byte header to add to the SEG-Y file directly.</p> required <code>line_counter</code> <code>bool</code> <p>Add a line counter with format \"CXX \" to the start of each line. This reduces the maximum content per line to 76 chars.</p> <code>True</code> Source code in <code>segysak/segy/_segy_text.py</code> Python<pre><code>def put_segy_texthead(\n    segy_file: Union[str, os.PathLike],\n    ebcidc: Union[str, List[str], Dict[int, str], ByteString],\n    line_counter: bool = True,\n    **segyio_kwargs,\n):\n    \"\"\"Puts a text header (ebcidc) into a SEG-Y file.\n\n    Args:\n        segy_file: The path to the file to update.\n        ebcidc:\n            A standard string, new lines will be preserved.\n            A list or lines to add.\n            A dict with numeric keys for line numbers e.g. {1: 'line 1'}.\n            A pre-encoded byte header to add to the SEG-Y file directly.\n        line_counter: Add a line counter with format \"CXX \" to the start of each line.\n            This reduces the maximum content per line to 76 chars.\n    \"\"\"\n    header = \"\"\n    n = 80\n\n    if not isinstance(ebcidc, (str, list, dict, bytes)):\n        raise ValueError(f\"Unknown type for ebcidc: {type(ebcidc)}\")\n\n    if isinstance(ebcidc, dict):\n        lines = _process_dict_texthead(ebcidc, n)\n    elif isinstance(ebcidc, str):\n        lines = _process_string_texthead(ebcidc, n)\n    elif isinstance(ebcidc, list):\n        lines = ebcidc\n    else:\n        lines = []\n\n    if not isinstance(ebcidc, bytes):\n        lines = [\n            _process_line(line, i, line_counter=line_counter)\n            for i, line in enumerate(lines)\n        ]\n        # convert to bytes line by line to ensure end lines don't get pushed,\n        # truncate lines with bad chars instead\n        header = b\"\".join([ln.encode(\"utf-8\")[:n] for ln in lines])\n    else:\n        header = ebcidc\n\n    # check size\n    if len(header) &gt; 3200:\n        warn(\"Byte EBCIDC is too large - truncating\", UserWarning)\n        header = header[:3200]\n\n    segyio_kwargs[\"ignore_geometry\"] = True\n    with segyio.open(segy_file, \"r+\", **segyio_kwargs) as segyf:\n        segyf.text[0] = header\n</code></pre>"},{"location":"api/segy_text.html#segysak.segy.create_default_texthead","title":"<code>create_default_texthead(override=None)</code>","text":"<p>Returns a simple default textual header dictionary.</p> <p>Basic fields are auto populated and a dictionary indexing lines 1-40 can be passed to override keyword for adjustment. By default lines 6-34 are empty.</p> <p>Line length rules apply, so overrides will be truncated if they have &gt;80 chars.</p> <p>Parameters:</p> Name Type Description Default <code>override</code> <code>Union[Dict[int, str], None]</code> <p>Override any line with custom values. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>text_header</code> <code>Dict[int, str]</code> <p>Dictionary with keys 1-40 for textual header of SEG-Y file</p> <p>Example</p> <p>Override lines 7 and 8 of the default text header.</p> Python<pre><code>&gt;&gt;&gt; create_default_texthead(override={7:'Hello', 8:'World!'})\n{1: 'segysak SEG-Y Output',\n2: 'Data created by: username ',\n3: '',\n4: 'DATA FORMAT: SEG-Y;  DATE: 2019-06-09 15:14:00',\n5: 'DATA DESCRIPTION: SEG-Y format data output from segysak',\n6: '',\n7: 'Hello',\n8: 'World!',\n9: '',\n...\n34: '',\n35: '*** BYTE LOCATION OF KEY HEADERS ***',\n36: 'CMP UTM-X 181-184, ALL COORDS X100, CMP UTM-Y 185-188',\n37: 'INLINE 189-193, XLINE 194-198, ',\n38: '',\n39: '',\n40: 'END TEXTUAL HEADER'}\n</code></pre> Source code in <code>segysak/segy/_segy_text.py</code> Python<pre><code>def create_default_texthead(\n    override: Union[Dict[int, str], None] = None\n) -&gt; Dict[int, str]:\n    \"\"\"Returns a simple default textual header dictionary.\n\n    Basic fields are auto populated and a dictionary indexing lines 1-40 can\n    be passed to override keyword for adjustment. By default lines 6-34 are\n    empty.\n\n    Line length rules apply, so overrides will be truncated if they have &gt;80 chars.\n\n    Args:\n        override: Override any line with custom values. Defaults to None.\n\n    Returns:\n        text_header: Dictionary with keys 1-40 for textual header of SEG-Y file\n\n    !!! example\n        Override lines 7 and 8 of the default text header.\n\n        ```python\n        &gt;&gt;&gt; create_default_texthead(override={7:'Hello', 8:'World!'})\n        {1: 'segysak SEG-Y Output',\n        2: 'Data created by: username ',\n        3: '',\n        4: 'DATA FORMAT: SEG-Y;  DATE: 2019-06-09 15:14:00',\n        5: 'DATA DESCRIPTION: SEG-Y format data output from segysak',\n        6: '',\n        7: 'Hello',\n        8: 'World!',\n        9: '',\n        ...\n        34: '',\n        35: '*** BYTE LOCATION OF KEY HEADERS ***',\n        36: 'CMP UTM-X 181-184, ALL COORDS X100, CMP UTM-Y 185-188',\n        37: 'INLINE 189-193, XLINE 194-198, ',\n        38: '',\n        39: '',\n        40: 'END TEXTUAL HEADER'}\n        ```\n\n    \"\"\"\n    user = _get_userid()\n    today, time = _get_datetime()\n    text_dict = {\n        #      123456789012345678901234567890123456789012345678901234567890123456\n        1: \"segysak Python Library SEG-Y Output\",\n        2: f\"Data created by: {user} \",\n        4: f\"DATA FORMAT: SEG-Y;  DATE: {today} {time}\",\n        5: \"DATA DESCRIPTION: SEG-Y format data output from segysak using segyio\",\n        6: \"\",\n        40: \"END TEXTUAL HEADER\",\n    }\n    if override is not None:\n        for key, line in override.items():\n            text_dict[key] = line\n    return _clean_texthead(text_dict)\n</code></pre>"},{"location":"api/segy_writing.html","title":"Depreciated Writing SEG-Y","text":"<p>Warning</p> <p>These methods have been depreciated in favour of an Xarray accessor method. Examples in this documentation have been updated to use the new method or consult the accessor reference.</p>"},{"location":"api/segy_writing.html#segysak.segy.segy_writer","title":"<code>segy_writer(seisnc, segyfile, trace_header_map=None, il_chunks=None, dimension=None, silent=False, use_text=False)</code>","text":"<p>Convert siesnc format (NetCDF4) to SEGY.</p> <p>Parameters:</p> Name Type Description Default <code>seisnc</code> <code>(Dataset, string)</code> <p>The input SEISNC file either a path or the in memory xarray.Dataset</p> required <code>segyfile</code> <code>string</code> <p>The output SEG-Y file</p> required <code>trace_header_map</code> <code>dict</code> <p>Defaults to None. A dictionary of seisnc variables and byte locations. The variable will be written to the trace headers in the assigned byte location. By default CMP=23, cdp_x=181, cdp_y=185, iline=189, xline=193.</p> <code>None</code> <code>il_chunks</code> <code>int</code> <p>The size of data to work on - if you have memory limitations. Defaults to 10. This is primarily used for large 3D and ignored for 2D data.</p> <code>None</code> <code>dimension</code> <code>str</code> <p>Data dimension to output, defaults to 'twt' or 'depth' whichever is present</p> <code>None</code> <code>silent</code> <code>bool</code> <p>Turn off progress reporting. Defaults to False.</p> <code>False</code> <code>use_text</code> <code>bool</code> <p>Use the seisnc text for the EBCIDC output. This text usally comes from the loaded SEG-Y file and may not match the segysak SEG-Y output. Defaults to False and writes the default segysak EBCIDC</p> <code>False</code> Source code in <code>segysak/segy/_segy_writer.py</code> Python<pre><code>def segy_writer(\n    seisnc,\n    segyfile,\n    trace_header_map=None,\n    il_chunks=None,\n    dimension=None,\n    silent=False,\n    use_text=False,\n):\n    \"\"\"Convert siesnc format (NetCDF4) to SEGY.\n\n    Args:\n        seisnc (xarray.Dataset, string): The input SEISNC file either a path or the in memory xarray.Dataset\n        segyfile (string): The output SEG-Y file\n        trace_header_map (dict, optional): Defaults to None. A dictionary of seisnc variables\n            and byte locations. The variable will be written to the trace headers in the\n            assigned byte location. By default CMP=23, cdp_x=181, cdp_y=185, iline=189,\n            xline=193.\n        il_chunks (int, optional): The size of data to work on - if you have memory\n            limitations. Defaults to 10. This is primarily used for large 3D and ignored for 2D data.\n        dimension (str): Data dimension to output, defaults to 'twt' or 'depth' whichever is present\n        silent (bool, optional): Turn off progress reporting. Defaults to False.\n        use_text (bool, optional): Use the seisnc text for the EBCIDC output. This text usally comes from\n            the loaded SEG-Y file and may not match the segysak SEG-Y output. Defaults to False and writes\n            the default segysak EBCIDC\n    \"\"\"\n    warn(\n        \"segy_writer will be removed in v0.6, please use the accessor ds.seisio.to_segy() method instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    if trace_header_map:\n        check_tracefield(trace_header_map.values())\n\n    if isinstance(seisnc, xr.Dataset):\n        _segy_writer_input_handler(\n            seisnc, segyfile, trace_header_map, dimension, silent, None, use_text\n        )\n    else:\n        ncfile = seisnc\n        if isinstance(il_chunks, int):\n            chunking = {\"iline\": il_chunks}\n        else:\n            chunking = None\n        with open_seisnc(ncfile, chunks=chunking) as seisnc:\n            _segy_writer_input_handler(\n                seisnc,\n                segyfile,\n                trace_header_map,\n                dimension,\n                silent,\n                il_chunks,\n                use_text,\n            )\n</code></pre>"},{"location":"api/segy_writing.html#segysak.segy.segy_freewriter","title":"<code>segy_freewriter(seisnc, segyfile, data_array='data', trace_header_map=None, use_text=False, dead_trace_key=None, vert_dimension='twt', chunk_spec=None, silent=False, **dim_kwargs)</code>","text":"<p>Convert siesnc format (NetCDF4) to SEG-Y.</p> <p>Parameters:</p> Name Type Description Default <code>seisnc</code> <code>(Dataset, string)</code> <p>The input SEISNC file either a path or the in memory xarray.Dataset</p> required <code>segyfile</code> <code>string</code> <p>The output SEG-Y file</p> required <code>data_array</code> <code>string</code> <p>The Dataset variable name for the output volume.</p> <code>'data'</code> <code>trace_header_map</code> <code>dict</code> <p>Defaults to None. A dictionary of seisnc variables and byte locations. The variable will be written to the trace headers in the assigned byte location. By default CMP=23, cdp_x=181, cdp_y=185, iline=189, xline=193.</p> <code>None</code> <code>use_text</code> <code>book</code> <p>Use the seisnc text for the EBCIDC output. This text usally comes from the loaded SEG-Y file and may not match the segysak SEG-Y output. Defaults to False and writes the default segysak EBCIDC</p> <code>False</code> <code>dead_trace_key</code> <code>str</code> <p>The key for the Dataset variable to use for trace output filter.</p> <code>None</code> <code>vert_dimension</code> <code>str</code> <p>Data dimension to output, defaults to 'twt' or 'depth' whichever is present.</p> <code>'twt'</code> <code>chunk_spec</code> <code>dict</code> <p>Xarray open_dataset chunking spec.</p> <code>None</code> <code>silent</code> <code>bool</code> <p>Turn off progress reporting. Defaults to False.</p> <code>False</code> <code>dim_kwargs</code> <code>int</code> <p>The dimension/byte location pairs to output dimensions to. The number of dim_kwargs should be equal to the number of dimensions on the output data_array. The sort order will be as per the order passed to the function.</p> <code>{}</code> Source code in <code>segysak/segy/_segy_freewriter.py</code> Python<pre><code>def segy_freewriter(\n    seisnc,\n    segyfile,\n    data_array=\"data\",\n    trace_header_map=None,\n    use_text=False,\n    dead_trace_key=None,\n    vert_dimension=\"twt\",\n    chunk_spec=None,\n    silent=False,\n    **dim_kwargs,\n):\n    \"\"\"Convert siesnc format (NetCDF4) to SEG-Y.\n\n    Args:\n        seisnc (xarray.Dataset, string): The input SEISNC file either a path or the in memory xarray.Dataset\n        segyfile (string): The output SEG-Y file\n        data_array (string): The Dataset variable name for the output volume.\n        trace_header_map (dict, optional): Defaults to None. A dictionary of seisnc variables\n            and byte locations. The variable will be written to the trace headers in the\n            assigned byte location. By default CMP=23, cdp_x=181, cdp_y=185, iline=189,\n            xline=193.\n        use_text (book, optional): Use the seisnc text for the EBCIDC output. This text usally comes from\n            the loaded SEG-Y file and may not match the segysak SEG-Y output. Defaults to False and writes\n            the default segysak EBCIDC\n        dead_trace_key (str): The key for the Dataset variable to use for trace output filter.\n        vert_dimension (str): Data dimension to output, defaults to 'twt' or 'depth' whichever\n            is present.\n        chunk_spec (dict, optional): Xarray open_dataset chunking spec.\n        silent (bool, optional): Turn off progress reporting. Defaults to False.\n        dim_kwargs (int): The dimension/byte location pairs to output dimensions to. The number of dim_kwargs should be\n            equal to the number of dimensions on the output data_array. The sort order will be as per the order passed\n            to the function.\n    \"\"\"\n    warn(\n        \"segyfree_writer will be removed in v0.6, please use the accessor ds.seisio.to_segy method instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    kwargs = dict(\n        data_array=data_array,\n        trace_header_map=trace_header_map,\n        silent=silent,\n        use_text=use_text,\n        dead_trace_key=dead_trace_key,\n        vert_dimension=vert_dimension,\n    )\n\n    if isinstance(seisnc, xr.Dataset):\n        _segy_freewriter(\n            seisnc,\n            segyfile,\n            **kwargs,\n            **dim_kwargs,\n        )\n    else:\n        ncfile = seisnc\n        with open_seisnc(ncfile, chunks=chunk_spec) as seisnc:\n            _segy_freewriter(\n                seisnc,\n                segyfile,\n                **kwargs,\n                **dim_kwargs,\n            )\n</code></pre>"},{"location":"api/segy_writing.html#segysak.segy.output_byte_locs","title":"<code>output_byte_locs(name)</code>","text":"<p>Return common bytes position variable_dict for segy_writer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>One of [standard_3d, petrel_3d]</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of SEG-Y byte positions and default seisnc variable pairs.</p> Example <p>segywriter(ncfile, 'segyfile.sgy', trace_header_map=output_byte_loc('petrel')) </p> Source code in <code>segysak/segy/_segy_writer.py</code> Python<pre><code>def output_byte_locs(name):\n    \"\"\"Return common bytes position variable_dict for segy_writer.\n\n    Args:\n        name (str): One of [standard_3d, petrel_3d]\n\n    Returns:\n        dict: A dictionary of SEG-Y byte positions and default seisnc variable\n            pairs.\n\n    Example:\n        &gt;&gt;&gt; segywriter(ncfile, 'segyfile.sgy', trace_header_map=output_byte_loc('petrel'))\n        &gt;&gt;&gt;\n    \"\"\"\n    try:\n        return OUTPUT_BYTES[name]\n    except KeyError:\n        raise ValueError(\n            f\"No byte locatons for {name}, select from {list(OUTPUT_BYTES.keys())}\"\n        )\n</code></pre>"},{"location":"api/segy_xarray.html","title":"SEG-Y via Xarray","text":""},{"location":"api/segy_xarray.html#opening-seg-y-data","title":"Opening SEG-Y Data","text":"<p>SEGY Backend for reading Seismic SEGY files.</p>"},{"location":"api/segy_xarray.html#segysak.segy._xarray.SgyBackendEntrypoint","title":"<code>SgyBackendEntrypoint</code>","text":"<p>               Bases: <code>BackendEntrypoint</code></p>"},{"location":"api/segy_xarray.html#segysak.segy._xarray.SgyBackendEntrypoint.open_dataset","title":"<code>open_dataset(filename_or_obj, dim_byte_fields, drop_variables=None, head_df=None, extra_byte_fields=None, segyio_kwargs=None)</code>","text":"<p>Open a SEGY file with a native Xarray front end.</p> <p>Parameters:</p> Name Type Description Default <code>filename_or_obj</code> <code>Union[str, PathLike]</code> <p>The SEG-Y file/path.</p> required <code>dim_byte_fields</code> <code>Dict[str, int]</code> <p>Dimension names and byte location pairs. This should at a minimum have the keys <code>cdp</code> or <code>iline</code> and <code>xline</code>.</p> required <code>drop_variables</code> <code>Union[tuple[str], None]</code> <p>Ignored</p> <code>None</code> <code>head_df</code> <code>Union[DataFrame, None]</code> <p>The DataFrame output from <code>segy_header_scrape</code>. This DataFrame can be filtered by the user to load select trace sets. Trace loading is based upon the DataFrame index.</p> <code>None</code> <code>extra_byte_fields</code> <code>dict</code> <p>Additional header information to load into the Dataset. Defaults to None.</p> <code>None</code> <code>segyio_kwargs</code> <code>Union[dict, None]</code> <p>Extra keyword arguments for segyio.open</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Lazy segy file Dataset</p> <p>Example</p> <p>This backend is registered within Xarray and is accessed automatically when using the <code>xr.open_dataset</code> method for files with extensions of <code>.segy</code> and <code>.sgy</code>.</p> <p>Ensure the  byte locations are correct for your data.</p> Python<pre><code># 3D data\nds3d = xr.open_dataset(\n    segyfile_path,\n    dim_byte_fields={'iline':189, 'xline':193},\n    extra_byte_fields={'cdp_x':181, 'cdp_y':185},\n)\n\n# 2D data\nds2d = xr.open_dataset(\n    segyfile_path,\n    dim_byte_fields={'cdp':22},\n    extra_byte_fields={'cdp_x':181, 'cdp_y':185},\n)\n</code></pre>"},{"location":"api/segy_xarray.html#saving-seg-y-data","title":"Saving SEG-Y Data","text":""},{"location":"api/segy_xarray.html#segysak.segy._xarray_writer.SegyWriter","title":"<code>SegyWriter</code>","text":"<p>Handles the writing of SEG-Y files from compatible Datasets. Usually this class would not be used and SEGY-SAK users can write SEG-Y file via the <code>seisio</code> accessor.</p> <p>Example</p> <p>Writing data with the <code>seisio</code> accessor.</p> Python<pre><code>ds.seisio.to_segy()\n</code></pre>"},{"location":"api/segy_xarray.html#segysak.segy._xarray_writer.SegyWriter.to_segy","title":"<code>to_segy(ds, data_var='data', vert_dimension='samples', dead_trace_var=None, trace_header_map=None, **dim_kwargs)</code>","text":"<p>Output xarray dataset to SEG-Y format.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset</p> required <code>data_var</code> <code>str</code> <p>The Dataset variable name for the output volume.</p> <code>'data'</code> <code>vert_dimension</code> <code>str</code> <p>Data dimension to output as vertical trace, defaults to 'samples'.</p> <code>'samples'</code> <code>dead_trace_var</code> <code>Union[str, None]</code> <p>A variable in the Dataset which signifies dead traces.</p> <code>None</code> <code>trace_header_map</code> <code>Dict[str, int]</code> <p>Defaults to None. A dictionary of Dataset variables and byte locations. The variable will be written to the trace headers in the assigned byte location. By default CMP=23, cdp_x=181, cdp_y=185, iline=189, xline=193.</p> <code>None</code> <code>dim_kwargs</code> <p>The dimension/byte location pairs to output dimensions to. The number of dim_kwargs should be equal to the number of dimensions on the output data_array. The sort order will be as per the order passed to the function.</p> <code>{}</code>"},{"location":"api/segy_xarray.html#segysak.segy._xarray_writer.SegyWriter.write_text_header","title":"<code>write_text_header(text, line_numbers=True)</code>","text":"<p>Write the text header for the file. The line length limit is 80 characters.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to write, new lines have newline character <code>\\n</code>. Long lines will be truncated.</p> required <code>line_numbers</code> <code>bool</code> <p>Add line numbers to the output text header.</p> <code>True</code>"},{"location":"api/utils.html","title":"Package Utils","text":"<p>Shared progress bar class for SEGY-SAK</p>"},{"location":"api/utils.html#segysak.progress.Progress","title":"<code>Progress</code>","text":"<p>Progress class tracks the shared state of all progress trackers in SEGY-SAK</p> Source code in <code>segysak/progress.py</code> Python<pre><code>class Progress:\n    \"\"\"Progress class tracks the shared state of all progress trackers in SEGY-SAK\"\"\"\n\n    _segysak_tqdm_func = tqdm_auto\n    _segysak_tqdm_kwargs = {\n        \"disable\": False,\n        \"leave\": False,\n        \"unit_scale\": True,\n    }\n\n    @classmethod\n    def silent(cls):\n        return cls._segysak_tqdm_kwargs[\"disable\"]\n\n    def __init__(self, tqdm_func: Union[str, None] = None, **tqdm_kwargs):\n        if tqdm_func == \"a\":\n            self.tqdm_func = tqdm_auto\n        elif tqdm_func == \"t\":\n            self.tqdm_func = tqdm\n        elif tqdm_func == \"n\":\n            self.tqdm_func = tqdm_notebook\n        else:\n            self.tqdm_func = self._segysak_tqdm_func\n\n        self.tqdm_kwargs = tqdm_kwargs\n\n    def __enter__(self):\n        kwargs = self._segysak_tqdm_kwargs.copy()\n        kwargs.update(self.tqdm_kwargs)\n        self.pbar = self.tqdm_func(**kwargs)\n        return self.pbar\n\n    def __exit__(self, type: Any, value: Any, traceback: Any):\n        self.pbar.clear()\n        self.pbar.close()\n\n    @classmethod\n    def set_defaults(cls, **tqdm_kwargs: Dict[str, Any]) -&gt; None:\n        \"\"\"Set the default arguments for tqdm progress reporting in SEGY-SAK.\n\n        The defaults are Global and affect all tqdm reporting in the active Python\n        session.\n\n        Args:\n            tqdm_kwargs: Any valid [`tqdm.tqdm` argument](https://tqdm.github.io/docs/tqdm/).\n        \"\"\"\n        valid_args = inspect.signature(tqdm.__init__).parameters\n        for kwarg in tqdm_kwargs:\n            try:\n                assert kwarg in valid_args\n            except AssertionError:\n                raise AssertionError(\n                    f\"{kwarg} is not a valid tqdm argument -&gt; {valid_args.keys()}\"\n                )\n        cls._segysak_tqdm_kwargs.update(tqdm_kwargs)\n</code></pre>"},{"location":"api/utils.html#segysak.progress.Progress.set_defaults","title":"<code>set_defaults(**tqdm_kwargs)</code>  <code>classmethod</code>","text":"<p>Set the default arguments for tqdm progress reporting in SEGY-SAK.</p> <p>The defaults are Global and affect all tqdm reporting in the active Python session.</p> <p>Parameters:</p> Name Type Description Default <code>tqdm_kwargs</code> <code>Dict[str, Any]</code> <p>Any valid <code>tqdm.tqdm</code> argument.</p> <code>{}</code> Source code in <code>segysak/progress.py</code> Python<pre><code>@classmethod\ndef set_defaults(cls, **tqdm_kwargs: Dict[str, Any]) -&gt; None:\n    \"\"\"Set the default arguments for tqdm progress reporting in SEGY-SAK.\n\n    The defaults are Global and affect all tqdm reporting in the active Python\n    session.\n\n    Args:\n        tqdm_kwargs: Any valid [`tqdm.tqdm` argument](https://tqdm.github.io/docs/tqdm/).\n    \"\"\"\n    valid_args = inspect.signature(tqdm.__init__).parameters\n    for kwarg in tqdm_kwargs:\n        try:\n            assert kwarg in valid_args\n        except AssertionError:\n            raise AssertionError(\n                f\"{kwarg} is not a valid tqdm argument -&gt; {valid_args.keys()}\"\n            )\n    cls._segysak_tqdm_kwargs.update(tqdm_kwargs)\n</code></pre>"},{"location":"api/xarray_accessor.html","title":"Depreciated Xarray Accessor modules","text":"<p>Warning</p> <p>These methods have been depreciated in favour of a new <code>segysak</code> Xarray accessor. Examples in this documentation have been updated to use the new accessor or consult the reference.</p> <p>Accessor modules are accessed as namespaces within the Xarray.Dataset objects created by SEGY-SAK. When <code>segysak</code> is imported, all <code>xarray.Dataset</code> objects will contain the <code>.seis</code> and <code>.seisio</code> namespaces.</p>"},{"location":"api/xarray_accessor.html#segysak-xarray-accessor-modules","title":"segysak Xarray accessor modules","text":""},{"location":"api/xarray_accessor.html#segysak._accessor.SeisIO","title":"<code>SeisIO</code>","text":""},{"location":"api/xarray_accessor.html#segysak._accessor.SeisIO.to_netcdf","title":"<code>to_netcdf(file_path, **kwargs)</code>","text":"<p>Output to netcdf4 with specs for seisnc.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, PathLike]</code> <p>The output file path.</p> required <code>**kwargs</code> <p>As per xarray function to_netcdf.</p> <code>{}</code>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisIO.to_segy","title":"<code>to_segy(segy_file, use_text=True, coord_scalar=None, data_var='data', vert_dimension='samples', write_dead_traces=False, dead_trace_var=None, trace_header_map=None, **dim_kwargs)</code>","text":"<p>Output Xarray Dataset to SEG-Y format.</p> <p>Parameters:</p> Name Type Description Default <code>segy_file</code> <code>Union[str, PathLike]</code> <p>The output file to write to.</p> required <code>use_text</code> <code>bool</code> <p>Write the <code>text</code> attribute from the dataset to the text header.</p> <code>True</code> <code>coord_scalar</code> <code>Union[float, None]</code> <p>A SEG-Y compatible coordinate scalar.</p> <code>None</code> <code>data_var</code> <code>str</code> <p>The variable name of the trace data in the Dataset.</p> <code>'data'</code> <code>vert_dimension</code> <code>str</code> <p>The vertical (samples) dimension of the data_var.</p> <code>'samples'</code> <code>write_dead_traces</code> <code>bool</code> <p>Write dead traces as zeros to the SEG-Y file.</p> <code>False</code> <code>dead_trace_var</code> <code>Union[str, None]</code> <p>A dataset variable containing boolean values that identifies dead traces on the non-vertical dimension.</p> <code>None</code> <code>trace_header_map</code> <code>Dict[str, int]</code> <p>Defaults to None. A dictionary of Dataset variables and byte locations. The variable will be written to the trace headers in the assigned byte location. By default CMP=23, cdp_x=181, cdp_y=185, iline=189, xline=193.</p> <code>None</code> <code>dim_kwargs</code> <code>int</code> <p>The dimension/byte location pairs to output dimensions to. The number of dim_kwargs should be equal to the number of dimensions on the output data_array. The trace sort order will be as per the order passed to the function.</p> <code>{}</code> <p>Example</p> Python<pre><code>ds3d.seisio.to_segy(\n    \"output_file.segy\",\n    use_text = True,\n    vert_dimension = \"samples\",\n    trace_header_map = {'cdp_x':181, 'cdp_y':185}\n    iline = 189,\n    xline = 193,\n)\n</code></pre>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisIO.to_subsurface","title":"<code>to_subsurface()</code>","text":"<p>Convert seismic data to a subsurface StructuredData Object</p> <p>Raises:</p> Type Description <code>err</code> <p>[description]</p> <code>NotImplementedError</code> <p>[description]</p> <p>Returns:</p> Type Description <p>subsurface.structs.base_structures.StructuredData: subsurface struct</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom","title":"<code>SeisGeom</code>","text":"<p>               Bases: <code>TemplateAccessor</code></p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.calc_corner_points","title":"<code>calc_corner_points()</code>","text":"<p>Calculate the corner points of the geometry or end points of a 2D line.</p> <p>This puts two properties in the seisnc attrs with the calculated il/xl and cdp_x and cdp_y if available.</p> Attr <p>ds.attrs['corner_points'] ds.attrs['corner_points_xy']</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.fill_cdpna","title":"<code>fill_cdpna()</code>","text":"<p>Fills NaN cdp locations by fitting known cdp x and y values to the local grid using a planar surface relationshipt.</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.get_affine_transform","title":"<code>get_affine_transform()</code>","text":"<p>Calculate the forward iline/xline -&gt; cdp_x, cdp_y Affine transform for Matplotlib using corner point geometry.</p> <p>Returns:</p> Type Description <p>matplotlib.transforms.Affine2D:</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Dataset is not 3D</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.get_dead_trace_map","title":"<code>get_dead_trace_map(scan=None, zeros_as_nan=False)</code>","text":"<p>Scan the vertical axis of a volume to find traces that are all NaN and return an DataArray which maps the all dead traces.</p> <p>Faster scans can be performed by setting scan to an int or list of int representing horizontal slice indexes to use for the scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan</code> <code>int/list of int</code> <p>Horizontal indexes to scan. Defaults to None (scan full volume).</p> <code>None</code> <code>zeros_as_nan</code> <code>bool</code> <p>Treat zeros as NaN during scan.</p> <code>False</code> <p>Returns:</p> Type Description <p>xarray.DataArray: boolean dead trace map.</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.get_measurement_system","title":"<code>get_measurement_system()</code>","text":"<p>Return measurement_system if present, else None</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.grid_rotation","title":"<code>grid_rotation()</code>","text":"<p>Calculate the rotation of the grid using the sum under the curve method.</p> <p>(x2 \u2212 x1)(y2 + y1)</p> <p>Returns a value &gt;0 if the rotation of points along inline is clockwise.</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.interp_line","title":"<code>interp_line(cdp_x, cdp_y, extra=None, bin_spacing_hint=10, line_method='slinear', xysel_method='linear')</code>","text":"<p>Select data at x and y coordinates</p> <p>Parameters:</p> Name Type Description Default <code>bin_spacing_hint</code> <code>number</code> <p>a bin spacing to stay close to, in cdp world units. Default: 10</p> <code>10</code> <code>line_method</code> <code>string</code> <p>valid values for the kind argument in scipy.interpolate.interp1d</p> <code>'slinear'</code> <code>xysel_method</code> <code>string</code> <p>valid values for DataArray.interp</p> <code>'linear'</code> <p>Returns:</p> Type Description <p>xarray.Dataset: Interpolated traces along the arbitrary line</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.is_2d","title":"<code>is_2d()</code>","text":"<p>Returns True if the dataset is 2D peformant else False</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.is_2dgath","title":"<code>is_2dgath()</code>","text":"<p>Returns True if the dataset is 2D peformant and has offset or angle else False</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.is_3d","title":"<code>is_3d()</code>","text":"<p>Returns True if the dataset is 3D peformant else False</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.is_3dgath","title":"<code>is_3dgath()</code>","text":"<p>Returns True if the dataset is 3D peformant and has offset or angle else False</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.is_depth","title":"<code>is_depth()</code>","text":"<p>Check if seisnc volume is in depth</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.is_empty","title":"<code>is_empty()</code>","text":"<p>Check if empty</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.is_twt","title":"<code>is_twt()</code>","text":"<p>Check if seisnc volume is in twt</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.plot_bounds","title":"<code>plot_bounds(ax=None)</code>","text":"<p>Plot survey bbounding box to a new or existing axis</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <p>(optional) axis to plot to</p> <code>None</code> <p>Returns:</p> Type Description <p>matplotlib axis used</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.subsample_dims","title":"<code>subsample_dims(**dim_kwargs)</code>","text":"<p>Return a dictionary of subsampled dims suitable for xarray.interp.</p> <p>This tool halves</p> <p>Parameters:</p> Name Type Description Default <code>dim_kwargs</code> <p>dimension names as keyword arguments with values of how many times we should divide the dimension by 2.</p> <code>{}</code>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.surface_from_points","title":"<code>surface_from_points(points, attr, left=('cdp_x', 'cdp_y'), right=None, key=None, method='linear')</code>","text":"<p>Sample a 2D point set with an attribute (like Z) to the seisnc geometry using interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>array - like</code> <p>Nx2 array-like of points with coordinates corresponding to coord1 and coord2.</p> required <code>attr</code> <code>(array - like, str)</code> <p>If str points should be a DataFrame where attr is the column name of the attribute to be interpolated to the seisnc geometry. Else attr is a 1D array of length N.</p> required <code>left</code> <code>tuple</code> <p>Length 2 tuple of coordinate dimensions to interpolate to.</p> <code>('cdp_x', 'cdp_y')</code> <code>right</code> <code>tuple</code> <p>If points is DataFrame right is a length 2 tuple of column keys corresponding to coordinates in argument left.</p> <code>None</code> <code>method</code> <code>str</code> <p>The interpolation method to use for griddata from scipy. Defaults to 'linear'</p> <code>'linear'</code> <p>Returns:</p> Type Description <p>xr.Dataset: Surface with geometry specified in left.</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.xysel","title":"<code>xysel(cdp_x, cdp_y, method='nearest', sample_dim_name='cdp')</code>","text":"<p>Select data at x and y coordinates</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Same as methods for xarray.Dataset.interp</p> <code>'nearest'</code> <code>sample_dim_name</code> <code>str</code> <p>The name to give the output sampling dimension.</p> <code>'cdp'</code> <p>Returns:</p> Type Description <p>xarray.Dataset: At selected coordinates.</p>"},{"location":"api/xarray_accessor.html#segysak._accessor.SeisGeom.zeros_like","title":"<code>zeros_like()</code>","text":"<p>Create a new dataset with the same attributes and coordinates and dimensions but with data filled by zeros.</p>"},{"location":"api/xarray_new.html","title":"Xarray Accessor modules","text":"<p>Accessor modules are accessed as namespaces within the Xarray.Dataset objects created by SEGY-SAK. When <code>segysak</code> is imported, all <code>xarray.Dataset</code> objects will contain the <code>.segysak</code> and <code>.seisio</code> namespaces.</p>"},{"location":"api/xarray_new.html#segysak._accessor.SeisIO","title":"<code>SeisIO</code>","text":""},{"location":"api/xarray_new.html#segysak._accessor.SeisIO.to_segy","title":"<code>to_segy(segy_file, use_text=True, coord_scalar=None, data_var='data', vert_dimension='samples', write_dead_traces=False, dead_trace_var=None, trace_header_map=None, **dim_kwargs)</code>","text":"<p>Output Xarray Dataset to SEG-Y format.</p> <p>Parameters:</p> Name Type Description Default <code>segy_file</code> <code>Union[str, PathLike]</code> <p>The output file to write to.</p> required <code>use_text</code> <code>bool</code> <p>Write the <code>text</code> attribute from the dataset to the text header.</p> <code>True</code> <code>coord_scalar</code> <code>Union[float, None]</code> <p>A SEG-Y compatible coordinate scalar.</p> <code>None</code> <code>data_var</code> <code>str</code> <p>The variable name of the trace data in the Dataset.</p> <code>'data'</code> <code>vert_dimension</code> <code>str</code> <p>The vertical (samples) dimension of the data_var.</p> <code>'samples'</code> <code>write_dead_traces</code> <code>bool</code> <p>Write dead traces as zeros to the SEG-Y file.</p> <code>False</code> <code>dead_trace_var</code> <code>Union[str, None]</code> <p>A dataset variable containing boolean values that identifies dead traces on the non-vertical dimension.</p> <code>None</code> <code>trace_header_map</code> <code>Dict[str, int]</code> <p>Defaults to None. A dictionary of Dataset variables and byte locations. The variable will be written to the trace headers in the assigned byte location. By default CMP=23, cdp_x=181, cdp_y=185, iline=189, xline=193.</p> <code>None</code> <code>dim_kwargs</code> <code>int</code> <p>The dimension/byte location pairs to output dimensions to. The number of dim_kwargs should be equal to the number of dimensions on the output data_array. The trace sort order will be as per the order passed to the function.</p> <code>{}</code> <p>Example</p> Python<pre><code>ds3d.seisio.to_segy(\n    \"output_file.segy\",\n    use_text = True,\n    vert_dimension = \"samples\",\n    trace_header_map = {'cdp_x':181, 'cdp_y':185}\n    iline = 189,\n    xline = 193,\n)\n</code></pre>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDataArrayAccessor","title":"<code>SegysakDataArrayAccessor</code>","text":"<p>               Bases: <code>TemplateAccessor</code></p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDataArrayAccessor.attrs","title":"<code>attrs: Dict[str, Any]</code>  <code>property</code>","text":"<p>Return the seisnc attributes for this Xarray object</p> <p>Note</p> <p>The <code>ds.segysak.attrs</code> object is stored as a JSON string in ds.attrs['seisnc']. Doing this allows for normal methods to export the seisnc values. Accessing the <code>ds.segysak.attrs</code> automatically deserialises the string into a Python dict.</p> <p>Example</p> Python<pre><code>&gt;&gt;&gt; ds.segysak.attrs\n{'coord_scalar':-100}\n</code></pre>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDataArrayAccessor.humanbytes","title":"<code>humanbytes: str</code>  <code>property</code>","text":"<p>Prints Human Friendly size of Dataset to return the bytes as an int use <code>xarray.Dataset.nbytes</code></p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Human readable size of dataset.</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDataArrayAccessor.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Return an attribute key from ds.segysak.attrs</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDataArrayAccessor.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set an attribute key to value in ds.segysak.attrs</p> <p>Note</p> <p>The value must be a JSON compatible object such as an int, string or list.</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDataArrayAccessor.get","title":"<code>get(key, default=None)</code>","text":"<p>Returns the value of key else default from ds.segysak.attrs similar to <code>dict.get</code></p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDataArrayAccessor.get_dimensions","title":"<code>get_dimensions()</code>","text":"<p>Returns <code>ds.segysak.attrs['dimensions']</code> if available. Else use <code>ds.segysak.infer_dimensions()</code>.</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDataArrayAccessor.infer_dimensions","title":"<code>infer_dimensions(ignore=None)</code>","text":"<p>Infer the dimensions from those available in the dataset/array. They should match good seisnc dimension names from CoordKeyField</p> <p>Parameters:</p> Name Type Description Default <code>ignore</code> <code>List[str]</code> <p>A list of dimensions to ignore when considering mappings.</p> <code>None</code>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDataArrayAccessor.set_dimensions","title":"<code>set_dimensions(seisnc_dims=None, seisnc_vert_dim=None)</code>","text":"<p>Set the dimensions of the DataArray/Dataset. This is required to ensure that other functions in this module can interpret the DataArray correctly.</p> <p>Parameters:</p> Name Type Description Default <code>seisnc_dims</code> <code>Dict[str, str]</code> <p>Pairs from segysak.CoordKeyField and dimension vars.</p> <code>None</code> <code>seisnc_vert_dim</code> <code>Union[str, None]</code> <p>The variable name for the vertical dimension.</p> <code>None</code>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDataArrayAccessor.set_vertical_domain","title":"<code>set_vertical_domain(vert_domain)</code>","text":"<p>Set the vertical domain of the DataArray, usually twt or depth.</p> <p>Parameters:</p> Name Type Description Default <code>vert_domain</code> <code>_VerticalKeyField</code> <p>The vertical domain key to use (i.e. twt or depth)</p> required"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDataArrayAccessor.store_attributes","title":"<code>store_attributes(**kwargs)</code>","text":"<p>Store attributes in for seisnc. NetCDF attributes are a little limited, so we expand the capabilities by serializing and deserializing from JSON text.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Dict[str, JSON]</code> <p>name and JSON compatible values to store in <code>ds.segysak.attrs</code></p> <code>{}</code>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor","title":"<code>SegysakDatasetAccessor</code>","text":"<p>               Bases: <code>TemplateAccessor</code></p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.attrs","title":"<code>attrs: Dict[str, Any]</code>  <code>property</code>","text":"<p>Return the seisnc attributes for this Xarray object</p> <p>Note</p> <p>The <code>ds.segysak.attrs</code> object is stored as a JSON string in ds.attrs['seisnc']. Doing this allows for normal methods to export the seisnc values. Accessing the <code>ds.segysak.attrs</code> automatically deserialises the string into a Python dict.</p> <p>Example</p> Python<pre><code>&gt;&gt;&gt; ds.segysak.attrs\n{'coord_scalar':-100}\n</code></pre>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.humanbytes","title":"<code>humanbytes: str</code>  <code>property</code>","text":"<p>Prints Human Friendly size of Dataset to return the bytes as an int use <code>xarray.Dataset.nbytes</code></p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Human readable size of dataset.</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Return an attribute key from ds.segysak.attrs</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set an attribute key to value in ds.segysak.attrs</p> <p>Note</p> <p>The value must be a JSON compatible object such as an int, string or list.</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.calc_corner_points","title":"<code>calc_corner_points()</code>","text":"<p>Calculate the corner points of the 3d geometry or end points of a 2D line.</p> <p>Returns:</p> Name Type Description <code>corner_points</code> <code>List[Tuple[str, str]]</code> <p>A list of cdp_x, cdp_y pairs for the corner points of the dataset.</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.coordinate_df","title":"<code>coordinate_df(linear_fillna=True, three_d_only=False)</code>","text":"<p>Return the coordinates of a Dataset as a DataFrame. Optionally do-not fill the missing coordinates.</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.fill_cdpna","title":"<code>fill_cdpna(method='linear')</code>","text":"<p>Fills NaN cdp_x and cdp_y locations, usually caused by dead traces.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>One of 'linear', 'affine'. Linear uses a planar linear transform, whilst affine uses the derived affine transformation.</p> <code>'linear'</code>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.get","title":"<code>get(key, default=None)</code>","text":"<p>Returns the value of key else default from ds.segysak.attrs similar to <code>dict.get</code></p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.get_affine_transform","title":"<code>get_affine_transform(force_recalc=False)</code>","text":"<p>Returns a matplotlib Affine forward transform dims -&gt; (cdp_x, cdp_y)</p> <p>The matrix is only calculated once and then stored in the Dataset attributes as the matrix coefficients. To recalculate the matrix set <code>force_recalc=True</code>.</p> <p>The reverse transform (cdp_x, cdp_y) is provided by <code>get_affine_transform().inverted()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>force_recalc</code> <code>bool</code> <p>Force recalculation of affine transform matrix.</p> <code>False</code> <p>Returns:</p> Type Description <code>Affine2D</code> <p>The forward affine transform.</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.get_coords","title":"<code>get_coords()</code>","text":"<p>Returns attrs['cdp_x'], attrs['cdp_y'] if exist else user infer_coords()</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.get_dimensions","title":"<code>get_dimensions()</code>","text":"<p>Returns <code>ds.segysak.attrs['dimensions']</code> if available. Else use <code>ds.segysak.infer_dimensions()</code>.</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.infer_coords","title":"<code>infer_coords(ignore=None)</code>","text":""},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.infer_dimensions","title":"<code>infer_dimensions(ignore=None)</code>","text":"<p>Infer the dimensions from those available in the dataset/array. They should match good seisnc dimension names from CoordKeyField</p> <p>Parameters:</p> Name Type Description Default <code>ignore</code> <code>List[str]</code> <p>A list of dimensions to ignore when considering mappings.</p> <code>None</code>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.interp_line","title":"<code>interp_line(points, bin_spacing_hint=10.0, line_method='slinear', xysel_method='linear')</code>","text":"<p>Select data at regular intervals along a set of path segments in X, Y defined by points.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>array</code> required <code>bin_spacing_hint</code> <code>float</code> <p>A bin spacing to stay close to, in cdp world units. Default: 10</p> <code>10.0</code> <code>line_method</code> <p>Valid values for the kind argument in scipy.interpolate.interp1d</p> <code>'slinear'</code> <code>xysel_method</code> <p>Valid values for DataArray.interp for the data interpolation to the path.</p> <code>'linear'</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Interpolated dataset on points: Interpolated traces along the arbitrary line</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.plot_bounds","title":"<code>plot_bounds(ax=None)</code>","text":"<p>Plot survey bounding box to a new or existing matplotlib.Axis.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axis</code> <p>Axis to plot onto.</p> <code>None</code> <p>Returns:</p> Type Description <code>Axis</code> <p>matplotlib Axis</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.scale_coords","title":"<code>scale_coords(coord_scalar=None)</code>","text":"<p>Scale the coordinates using a SEG-Y coord_scalar</p> <p>The coordinate multiplier is given by:</p> <pre><code>coord_scalar_mult = np.power(abs(coord_scalar), np.sign(coord_scalar))\n</code></pre> <p>Or | scalar | multiplier | | ------ | ---------- | | 1000   | 1000       | | 100    | 100        | | 10     | 10         | | 1      | 1          | | 0      | 1          | | -1     | 1          | | -10    | 0.1        | | -100   | 0.01       | | -1000  | 0.001      |</p>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.set_coords","title":"<code>set_coords(seisnc_coords)</code>","text":"<p>Set the seisnc coordinate mapping. This maps coordinate variable names to known seisnc coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>seisnc_coords</code> <code>Dict[Union[_CoordKeyField, str], str]</code> <p>A mapping of coordinates to known seisnc standard Keyfields.</p> required"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.set_dimensions","title":"<code>set_dimensions(seisnc_dims=None, seisnc_vert_dim=None)</code>","text":"<p>Set the dimensions of the DataArray/Dataset. This is required to ensure that other functions in this module can interpret the DataArray correctly.</p> <p>Parameters:</p> Name Type Description Default <code>seisnc_dims</code> <code>Dict[str, str]</code> <p>Pairs from segysak.CoordKeyField and dimension vars.</p> <code>None</code> <code>seisnc_vert_dim</code> <code>Union[str, None]</code> <p>The variable name for the vertical dimension.</p> <code>None</code>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.store_attributes","title":"<code>store_attributes(**kwargs)</code>","text":"<p>Store attributes in for seisnc. NetCDF attributes are a little limited, so we expand the capabilities by serializing and deserializing from JSON text.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Dict[str, JSON]</code> <p>name and JSON compatible values to store in <code>ds.segysak.attrs</code></p> <code>{}</code>"},{"location":"api/xarray_new.html#segysak._accessor.SegysakDatasetAccessor.xysel","title":"<code>xysel(points, method='nearest', sample_dim_name='cdp')</code>","text":"<p>Perform selection on the dataset based upon <code>cdp_x</code> and <code>cdp_y</code> coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>array</code> <p>[M, 2] array of cdp_x and cdp_y points to select.</p> required <code>method</code> <code>str</code> <p>Sampling interpolation method, as per xr.Dataset.interp().</p> <code>'nearest'</code> <code>sample_dim_name</code> <code>str</code> <p>The output dimension name.</p> <code>'cdp'</code> <p>Returns:</p>"},{"location":"api/xarray_new.html#creating-blank-datasets","title":"Creating blank datasets","text":"<p>These methods help to create datasets with appropriate coordinates for seismic data.</p>"},{"location":"api/xarray_new.html#segysak.create3d_dataset","title":"<code>create3d_dataset(dims, first_sample=0, sample_rate=1, first_iline=1, iline_step=1, first_xline=1, xline_step=1, first_offset=None, offset_step=None, vert_domain='TWT', vert_units=None)</code>","text":"<p>Create a regular 3D seismic dataset from basic grid geometry with optional offset dimension for pre-stack data.</p> <p>Parameters:</p> Name Type Description Default <code>dims</code> <code>tuple of int</code> <p>The dimensions of the dataset to create (iline, xline, vertical). If first_offset is specified then (iline, xline, vertical, offset)</p> required <code>first_sample</code> <code>int</code> <p>The first vertical sample. Defaults to 0.</p> <code>0</code> <code>sample_rate</code> <code>int</code> <p>The vertical sample rate. Defaults to 1.</p> <code>1</code> <code>first_iline</code> <code>int</code> <p>First inline number. Defaults to 1.</p> <code>1</code> <code>iline_step</code> <code>int</code> <p>Inline increment. Defaults to 1.</p> <code>1</code> <code>first_xline</code> <code>int</code> <p>First crossline number. Defaults to 1.</p> <code>1</code> <code>xline_step</code> <code>int</code> <p>Crossline increment. Defaults to 1.</p> <code>1</code> <code>first_offset</code> <code>int / float</code> <p>If not none, the offset dimension will be added starting at first offset. Defaults to None.</p> <code>None</code> <code>offset_step</code> <code>(int, float)</code> <p>Required if first_offset is specified. The offset increment.</p> <code>None</code> <code>vert_domain</code> <code>str</code> <p>Vertical domain, one of ('DEPTH', 'TWT'). Defaults to 'TWT'.</p> <code>'TWT'</code> <code>vert_units(str,</code> <code>optional</code> <p>Measurement system of of vertical coordinates. One of ('ms', 's', 'm', 'km', 'ft'): Defaults to None for unknown.</p> required Source code in <code>segysak/_seismic_dataset.py</code> Python<pre><code>def create3d_dataset(\n    dims,\n    first_sample=0,\n    sample_rate=1,\n    first_iline=1,\n    iline_step=1,\n    first_xline=1,\n    xline_step=1,\n    first_offset=None,\n    offset_step=None,\n    vert_domain=\"TWT\",\n    vert_units=None,\n):\n    \"\"\"Create a regular 3D seismic dataset from basic grid geometry with optional\n    offset dimension for pre-stack data.\n\n    Args:\n        dims (tuple of int): The dimensions of the dataset to create (iline, xline, vertical).\n            If first_offset is specified then (iline, xline, vertical, offset)\n        first_sample (int, optional): The first vertical sample. Defaults to 0.\n        sample_rate (int, optional): The vertical sample rate. Defaults to 1.\n        first_iline (int, optional): First inline number. Defaults to 1.\n        iline_step (int, optional): Inline increment. Defaults to 1.\n        first_xline (int, optional): First crossline number. Defaults to 1.\n        xline_step (int, optional): Crossline increment. Defaults to 1.\n        first_offset (int/float, optional): If not none, the offset dimension will be added starting\n            at first offset. Defaults to None.\n        offset_step (int, float, optional): Required if first_offset is specified. The offset increment.\n        vert_domain (str, optional): Vertical domain, one of ('DEPTH', 'TWT'). Defaults to 'TWT'.\n        vert_units(str, optional): Measurement system of of vertical coordinates.\n            One of ('ms', 's', 'm', 'km', 'ft'): Defaults to None for unknown.\n    \"\"\"\n    vert_domain = vert_domain.upper()\n\n    if first_offset is None:\n        ni, nx, ns = dims\n    else:\n        ni, nx, ns, no = dims\n\n    units = _check_vert_units(vert_units)\n\n    # 1e-10 to stabilise range on rounding errors for weird floats from hypothesis\n\n    vert = np.arange(\n        first_sample,\n        first_sample + sample_rate * ns - 1e-10,\n        sample_rate,\n        dtype=int,\n    )\n    ilines = np.arange(\n        first_iline,\n        first_iline + iline_step * ni - 1e-10,\n        iline_step,\n        dtype=int,\n    )\n    xlines = np.arange(\n        first_xline,\n        first_xline + xline_step * nx - 1e-10,\n        xline_step,\n        dtype=int,\n    )\n\n    if first_offset is None:\n        offset = None\n    else:\n        offset = np.arange(\n            first_offset,\n            first_offset + offset_step * no - 1e-10,\n            offset_step,\n            dtype=float,\n        )\n\n    builder, domain = _dataset_coordinate_helper(\n        vert, vert_domain, iline=ilines, xline=xlines, offset=offset\n    )\n\n    ds = create_seismic_dataset(**builder)\n    ds.attrs[AttrKeyField.measurement_system] = units\n    ds.attrs[AttrKeyField.d3_domain] = domain\n    ds.attrs[AttrKeyField.sample_rate] = sample_rate\n    ds.attrs[AttrKeyField.text] = \"SEGY-SAK Create 3D Dataset\"\n    ds.attrs[AttrKeyField.corner_points] = [\n        (ilines[0], xlines[0]),\n        (ilines[-1], xlines[0]),\n        (ilines[-1], xlines[-1]),\n        (ilines[0], xlines[-1]),\n        (ilines[0], xlines[0]),\n    ]\n\n    return ds\n</code></pre>"},{"location":"api/xarray_new.html#segysak.create2d_dataset","title":"<code>create2d_dataset(dims, first_sample=0, sample_rate=1, first_cdp=1, cdp_step=1, first_offset=None, offset_step=None, vert_domain='TWT', vert_units=None)</code>","text":"<p>Create a regular 2D seismic dataset from basic geometry.</p> <p>Parameters:</p> Name Type Description Default <code>dims</code> <code>tuple of int</code> <p>The dimensions of the dataset to create (ncdp, vertical). If first_offset is specified then (ncdp, vertical, offset)</p> required <code>first_sample</code> <code>int</code> <p>The first vertical sample. Defaults to 0.</p> <code>0</code> <code>sample_rate</code> <code>int</code> <p>The vertical sample rate. Defaults to 1.</p> <code>1</code> <code>first_cdp</code> <code>int</code> <p>First CDP number. Defaults to 1.</p> <code>1</code> <code>cdp_step</code> <code>int</code> <p>CDP increment. Defaults to 1.</p> <code>1</code> <code>first_offset</code> <code>int / float</code> <p>If not none, the offset dimension will be added starting at first offset. Defaults to None.</p> <code>None</code> <code>offset_step</code> <code>(int, float)</code> <p>Required if first_offset is specified. The offset increment.</p> <code>None</code> <code>vert_domain</code> <code>str</code> <p>Vertical domain, one of ('DEPTH', 'TWT'). Defaults to 'TWT'.</p> <code>'TWT'</code> <code>vert_units(str,</code> <code>optional</code> <p>Measurement system of of vertical coordinates. One of ('ms', 's', 'm', 'km', 'ft'): Defaults to None for unknown.</p> required Source code in <code>segysak/_seismic_dataset.py</code> Python<pre><code>def create2d_dataset(\n    dims,\n    first_sample=0,\n    sample_rate=1,\n    first_cdp=1,\n    cdp_step=1,\n    first_offset=None,\n    offset_step=None,\n    vert_domain=\"TWT\",\n    vert_units=None,\n):\n    \"\"\"Create a regular 2D seismic dataset from basic geometry.\n\n    Args:\n        dims (tuple of int): The dimensions of the dataset to create (ncdp, vertical).\n            If first_offset is specified then (ncdp, vertical, offset)\n        first_sample (int, optional): The first vertical sample. Defaults to 0.\n        sample_rate (int, optional): The vertical sample rate. Defaults to 1.\n        first_cdp (int, optional): First CDP number. Defaults to 1.\n        cdp_step (int, optional): CDP increment. Defaults to 1.\n        first_offset (int/float, optional): If not none, the offset dimension will be added starting\n            at first offset. Defaults to None.\n        offset_step (int, float, optional): Required if first_offset is specified. The offset increment.\n        vert_domain (str, optional): Vertical domain, one of ('DEPTH', 'TWT'). Defaults to 'TWT'.\n        vert_units(str, optional): Measurement system of of vertical coordinates.\n            One of ('ms', 's', 'm', 'km', 'ft'): Defaults to None for unknown.\n    \"\"\"\n    vert_domain = vert_domain.upper()\n\n    if first_offset is None:\n        ncdp, ns = dims\n    else:\n        ncdp, ns, no = dims\n\n    # check units\n    units = _check_vert_units(vert_units)\n\n    vert = np.arange(\n        first_sample, first_sample + sample_rate * ns, sample_rate, dtype=int\n    )\n    cdps = np.arange(first_cdp, first_cdp + cdp_step * ncdp, cdp_step, dtype=int)\n\n    if first_offset is None:\n        offset = None\n    else:\n        offset = np.arange(\n            first_offset, first_offset + offset_step * no, offset_step, dtype=float\n        )\n\n    builder, domain = _dataset_coordinate_helper(\n        vert, vert_domain, cdp=cdps, offset=offset\n    )\n\n    ds = create_seismic_dataset(**builder)\n    ds.attrs[AttrKeyField.measurement_system] = units\n    ds.attrs[AttrKeyField.d3_domain] = domain\n    ds.attrs[AttrKeyField.sample_rate] = sample_rate\n    ds.attrs[AttrKeyField.text] = \"SEGY-SAK Create 2D Dataset\"\n\n    return ds\n</code></pre>"},{"location":"api/xarray_new.html#segysak.create_seismic_dataset","title":"<code>create_seismic_dataset(twt=None, depth=None, cdp=None, iline=None, xline=None, offset=None, segysak_attr=True, **dim_args)</code>","text":"<p>Create a blank seismic dataset by setting the dimension sizes (d#) or by passing arrays for known dimensions.</p> <p>iline and xline must be specified together and are mutually exclusive to cdp argument.</p> <p>Parameters:</p> Name Type Description Default <code>twt</code> <code>int / array - like</code> <p>Two-way time vertical sampling coordinates. Cannot be used with depth argument. Defaults to None.</p> <code>None</code> <code>depth</code> <code>int / array - like</code> <p>Depth vertical sampling coordinates. Cannot be used with twt argument. Defaults to None.</p> <code>None</code> <code>cdp</code> <code>int / array - like</code> <p>The CDP numbering for 2D data, cannot be used with iline or xline. Use for 2D seismic data. Defaults to None.</p> <code>None</code> <code>iline</code> <code>int / array - like</code> <p>The iline numbering, cannot be used with cdp argument. Use for 3D seismic data. Defaults to None.</p> <code>None</code> <code>xline</code> <code>int / array - like</code> <p>The xline numbering, cannot be used with cdp argument. Use for 3D seismic data. Defaults to None.</p> <code>None</code> <code>offset</code> <code>int / array - like</code> <p>The offset. This will fill dimension d4. Use for pre-stack data. Defaults to None.</p> <code>None</code> <code>segysak_attr</code> <code>bool</code> <p>Add SEGYSAK attributes to the Dataset</p> <code>True</code> <code>dim_args</code> <code>int / array - like</code> <p>Other dimensions you would like in your dataset. The key will be the dimension name.</p> <code>{}</code> <p>Returns:</p> Type Description <p>xarray.Dataset: A dataset with the defined dimensions of input setup to work with seisnc standards.</p> Source code in <code>segysak/_seismic_dataset.py</code> Python<pre><code>def create_seismic_dataset(\n    twt=None,\n    depth=None,\n    cdp=None,\n    iline=None,\n    xline=None,\n    offset=None,\n    segysak_attr=True,\n    **dim_args,\n):\n    \"\"\"Create a blank seismic dataset by setting the dimension sizes (d#) or by passing\n    arrays for known dimensions.\n\n    iline and xline must be specified together and are mutually exclusive to cdp argument.\n\n    Args:\n        twt (int/array-like, optional): Two-way time vertical sampling coordinates.\n            Cannot be used with depth argument. Defaults to None.\n        depth (int/array-like, optional): Depth vertical sampling coordinates.\n            Cannot be used with twt argument. Defaults to None.\n        cdp (int/array-like, optional): The CDP numbering for 2D data, cannot be used\n            with iline or xline. Use for 2D seismic data. Defaults to None.\n        iline (int/array-like, optional): The iline numbering, cannot be\n            used with cdp argument. Use for 3D seismic data. Defaults to None.\n        xline (int/array-like, optional): The xline numbering, cannot be\n            used with cdp argument. Use for 3D seismic data. Defaults to None.\n        offset (int/array-like, optional): The offset. This will fill dimension d4.\n            Use for pre-stack data. Defaults to None.\n        segysak_attr (bool, optional): Add SEGYSAK attributes to the Dataset\n        dim_args (int/array-like): Other dimensions you would like in your dataset. The key will be the dimension name.\n\n    Returns:\n        xarray.Dataset: A dataset with the defined dimensions of input setup to work with seisnc standards.\n    \"\"\"\n    # cdp not allowed with iline or xline\n    if cdp is not None and (iline is not None or xline is not None):\n        raise ValueError(\n            \"cdp argument cannot be used with 3D dimensions (iline or xline)\"\n        )\n\n    if iline is not None and xline is None:\n        raise ValueError(\"xline needed with iline argument, 3d geometry requires both\")\n\n    if xline is not None and iline is None:\n        raise ValueError(\"xline needed with iline argument, 3d geometry requires both\")\n\n    # check inputs\n    twt = _check_input(twt)\n    depth = _check_input(depth)\n    cdp = _check_input(cdp)\n    iline = _check_input(iline)\n    xline = _check_input(xline)\n    offset = _check_input(offset)\n    # # make sure other dim args are sensible and to spec\n    for key in dim_args.keys():\n        dim_args[key] = _check_input(dim_args[key])\n\n    dimensions = dict()\n    # create dimension d1\n    if cdp is not None:\n        dimensions[DimKeyField.cdp] = ([DimKeyField.cdp], cdp)\n    elif iline is not None:  # 3d data\n        dimensions[DimKeyField.iline] = ([DimKeyField.iline], iline)\n        dimensions[DimKeyField.xline] = ([DimKeyField.xline], xline)\n\n    # create dimension d3\n    if twt is not None:\n        dimensions[DimKeyField.twt] = ([DimKeyField.twt], twt)\n    if depth is not None:\n        dimensions[DimKeyField.depth] = ([DimKeyField.depth], depth)\n\n    # create dimension d4\n    if offset is not None:\n        dimensions[DimKeyField.offset] = (\n            [DimKeyField.offset],\n            offset,\n        )\n\n    # any left over dims\n    for arg, val in dim_args.items():\n        dimensions[arg] = ([arg], val)\n\n    ds = xr.Dataset(coords=dimensions)\n\n    if segysak_attr:\n        if twt is not None:\n            ds.attrs[AttrKeyField.ns] = twt.size\n        elif depth is not None:\n            ds.attrs[AttrKeyField.ns] = depth.size\n\n        ds.attrs.update({name: None for name in AttrKeyField})\n\n    return ds\n</code></pre>"},{"location":"cli/about.html","title":"SEGY-SAK - Command Line Interface","text":"<p>SEGY-SAK offers an interface on the command line to inspect SEG-Y files and to convert between data formats. These are convenience wrappers around core <code>segysak</code> functions to allow fast an easy interrogation of/interaction wtih SEG-Y files.</p> <p>For example, it is possible to scrape the text header from a SEG-Y file</p> Bash<pre><code>segysak ebcidc volve10r12-full-twt-sub3d.sgy\nC 1 SEGY OUTPUT FROM Petrel 2017.2 Saturday, June 06 2020 10:15:00\nC 2 Name: ST10010ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534\n\u00ddCroC 3\nC 4 First inline: 10090  Last inline: 10150\nC 5 First xline:  2150   Last xline:  2351\n...\nC37\nC38\nC39\nC40 END EBCDIC\n</code></pre> <p>Standard linux redirects can be used to output the header to a file or to other command line tools.</p> <p>Bash<pre><code>segysak ebcidc volve10r12-full-twt-sub3d.sgy &gt; header.txt\nsegysak ebcidc volve10r12-full-twt-sub3d.sgy | less\n</code></pre> From <code>segysak&gt;=0.5</code> file conversion is conducted with lazy loading, this should allow very large SEG-Y files to be converted to more performant file formats such as ZGY<sup>1</sup> and NetCDF4.</p> Bash<pre><code>segysak convert \n</code></pre> <p>A full list of sub-commands is available in the CLI Reference or individually for each sub-command individually using the <code>--help</code> flag.</p> Bash<pre><code>segysak scan --help\n</code></pre> <ol> <li> <p>ZGY file operations require <code>pyzgy&gt;=0.10</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"cli/command-line-ref.html","title":"CLI Reference","text":""},{"location":"cli/command-line-ref.html#base-command","title":"Base command","text":"<p>segysak</p> Bash<pre><code>Usage: segysak [OPTIONS] COMMAND [ARGS]...\n\n  The SEG-Y Swiss Army Knife (segysak) is a tool for managing segy data. It\n  can read and dump ebcidc headers, scan trace headers, convert SEG-Y to\n  SEISNC and vice versa\n\nOptions:\n  -v, --version  Print application version name\n  --help         Show this message and exit.\n\nCommands:\n  convert  Convert file between SEG-Y and NETCDF (direction is guessed or...\n  ebcidc   Print SEG-Y EBCIDC header\n  scan     Scan trace headers and print value ranges\n  scrape   Scrape the file meta information and output it to text file.\n</code></pre>"},{"location":"cli/command-line-ref.html#ebcidc-headers","title":"EBCIDC headers","text":"<p>segysak ebcidc</p> Bash<pre><code>segysak v0.6.dev4\nUsage: segysak ebcidc [OPTIONS] FILENAME\n\n  Print SEG-Y EBCIDC header\n\nOptions:\n  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/command-line-ref.html#trace-headers","title":"Trace Headers","text":"<p>segysak scan</p> Bash<pre><code>segysak v0.6.dev4\nUsage: segysak scan [OPTIONS] FILENAME\n\n  Scan trace headers and print value ranges\n\nOptions:\n  -m, --max-traces INTEGER  Number of traces to scan\n  --help                    Show this message and exit.\n</code></pre> <p>segysak scrape</p> Bash<pre><code>segysak v0.6.dev4\nUsage: segysak scrape [OPTIONS] [FILENAME]...\n\n  Scrape the file meta information and output it to text file.\n\n  If no options are specified both will be output. The output file will be\n  &lt;filename&gt;.txt for the EBCIDC and &lt;filename&gt;.csv for trace headers.\n\n  The trace headers can be read back into Python using\n  pandas.read_csv(&lt;filename&gt;.csv, index_col=0)\n\nOptions:\n  -e, --ebcidc         Output the text header\n  -h, --trace-headers  Output the trace headers to csv\n  --help               Show this message and exit.\n</code></pre>"},{"location":"cli/command-line-ref.html#file-format-conversion","title":"File format conversion","text":"<p>segysak convert</p> Bash<pre><code>segysak v0.6.dev4\nUsage: segysak convert [OPTIONS] [INPUT_FILES]...\n\n  Convert file between SEG-Y and NETCDF (direction is guessed or can be made\n  explicit with the --output-type option)\n\nOptions:\n  -o, --output-file TEXT        Output file name\n  -il, --iline INTEGER          Inline byte location\n  -xl, --xline INTEGER          Crossline byte location\n  -x, --cdp-x INTEGER           CDP X byte location\n  -y, --cdp-y INTEGER           CDP Y byte location\n  --crop INTEGER...             Crop the input volume providing 4 parameters:\n                                minil maxil minxl maxxl\n  --output-type [SEG-Y|NETCDF]  Explicitly state the desired output file type\n                                by choosing one of the options\n  -d, --dimension TEXT          Data dimension (domain) to write out, will\n                                default to TWT or DEPTH. Only used for writing\n                                to SEG-Y.\n  --help                        Show this message and exit.\n</code></pre>"},{"location":"examples/QuickOverview.html","title":"Quick Overview","text":"<p>The library is imported as segysak and the loaded <code>xarray</code> objects are compatible with numpy and matplotlib.</p> <p>The cropped volume from the Volve field in the North Sea (made available by Equinor) is used for this example, and all the examples and data in this documentation are available from the <code>examples</code> folder of the Github respository.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport pathlib\n</pre> import matplotlib.pyplot as plt import pathlib In\u00a0[2]: Copied! <pre>V3D_path = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\")\nprint(\"3D\", V3D_path, V3D_path.exists())\n</pre> V3D_path = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\") print(\"3D\", V3D_path, V3D_path.exists()) <pre>3D data/volve10r12-full-twt-sub3d.sgy True\n</pre> <p>A basic operation would be to check the text header included in the SEG-Y file. The get_segy_texthead function accounts for common encoding issues and returns the header as a text string. It is important to check the text header if you are unfamiliar with your data, it may provide important information about the contents/location of trace header values which are needed by SEGY-SAK to successfully load your data into a <code>xarray.Dataset</code>.</p> In\u00a0[4]: Copied! <pre>from segysak.segy import get_segy_texthead\n\nget_segy_texthead(V3D_path)\n</pre> from segysak.segy import get_segy_texthead  get_segy_texthead(V3D_path) Out[4]: Text HeaderC 1 SEGY OUTPUT FROM Petrel 2017.2 Saturday, June 06 2020 10:15:00              C 2 Name: ST10010ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534 \u00ddCroC 3                                                                             C 4 First inline: 10090  Last inline: 10150                                     C 5 First xline:  2150   Last xline:  2351                                      C 6 CRS: ED50-UTM31 (\"MENTOR:ED50-UTM31:European 1950 Based UTM, Zone 31 North, C 7 X min: 433955.09 max: 436589.56 delta: 2634.47                              C 8 Y min: 6477439.46 max: 6478790.23 delta: 1350.77                            C 9 Time min: -3402.00 max: -2.00 delta: 3400.00                                C10 Lat min: 58.25'52.8804\"N max: 58.26'37.9493\"N delta: 0.00'45.0689\"          C11 Long min: 1.52'7.1906\"E max: 1.54'50.9616\"E delta: 0.02'43.7710\"            C12 Trace min: -3400.00 max: -4.00 delta: 3396.00                               C13 Seismic (template) min: -58.55 max: 54.55 delta: 113.10                     C14 Amplitude (data) min: -58.55 max: 54.55 delta: 113.10                       C15 Trace sample format: IEEE floating point                                    C16 Coordinate scale factor: 100.00000                                          C17                                                                             C18 Binary header locations:                                                    C19 Sample interval             : bytes 17-18                                   C20 Number of samples per trace : bytes 21-22                                   C21 Trace date format           : bytes 25-26                                   C22                                                                             C23 Trace header locations:                                                     C24 Inline number               : bytes 5-8                                     C25 Xline number                : bytes 21-24                                   C26 Coordinate scale factor     : bytes 71-72                                   C27 X coordinate                : bytes 73-76                                   C28 Y coordinate                : bytes 77-80                                   C29 Trace start time/depth      : bytes 109-110                                 C30 Number of samples per trace : bytes 115-116                                 C31 Sample interval             : bytes 117-118                                 C32                                                                             C33                                                                             C34                                                                             C35                                                                             C36                                                                             C37                                                                             C38                                                                             C39                                                                             C40 END EBCDIC                                                                   <p>If you need to investigate the trace header data more deeply, then segy_header_scan can be used to report basic statistics of each byte position for a limited number of traces.</p> <p>segy_header_scan returns a <code>pandas.DataFrame</code>. To see the full DataFrame use the <code>pandas</code> option_context manager.</p> In\u00a0[5]: Copied! <pre>from segysak.segy import segy_header_scan\nimport pandas as pd\n\nscan = segy_header_scan(V3D_path)\n\nwith pd.option_context('display.max_rows', 100, 'display.max_columns', 10):\n    # drop byte locations where the mean is zero, these are likely empty.\n    display(scan)\n</pre> from segysak.segy import segy_header_scan import pandas as pd  scan = segy_header_scan(V3D_path)  with pd.option_context('display.max_rows', 100, 'display.max_columns', 10):     # drop byte locations where the mean is zero, these are likely empty.     display(scan) byte_loc count mean std min 25% 50% 75% max TRACE_SEQUENCE_LINE 1 1000.0 1.005400e+02 57.831072 1.0 5.075000e+01 100.5 1.502500e+02 202.0 TRACE_SEQUENCE_FILE 5 1000.0 1.009198e+04 1.407687 10090.0 1.009100e+04 10092.0 1.009300e+04 10094.0 FieldRecord 9 1000.0 1.009198e+04 1.407687 10090.0 1.009100e+04 10092.0 1.009300e+04 10094.0 TraceNumber 13 1000.0 1.005400e+02 57.831072 1.0 5.075000e+01 100.5 1.502500e+02 202.0 EnergySourcePoint 17 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 CDP 21 1000.0 2.249540e+03 57.831072 2150.0 2.199750e+03 2249.5 2.299250e+03 2351.0 CDP_TRACE 25 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 TraceIdentificationCode 29 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 NSummedTraces 31 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 NStackedTraces 33 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 DataUse 35 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 offset 37 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 ReceiverGroupElevation 41 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceSurfaceElevation 45 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceDepth 49 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 ReceiverDatumElevation 53 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceDatumElevation 57 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceWaterDepth 61 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GroupWaterDepth 65 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 ElevationScalar 69 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 SourceGroupScalar 71 1000.0 -1.000000e+02 0.000000 -100.0 -1.000000e+02 -100.0 -1.000000e+02 -100.0 SourceX 73 1000.0 4.351992e+07 70152.496037 43396267.0 4.345933e+07 43519976.5 4.358062e+07 43641261.0 SourceY 77 1000.0 6.477772e+08 17532.885301 647744704.0 6.477622e+08 647777222.0 6.477923e+08 647809133.0 GroupX 81 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GroupY 85 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 CoordinateUnits 89 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 WeatheringVelocity 91 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SubWeatheringVelocity 93 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceUpholeTime 95 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GroupUpholeTime 97 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceStaticCorrection 99 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GroupStaticCorrection 101 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TotalStaticApplied 103 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 LagTimeA 105 1000.0 4.000000e+00 0.000000 4.0 4.000000e+00 4.0 4.000000e+00 4.0 LagTimeB 107 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 DelayRecordingTime 109 1000.0 4.000000e+00 0.000000 4.0 4.000000e+00 4.0 4.000000e+00 4.0 MuteTimeStart 111 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 MuteTimeEND 113 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TRACE_SAMPLE_COUNT 115 1000.0 8.500000e+02 0.000000 850.0 8.500000e+02 850.0 8.500000e+02 850.0 TRACE_SAMPLE_INTERVAL 117 1000.0 4.000000e+03 0.000000 4000.0 4.000000e+03 4000.0 4.000000e+03 4000.0 GainType 119 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 InstrumentGainConstant 121 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 InstrumentInitialGain 123 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 Correlated 125 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 SweepFrequencyStart 127 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SweepFrequencyEnd 129 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SweepLength 131 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SweepType 133 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 SweepTraceTaperLengthStart 135 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SweepTraceTaperLengthEnd 137 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TaperType 139 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 AliasFilterFrequency 141 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 AliasFilterSlope 143 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 NotchFilterFrequency 145 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 NotchFilterSlope 147 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 LowCutFrequency 149 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 HighCutFrequency 151 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 LowCutSlope 153 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 HighCutSlope 155 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 YearDataRecorded 157 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 DayOfYear 159 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 HourOfDay 161 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 MinuteOfHour 163 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SecondOfMinute 165 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TimeBaseCode 167 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 TraceWeightingFactor 169 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GeophoneGroupNumberRoll1 171 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GeophoneGroupNumberFirstTraceOrigField 173 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GeophoneGroupNumberLastTraceOrigField 175 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GapSize 177 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 OverTravel 179 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 CDP_X 181 1000.0 4.351992e+07 70152.496037 43396267.0 4.345933e+07 43519976.5 4.358062e+07 43641261.0 CDP_Y 185 1000.0 6.477772e+08 17532.885301 647744704.0 6.477622e+08 647777222.0 6.477923e+08 647809133.0 INLINE_3D 189 1000.0 1.009198e+04 1.407687 10090.0 1.009100e+04 10092.0 1.009300e+04 10094.0 CROSSLINE_3D 193 1000.0 2.249540e+03 57.831072 2150.0 2.199750e+03 2249.5 2.299250e+03 2351.0 ShotPoint 197 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 ShotPointScalar 201 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TraceValueMeasurementUnit 203 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TransductionConstantMantissa 205 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TransductionConstantPower 209 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TransductionUnit 211 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TraceIdentifier 213 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 ScalarTraceHeader 215 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceType 217 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceEnergyDirectionMantissa 219 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceEnergyDirectionExponent 223 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceMeasurementMantissa 225 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceMeasurementExponent 229 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceMeasurementUnit 231 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 UnassignedInt1 233 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 UnassignedInt2 237 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 <p>The header report can also be reduced by filtering blank byte locations. Here we use the standard deviation <code>std</code> to filter away blank values which can help us to understand the composition of the data.</p> <p>For instance, key values like trace UTM coordinates are located in bytes 73 for X &amp; 77 for Y. We can also see the byte positions of the local grid for INLINE_3D in byte 189 and for CROSSLINE_3D in byte 193.</p> In\u00a0[6]: Copied! <pre>scan[scan[\"mean\"] &gt; 0]\n</pre> scan[scan[\"mean\"] &gt; 0] Out[6]: byte_loc count mean std min 25% 50% 75% max TRACE_SEQUENCE_LINE 1 1000.0 1.005400e+02 57.831072 1.0 5.075000e+01 100.5 1.502500e+02 202.0 TRACE_SEQUENCE_FILE 5 1000.0 1.009198e+04 1.407687 10090.0 1.009100e+04 10092.0 1.009300e+04 10094.0 FieldRecord 9 1000.0 1.009198e+04 1.407687 10090.0 1.009100e+04 10092.0 1.009300e+04 10094.0 TraceNumber 13 1000.0 1.005400e+02 57.831072 1.0 5.075000e+01 100.5 1.502500e+02 202.0 CDP 21 1000.0 2.249540e+03 57.831072 2150.0 2.199750e+03 2249.5 2.299250e+03 2351.0 CDP_TRACE 25 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 TraceIdentificationCode 29 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 DataUse 35 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 ElevationScalar 69 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 SourceX 73 1000.0 4.351992e+07 70152.496037 43396267.0 4.345933e+07 43519976.5 4.358062e+07 43641261.0 SourceY 77 1000.0 6.477772e+08 17532.885301 647744704.0 6.477622e+08 647777222.0 6.477923e+08 647809133.0 CoordinateUnits 89 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 LagTimeA 105 1000.0 4.000000e+00 0.000000 4.0 4.000000e+00 4.0 4.000000e+00 4.0 DelayRecordingTime 109 1000.0 4.000000e+00 0.000000 4.0 4.000000e+00 4.0 4.000000e+00 4.0 TRACE_SAMPLE_COUNT 115 1000.0 8.500000e+02 0.000000 850.0 8.500000e+02 850.0 8.500000e+02 850.0 TRACE_SAMPLE_INTERVAL 117 1000.0 4.000000e+03 0.000000 4000.0 4.000000e+03 4000.0 4.000000e+03 4000.0 Correlated 125 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 SweepType 133 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 TaperType 139 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 TimeBaseCode 167 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 CDP_X 181 1000.0 4.351992e+07 70152.496037 43396267.0 4.345933e+07 43519976.5 4.358062e+07 43641261.0 CDP_Y 185 1000.0 6.477772e+08 17532.885301 647744704.0 6.477622e+08 647777222.0 6.477923e+08 647809133.0 INLINE_3D 189 1000.0 1.009198e+04 1.407687 10090.0 1.009100e+04 10092.0 1.009300e+04 10094.0 CROSSLINE_3D 193 1000.0 2.249540e+03 57.831072 2150.0 2.199750e+03 2249.5 2.299250e+03 2351.0 <p>To retreive the raw header content (values) use <code>segy_header_scrape</code>. Setting <code>partial_scan=None</code> will return the full dataframe of trace header information.</p> In\u00a0[7]: Copied! <pre>from segysak.segy import segy_header_scrape\n\nscrape = segy_header_scrape(V3D_path, partial_scan=1000)\nscrape\n</pre> from segysak.segy import segy_header_scrape  scrape = segy_header_scrape(V3D_path, partial_scan=1000) scrape Out[7]: TRACE_SEQUENCE_LINE TRACE_SEQUENCE_FILE FieldRecord TraceNumber EnergySourcePoint CDP CDP_TRACE TraceIdentificationCode NSummedTraces NStackedTraces ... TraceIdentifier ScalarTraceHeader SourceType SourceEnergyDirectionMantissa SourceEnergyDirectionExponent SourceMeasurementMantissa SourceMeasurementExponent SourceMeasurementUnit UnassignedInt1 UnassignedInt2 0 1 10090 10090 1 0 2150 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 2 10090 10090 2 0 2151 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 3 10090 10090 3 0 2152 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 4 10090 10090 4 0 2153 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 5 10090 10090 5 0 2154 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 995 188 10094 10094 188 0 2337 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 996 189 10094 10094 189 0 2338 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 997 190 10094 10094 190 0 2339 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 998 191 10094 10094 191 0 2340 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 999 192 10094 10094 192 0 2341 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 <p>1000 rows \u00d7 91 columns</p> <p>All SEG-Y (2D, 2D gathers, 3D &amp; 3D gathers) are ingested into <code>xarray.Dataset</code> using the Xarray SEGY engine provided by segysak. The engine recognizes the '.segy' and <code>.sgy' extensions automatically when passed to </code>xr.open_dataset()<code>. It is important to provide the </code>dim_bytes_fields<code>and if required the</code>extra_bytes_fields` variables.</p> <p><code>dim_byte_fields</code> specifies the byte locations which determine the orthogonal geometry of your data. This could be CDP for 2D data, iline and xline for 3D data, or iline, xline and offset for pre-stack gathers over 3D data. UTM coordinates are not usually valid dimensions, because if a survey is rotated relative to the UTM grid, you do not get orthogonal dimensions.</p> <p>UTM coordinates and other trace header information you require can be loaded via the <code>extra_bytes_fields</code> argument.</p> <p>Note that the keys you use to label byte fields will be the keys of the dimensions and variables loaded into the dataset.</p> <p>Since v0.5 of SEGY-SAK, the vertical dimension is always labelled as <code>samples</code>.</p> In\u00a0[8]: Copied! <pre>import xarray as xr\nV3D = xr.open_dataset(\n    V3D_path,\n    dim_byte_fields={\"iline\":189, \"xline\":193},\n    extra_byte_fields={\"cdp_x\":73, \"cdp_y\":77}\n)\nV3D\n</pre> import xarray as xr V3D = xr.open_dataset(     V3D_path,     dim_byte_fields={\"iline\":189, \"xline\":193},     extra_byte_fields={\"cdp_x\":73, \"cdp_y\":77} ) V3D Out[8]: <pre>&lt;xarray.Dataset&gt; Size: 42MB\nDimensions:  (iline: 61, xline: 202, samples: 850)\nCoordinates:\n  * iline    (iline) int16 122B 10090 10091 10092 10093 ... 10148 10149 10150\n  * xline    (xline) int16 404B 2150 2151 2152 2153 2154 ... 2348 2349 2350 2351\n  * samples  (samples) float32 3kB 4.0 8.0 12.0 ... 3.392e+03 3.396e+03 3.4e+03\nData variables:\n    cdp_x    (iline, xline) int32 49kB ...\n    cdp_y    (iline, xline) int32 49kB ...\n    data     (iline, xline, samples) float32 42MB ...\nAttributes:\n    seisnc:   {\"coord_scalar\": -100.0, \"coord_scaled\": false}</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>iline: 61</li><li>xline: 202</li><li>samples: 850</li></ul></li><li>Coordinates: (3)<ul><li>iline(iline)int1610090 10091 10092 ... 10149 10150<pre>array([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150], dtype=int16)</pre></li><li>xline(xline)int162150 2151 2152 ... 2349 2350 2351<pre>array([2150, 2151, 2152, ..., 2349, 2350, 2351], dtype=int16)</pre></li><li>samples(samples)float324.0 8.0 12.0 ... 3.396e+03 3.4e+03<pre>array([   4.,    8.,   12., ..., 3392., 3396., 3400.], dtype=float32)</pre></li></ul></li><li>Data variables: (3)<ul><li>cdp_x(iline, xline)int32...<pre>[12322 values with dtype=int32]</pre></li><li>cdp_y(iline, xline)int32...<pre>[12322 values with dtype=int32]</pre></li><li>data(iline, xline, samples)float32...seisnc :{\"source_file\": \"data/volve10r12-full-twt-sub3d.sgy\", \"measurement_system\": \"m\", \"sample_rate\": 4.0}text :C 1 SEGY OUTPUT FROM Petrel 2017.2 Saturday, June 06 2020 10:15:00               C 2 Name: ST10010ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534 \u00ddCro C 3                                                                              C 4 First inline: 10090  Last inline: 10150                                      C 5 First xline:  2150   Last xline:  2351                                       C 6 CRS: ED50-UTM31 (\"MENTOR:ED50-UTM31:European 1950 Based UTM, Zone 31 North,  C 7 X min: 433955.09 max: 436589.56 delta: 2634.47                               C 8 Y min: 6477439.46 max: 6478790.23 delta: 1350.77                             C 9 Time min: -3402.00 max: -2.00 delta: 3400.00                                 C10 Lat min: 58.25'52.8804\"N max: 58.26'37.9493\"N delta: 0.00'45.0689\"           C11 Long min: 1.52'7.1906\"E max: 1.54'50.9616\"E delta: 0.02'43.7710\"             C12 Trace min: -3400.00 max: -4.00 delta: 3396.00                                C13 Seismic (template) min: -58.55 max: 54.55 delta: 113.10                      C14 Amplitude (data) min: -58.55 max: 54.55 delta: 113.10                        C15 Trace sample format: IEEE floating point                                     C16 Coordinate scale factor: 100.00000                                           C17                                                                              C18 Binary header locations:                                                     C19 Sample interval             : bytes 17-18                                    C20 Number of samples per trace : bytes 21-22                                    C21 Trace date format           : bytes 25-26                                    C22                                                                              C23 Trace header locations:                                                      C24 Inline number               : bytes 5-8                                      C25 Xline number                : bytes 21-24                                    C26 Coordinate scale factor     : bytes 71-72                                    C27 X coordinate                : bytes 73-76                                    C28 Y coordinate                : bytes 77-80                                    C29 Trace start time/depth      : bytes 109-110                                  C30 Number of samples per trace : bytes 115-116                                  C31 Sample interval             : bytes 117-118                                  C32                                                                              C33                                                                              C34                                                                              C35                                                                              C36                                                                              C37                                                                              C38                                                                              C39                                                                              C40 END EBCDIC                                                                  <pre>[10473700 values with dtype=float32]</pre></li></ul></li><li>Indexes: (3)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150],\n      dtype='int16', name='iline'))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159,\n       ...\n       2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351],\n      dtype='int16', name='xline', length=202))</pre></li><li>samplesPandasIndex<pre>PandasIndex(Index([   4.0,    8.0,   12.0,   16.0,   20.0,   24.0,   28.0,   32.0,   36.0,\n         40.0,\n       ...\n       3364.0, 3368.0, 3372.0, 3376.0, 3380.0, 3384.0, 3388.0, 3392.0, 3396.0,\n       3400.0],\n      dtype='float32', name='samples', length=850))</pre></li></ul></li><li>Attributes: (1)seisnc :{\"coord_scalar\": -100.0, \"coord_scaled\": false}</li></ul> In\u00a0[9]: Copied! <pre>fig, ax1 = plt.subplots(ncols=1, figsize=(15, 8))\niline_sel = 10093\nV3D.data.transpose(\"samples\", \"iline\", \"xline\", transpose_coords=True).sel(\n    iline=iline_sel\n).plot(yincrease=False, cmap=\"seismic_r\")\nplt.grid(\"grey\")\nplt.ylabel(\"TWT\")\nplt.xlabel(\"XLINE\")\n</pre> fig, ax1 = plt.subplots(ncols=1, figsize=(15, 8)) iline_sel = 10093 V3D.data.transpose(\"samples\", \"iline\", \"xline\", transpose_coords=True).sel(     iline=iline_sel ).plot(yincrease=False, cmap=\"seismic_r\") plt.grid(\"grey\") plt.ylabel(\"TWT\") plt.xlabel(\"XLINE\") Out[9]: <pre>Text(0.5, 0, 'XLINE')</pre> In\u00a0[10]: Copied! <pre>V3D.to_netcdf(\"data/V3D.nc\")\n</pre> V3D.to_netcdf(\"data/V3D.nc\") In\u00a0[11]: Copied! <pre># Here the seismic volume to process one INLINE at a time to the NetCDF4 file.\nV3D.chunk({'iline':1, 'xline':-1, 'samples':-1}).to_netcdf(\"data/V3D_chks.nc\")\n</pre> # Here the seismic volume to process one INLINE at a time to the NetCDF4 file. V3D.chunk({'iline':1, 'xline':-1, 'samples':-1}).to_netcdf(\"data/V3D_chks.nc\") In\u00a0[12]: Copied! <pre>V3D.seisio.to_segy(\n    \"data/V3D-out.segy\",\n    trace_header_map={\"cdp_x\":73, \"cdp_y\":77},\n    iline=189,\n    xline=193,\n)\n</pre> V3D.seisio.to_segy(     \"data/V3D-out.segy\",     trace_header_map={\"cdp_x\":73, \"cdp_y\":77},     iline=189,     xline=193, )"},{"location":"examples/QuickOverview.html#quick-overview","title":"Quick Overview\u00b6","text":"<p>Here you can find some quick examples of what you can do with SEGY-SAK. For more details refer to the examples.</p>"},{"location":"examples/QuickOverview.html#scan-seg-y-headers","title":"Scan SEG-Y headers\u00b6","text":""},{"location":"examples/QuickOverview.html#load-seg-y-data","title":"Load SEG-Y data\u00b6","text":""},{"location":"examples/QuickOverview.html#visualising-data","title":"Visualising data\u00b6","text":"<p><code>xarray</code> objects use smart label based indexing techniques to retreive subsets of data. More details on <code>xarray</code> techniques for segysak are covered in the examples, but this demonstrates a general syntax for selecting data by label with <code>xarray</code>. Plotting is done by <code>matploblib</code> and <code>xarray</code> selections can be passed to normal <code>matplotlib.pyplot</code> functions.</p>"},{"location":"examples/QuickOverview.html#saving-data-to-netcdf4","title":"Saving data to NetCDF4\u00b6","text":"<p>SEGYSAK now loads data in a way that is compatable with the standard <code>to_netcdf</code> method of an <code>xarray.Dataset</code>. To output data to NetCDF4, simply specify the output file and Xarray should take care of the rest. If you have a particularly large SEG-Y file, that will not fit into memory, then you may need to chunk the dataset first prior to output. This will ensure lazy loading and output of data.</p>"},{"location":"examples/QuickOverview.html#saving-data-to-seg-y","title":"Saving data to SEG-Y\u00b6","text":"<p>To return data to SEG-Y after modification use the <code>seisio</code> accessor provided by SEGY-SAK. Unlike the <code>to_netcdf</code> method. Additional arguments are required by <code>to_segy</code> to successfuly write a SEG-Y file with appropriate header information. At a minimum, this should include byte locations for each dimension in the Dataset, but additional variables can be written to trace headers as required.</p>"},{"location":"examples/example_amplitude_extraction_displays.html","title":"Working with seismic and interpreted horizons","text":"In\u00a0[2]: Copied! <pre>import pathlib\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport matplotlib.pyplot as plt\n</pre> import pathlib import numpy as np import pandas as pd import xarray as xr import matplotlib.pyplot as plt In\u00a0[3]: Copied! <pre>from segysak.segy import (\n    get_segy_texthead,\n    segy_header_scan,\n    segy_header_scrape,\n)\n</pre> from segysak.segy import (     get_segy_texthead,     segy_header_scan,     segy_header_scrape, ) In\u00a0[4]: Copied! <pre>segy_file = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\")\ncube = xr.open_dataset(\n    segy_file,\n    dim_byte_fields={\"iline\": 5, \"xline\": 21},\n    extra_byte_fields={\"cdp_x\": 73, \"cdp_y\": 77},\n)\ncube.segysak.scale_coords()\n</pre> segy_file = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\") cube = xr.open_dataset(     segy_file,     dim_byte_fields={\"iline\": 5, \"xline\": 21},     extra_byte_fields={\"cdp_x\": 73, \"cdp_y\": 77}, ) cube.segysak.scale_coords() In\u00a0[5]: Copied! <pre>print(\"Loaded cube size: {}\".format(cube.segysak.humanbytes))\n</pre> print(\"Loaded cube size: {}\".format(cube.segysak.humanbytes)) <pre>Loaded cube size: 40.15 MB\n</pre> In\u00a0[6]: Copied! <pre>hrz_file = pathlib.Path(\"data/hor_twt_hugin_fm_top.dat\")\nhrz = pd.read_csv(hrz_file, names=[\"cdp_x\", \"cdp_y\", \"twt\"], sep=\"\\s+\")\nhrz.head()\n</pre> hrz_file = pathlib.Path(\"data/hor_twt_hugin_fm_top.dat\") hrz = pd.read_csv(hrz_file, names=[\"cdp_x\", \"cdp_y\", \"twt\"], sep=\"\\s+\") hrz.head() Out[6]: cdp_x cdp_y twt 0 432186.713151 6.477029e+06 2776.275147 1 432189.737524 6.477041e+06 2779.657715 2 432192.761898 6.477053e+06 2780.465088 3 432195.786271 6.477066e+06 2780.949951 4 432198.810645 6.477078e+06 2781.769775 In\u00a0[7]: Copied! <pre>cp = cube.segysak.calc_corner_points()\ncorners = np.array(cp)\ncorners\n</pre> cp = cube.segysak.calc_corner_points() corners = np.array(cp) corners Out[7]: <pre>array([[ 436400.52, 6477447.04],\n       [ 433962.67, 6478054.94],\n       [ 434144.13, 6478782.66],\n       [ 436581.98, 6478174.76],\n       [ 436400.52, 6477447.04]])</pre> <p>To display the horizon we need to grid the raw data first:</p> In\u00a0[8]: Copied! <pre>from scipy.interpolate import griddata\n\nxi = np.linspace(hrz.cdp_x.min(), hrz.cdp_x.max(), 250)\nyi = np.linspace(hrz.cdp_y.min(), hrz.cdp_y.max(), 250)\nX, Y = np.meshgrid(xi, yi)\nZ = griddata((hrz.cdp_x, hrz.cdp_y), hrz.twt, (X, Y), rescale=True)\n</pre> from scipy.interpolate import griddata  xi = np.linspace(hrz.cdp_x.min(), hrz.cdp_x.max(), 250) yi = np.linspace(hrz.cdp_y.min(), hrz.cdp_y.max(), 250) X, Y = np.meshgrid(xi, yi) Z = griddata((hrz.cdp_x, hrz.cdp_y), hrz.twt, (X, Y), rescale=True) <p>And this is the resulting plot with the extent of the loaded cube displayed as a thick red outline:</p> In\u00a0[9]: Copied! <pre>from matplotlib.patches import Polygon\n\nsurvey_limits = Polygon(\n    corners, fill=False, edgecolor=\"r\", linewidth=2, label=\"3D survey extent\"\n)\n\nf, ax = plt.subplots(figsize=(8, 6))\npp = ax.pcolormesh(X, Y, Z, cmap=\"terrain_r\")\nf.colorbar(pp, orientation=\"horizontal\", label=\"TWT [ms]\")\nax.add_patch(survey_limits)\nax.axis(\"equal\")\nax.legend()\nax.set_title(\"Top Hugin fm.\")\n</pre> from matplotlib.patches import Polygon  survey_limits = Polygon(     corners, fill=False, edgecolor=\"r\", linewidth=2, label=\"3D survey extent\" )  f, ax = plt.subplots(figsize=(8, 6)) pp = ax.pcolormesh(X, Y, Z, cmap=\"terrain_r\") f.colorbar(pp, orientation=\"horizontal\", label=\"TWT [ms]\") ax.add_patch(survey_limits) ax.axis(\"equal\") ax.legend() ax.set_title(\"Top Hugin fm.\") Out[9]: <pre>Text(0.5, 1.0, 'Top Hugin fm.')</pre> In\u00a0[10]: Copied! <pre>cube = cube.set_coords(('cdp_x', 'cdp_y'))\nhrz_mapped = cube.seis.surface_from_points(hrz, \"twt\", right=(\"cdp_x\", \"cdp_y\"))\n</pre> cube = cube.set_coords(('cdp_x', 'cdp_y')) hrz_mapped = cube.seis.surface_from_points(hrz, \"twt\", right=(\"cdp_x\", \"cdp_y\")) <p>And to extract seismic amplitudes along this horizon we use the magic of <code>xarray</code>:</p> In\u00a0[11]: Copied! <pre>amp = cube.data.interp(\n    {\"iline\": hrz_mapped.iline, \"xline\": hrz_mapped.xline, \"samples\": hrz_mapped.twt}\n)\n</pre> amp = cube.data.interp(     {\"iline\": hrz_mapped.iline, \"xline\": hrz_mapped.xline, \"samples\": hrz_mapped.twt} ) <p>For the next plot we use another attribute automatically calculated by segysak during loading to squeeze the colormap used when displaying amplitudes:</p> In\u00a0[12]: Copied! <pre>#minamp, maxamp = cube.attrs[\"percentiles\"][1], cube.attrs[\"percentiles\"][-2]\n</pre> #minamp, maxamp = cube.attrs[\"percentiles\"][1], cube.attrs[\"percentiles\"][-2] <p>...and we calculate the minimum and maximum X and Y coordinates using the <code>corners</code> array described above to set the figure extent and zoom in the area covered by the seismic cube:</p> In\u00a0[13]: Copied! <pre>xmin, xmax = corners[:, 0].min(), corners[:, 0].max()\nymin, ymax = corners[:, 1].min(), corners[:, 1].max()\n</pre> xmin, xmax = corners[:, 0].min(), corners[:, 0].max() ymin, ymax = corners[:, 1].min(), corners[:, 1].max() <p>We can plot <code>amp</code> now on top of the same twt grid we did above:</p> In\u00a0[14]: Copied! <pre>survey_limits = Polygon(\n    corners, fill=False, edgecolor=\"r\", linewidth=2, label=\"3D survey extent\"\n)\n\nf, ax = plt.subplots(nrows=2, figsize=(8, 10))\nax[0].pcolormesh(X, Y, Z, cmap=\"terrain_r\")\nax[0].add_patch(survey_limits)\nfor aa in ax:\n    hh = aa.pcolormesh(\n        amp.cdp_x, amp.cdp_y, amp.data, cmap=\"RdYlBu\", #vmin=minamp, vmax=maxamp\n    )\n    aa.axis(\"equal\")\nax[0].legend()\nax[1].set_xlim(xmin, xmax)\nax[1].set_ylim(ymin, ymax)\nax[0].set_title(\"Top Hugin fm. and amplitude extraction on loaded seismic\")\nax[1].set_title(\"Amplitude extraction at Top Hugin (zoom)\")\ncax = f.add_axes([0.15, 0.16, 0.5, 0.02])\nf.colorbar(hh, cax=cax, orientation=\"horizontal\")\n</pre> survey_limits = Polygon(     corners, fill=False, edgecolor=\"r\", linewidth=2, label=\"3D survey extent\" )  f, ax = plt.subplots(nrows=2, figsize=(8, 10)) ax[0].pcolormesh(X, Y, Z, cmap=\"terrain_r\") ax[0].add_patch(survey_limits) for aa in ax:     hh = aa.pcolormesh(         amp.cdp_x, amp.cdp_y, amp.data, cmap=\"RdYlBu\", #vmin=minamp, vmax=maxamp     )     aa.axis(\"equal\") ax[0].legend() ax[1].set_xlim(xmin, xmax) ax[1].set_ylim(ymin, ymax) ax[0].set_title(\"Top Hugin fm. and amplitude extraction on loaded seismic\") ax[1].set_title(\"Amplitude extraction at Top Hugin (zoom)\") cax = f.add_axes([0.15, 0.16, 0.5, 0.02]) f.colorbar(hh, cax=cax, orientation=\"horizontal\") Out[14]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7f6b80d34190&gt;</pre> <p>Another classic display is to superimpose the structure contours to the amplitudes. This is much faster and easier using survey coordinates (inlines and crosslines):</p> In\u00a0[15]: Copied! <pre>f, ax = plt.subplots(figsize=(12, 4))\namp.plot(cmap=\"RdYlBu\")\ncs = plt.contour(amp.xline, amp.iline, hrz_mapped.twt, levels=20, colors=\"grey\")\nplt.clabel(cs, fontsize=10, fmt=\"%.0f\")\nax.invert_xaxis()\n</pre> f, ax = plt.subplots(figsize=(12, 4)) amp.plot(cmap=\"RdYlBu\") cs = plt.contour(amp.xline, amp.iline, hrz_mapped.twt, levels=20, colors=\"grey\") plt.clabel(cs, fontsize=10, fmt=\"%.0f\") ax.invert_xaxis() In\u00a0[16]: Copied! <pre>opt = dict(\n    x=\"xline\",\n    y=\"samples\",\n    add_colorbar=True,\n    interpolation=\"spline16\",\n    robust=True,\n    yincrease=False,\n    cmap=\"Greys\",\n)\n\ninl_sel = [10130, 10100]\n\nf, ax = plt.subplots(nrows=2, figsize=(10, 6), sharey=True, constrained_layout=True)\nfor i, val in enumerate(inl_sel):\n    cube.data.sel(iline=val, samples=slice(2300, 3000)).plot.imshow(ax=ax[i], **opt)\n    x, t = hrz_mapped.sel(iline=val).xline, hrz_mapped.sel(iline=val).twt\n    ax[i].plot(x, t, color=\"r\")\n    ax[i].invert_xaxis()\n</pre> opt = dict(     x=\"xline\",     y=\"samples\",     add_colorbar=True,     interpolation=\"spline16\",     robust=True,     yincrease=False,     cmap=\"Greys\", )  inl_sel = [10130, 10100]  f, ax = plt.subplots(nrows=2, figsize=(10, 6), sharey=True, constrained_layout=True) for i, val in enumerate(inl_sel):     cube.data.sel(iline=val, samples=slice(2300, 3000)).plot.imshow(ax=ax[i], **opt)     x, t = hrz_mapped.sel(iline=val).xline, hrz_mapped.sel(iline=val).twt     ax[i].plot(x, t, color=\"r\")     ax[i].invert_xaxis() <p>We can also show an overlay of amplitudes and two-way-times along the same inlines:</p> In\u00a0[17]: Copied! <pre>inl_sel = [10130, 10100]\n\nf, ax = plt.subplots(nrows=2, figsize=(10, 6), sharey=True, constrained_layout=True)\n\nfor i, val in enumerate(inl_sel):\n    axz = ax[i].twinx()\n    x, t = amp.sel(iline=val).xline, amp.sel(iline=val).samples\n    a = amp.sel(iline=val).data\n    ax[i].plot(x, a, color=\"r\")\n    axz.plot(x, t, color=\"k\")\n    ax[i].invert_xaxis()\n    axz.invert_yaxis()\n    ax[i].set_ylabel(\"Amplitude\", color=\"r\")\n    plt.setp(ax[i].yaxis.get_majorticklabels(), color=\"r\")\n    axz.set_ylabel(\"TWT [ms]\")\n    ax[i].set_title(\"Amplitude and two-way-time at inline {}\".format(val))\n</pre> inl_sel = [10130, 10100]  f, ax = plt.subplots(nrows=2, figsize=(10, 6), sharey=True, constrained_layout=True)  for i, val in enumerate(inl_sel):     axz = ax[i].twinx()     x, t = amp.sel(iline=val).xline, amp.sel(iline=val).samples     a = amp.sel(iline=val).data     ax[i].plot(x, a, color=\"r\")     axz.plot(x, t, color=\"k\")     ax[i].invert_xaxis()     axz.invert_yaxis()     ax[i].set_ylabel(\"Amplitude\", color=\"r\")     plt.setp(ax[i].yaxis.get_majorticklabels(), color=\"r\")     axz.set_ylabel(\"TWT [ms]\")     ax[i].set_title(\"Amplitude and two-way-time at inline {}\".format(val)) <p>To create windows map extraction it is necessary to first mask the seisnc cube and then to collapse it along the chosen axis using a prefered method. In this case we are just going to calculate the mean amplitude in a 100ms window below our horizon.</p> In\u00a0[18]: Copied! <pre>mask_below = cube.where(cube.samples &lt; hrz_mapped.twt + 100)\nmask_above = cube.where(cube.samples &gt; hrz_mapped.twt)\nmasks = [mask_above, mask_below]\n</pre> mask_below = cube.where(cube.samples &lt; hrz_mapped.twt + 100) mask_above = cube.where(cube.samples &gt; hrz_mapped.twt) masks = [mask_above, mask_below] <p>Lets see what our masks look like</p> In\u00a0[19]: Copied! <pre>opt = dict(\n    x=\"xline\",\n    y=\"samples\",\n    add_colorbar=True,\n    interpolation=\"spline16\",\n    robust=True,\n    yincrease=False,\n    cmap=\"Greys\",\n)\n\ninl_sel = [10130, 10100]\n\nf, ax = plt.subplots(nrows=2, figsize=(10, 6), sharey=True, constrained_layout=True)\nfor i, val in enumerate(\n    inl_sel,\n):\n    masks[i].data.sel(iline=val, samples=slice(2300, 3000)).plot.imshow(ax=ax[i], **opt)\n    x, t = hrz_mapped.sel(iline=val).xline, hrz_mapped.sel(iline=val).twt\n    ax[i].plot(x, t, color=\"r\")\n    ax[i].invert_xaxis()\n</pre> opt = dict(     x=\"xline\",     y=\"samples\",     add_colorbar=True,     interpolation=\"spline16\",     robust=True,     yincrease=False,     cmap=\"Greys\", )  inl_sel = [10130, 10100]  f, ax = plt.subplots(nrows=2, figsize=(10, 6), sharey=True, constrained_layout=True) for i, val in enumerate(     inl_sel, ):     masks[i].data.sel(iline=val, samples=slice(2300, 3000)).plot.imshow(ax=ax[i], **opt)     x, t = hrz_mapped.sel(iline=val).xline, hrz_mapped.sel(iline=val).twt     ax[i].plot(x, t, color=\"r\")     ax[i].invert_xaxis() <p>And if we combine the masks.</p> In\u00a0[20]: Copied! <pre>opt = dict(\n    x=\"xline\",\n    y=\"samples\",\n    add_colorbar=True,\n    interpolation=\"spline16\",\n    robust=True,\n    yincrease=False,\n    cmap=\"Greys\",\n)\n\ninl_sel = [10130, 10100]\n\nf, ax = plt.subplots(nrows=1, figsize=(10, 3), sharey=True, constrained_layout=True)\n\nmasked_data = 0.5 * (mask_below + mask_above)\n\nmasked_data.data.sel(iline=val, samples=slice(2300, 3000)).plot.imshow(ax=ax, **opt)\nx, t = hrz_mapped.sel(iline=val).xline, hrz_mapped.sel(iline=val).twt\nax.plot(x, t, color=\"r\")\nax.invert_xaxis()\n</pre> opt = dict(     x=\"xline\",     y=\"samples\",     add_colorbar=True,     interpolation=\"spline16\",     robust=True,     yincrease=False,     cmap=\"Greys\", )  inl_sel = [10130, 10100]  f, ax = plt.subplots(nrows=1, figsize=(10, 3), sharey=True, constrained_layout=True)  masked_data = 0.5 * (mask_below + mask_above)  masked_data.data.sel(iline=val, samples=slice(2300, 3000)).plot.imshow(ax=ax, **opt) x, t = hrz_mapped.sel(iline=val).xline, hrz_mapped.sel(iline=val).twt ax.plot(x, t, color=\"r\") ax.invert_xaxis() <p>To get the horizon window extraction for sum of amplitudes we now need to sum along the time axis. Or we can use the <code>np.apply_along_axis</code> function to apply a custom function to our masked cube.</p> In\u00a0[21]: Copied! <pre>summed_amp = masked_data.sum(dim=\"samples\")\n\nf, ax = plt.subplots(figsize=(12, 4))\nsummed_amp.data.plot.imshow(\n    cmap=\"RdYlBu\",\n    interpolation=\"spline16\",\n)\ncs = plt.contour(amp.xline, amp.iline, hrz_mapped.twt, levels=20, colors=\"grey\")\nplt.clabel(cs, fontsize=10, fmt=\"%.0f\")\nax.invert_xaxis()\nax.set_title(\"Sum of amplitudes Top Hugin 0 to +100ms\")\n</pre> summed_amp = masked_data.sum(dim=\"samples\")  f, ax = plt.subplots(figsize=(12, 4)) summed_amp.data.plot.imshow(     cmap=\"RdYlBu\",     interpolation=\"spline16\", ) cs = plt.contour(amp.xline, amp.iline, hrz_mapped.twt, levels=20, colors=\"grey\") plt.clabel(cs, fontsize=10, fmt=\"%.0f\") ax.invert_xaxis() ax.set_title(\"Sum of amplitudes Top Hugin 0 to +100ms\") Out[21]: <pre>Text(0.5, 1.0, 'Sum of amplitudes Top Hugin 0 to +100ms')</pre>"},{"location":"examples/example_amplitude_extraction_displays.html#working-with-seismic-and-interpreted-horizons","title":"Working with seismic and interpreted horizons\u00b6","text":""},{"location":"examples/example_amplitude_extraction_displays.html#load-horizon-data","title":"Load horizon data\u00b6","text":"<p>First we load the horizon data to a Pandas DataFrame and take a look at the first few lines:</p>"},{"location":"examples/example_amplitude_extraction_displays.html#display-horizon-and-seismic-extents","title":"Display horizon and seismic extents\u00b6","text":"<p>We can build a plot to display the horizon in its entirety and overlay it with the extent of the seismic cube previously loaded.</p> <p>First we use segysak built-in <code>calc_corner_points()</code> method to calculate the corner points of the loaded cube and copy them to a numpy array to be used for the plot:</p>"},{"location":"examples/example_amplitude_extraction_displays.html#extracting-amplitudes-along-the-horizon","title":"Extracting amplitudes along the horizon\u00b6","text":"<p>This is where the magic of segysak comes in. We use <code>surface_from_points</code> to map the loaded horizon imported in tabular format to each seismic bin. The input horizon in this case is defined in geographical coordinates but it would also have worked if it was defined in inlines and crosslines:</p>"},{"location":"examples/example_amplitude_extraction_displays.html#display-horizon-in-section-view","title":"Display horizon in section view\u00b6","text":""},{"location":"examples/example_amplitude_extraction_displays.html#horizon-sculpting-and-windowed-map-extraction","title":"Horizon Sculpting and Windowed map Extraction\u00b6","text":""},{"location":"examples/example_extract_arbitrary_line.html","title":"Extract an arbitrary line from a 3D volume","text":"In\u00a0[2]: Copied! <pre>import pathlib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport wellpathpy as wpp\nimport xarray as xr\n</pre> import pathlib import numpy as np import pandas as pd import matplotlib.pyplot as plt import wellpathpy as wpp import xarray as xr In\u00a0[3]: Copied! <pre>from segysak import __version__\n\nprint(__version__)\n</pre> from segysak import __version__  print(__version__) <pre>0.6.dev4\n</pre> In\u00a0[4]: Copied! <pre>volve_3d_path = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\")\nprint(f\"{volve_3d_path} exists: {volve_3d_path.exists()}\")\n</pre> volve_3d_path = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\") print(f\"{volve_3d_path} exists: {volve_3d_path.exists()}\") <pre>data/volve10r12-full-twt-sub3d.sgy exists: True\n</pre> In\u00a0[5]: Copied! <pre>from segysak.segy import (\n    get_segy_texthead,\n    segy_header_scan,\n    segy_header_scrape,\n)\n\nvolve_3d = xr.open_dataset(volve_3d_path, dim_byte_fields={\"iline\":189, \"xline\":193},\n    extra_byte_fields={\"cdp_x\":73, \"cdp_y\":77} )\n\n# remember to scale the coordinate if required\nvolve_3d.segysak.scale_coords()\n_ = volve_3d.segysak.get_affine_transform()\n</pre> from segysak.segy import (     get_segy_texthead,     segy_header_scan,     segy_header_scrape, )  volve_3d = xr.open_dataset(volve_3d_path, dim_byte_fields={\"iline\":189, \"xline\":193},     extra_byte_fields={\"cdp_x\":73, \"cdp_y\":77} )  # remember to scale the coordinate if required volve_3d.segysak.scale_coords() _ = volve_3d.segysak.get_affine_transform() In\u00a0[6]: Copied! <pre>arb_line_A = np.array([\n    [434300, 6.4786e6],\n    [434600, 6.4780e6],\n    [435500, 6.4779e6],\n    [436300, 6.4781e6],\n])\n\narb_line_B = np.array([\n    [434000, 6.4786e6],\n    [434600, 6.4776e6],\n    [435500, 6.4786e6],\n    [436500, 6.4775e6],\n])\n</pre> arb_line_A = np.array([     [434300, 6.4786e6],     [434600, 6.4780e6],     [435500, 6.4779e6],     [436300, 6.4781e6], ])  arb_line_B = np.array([     [434000, 6.4786e6],     [434600, 6.4776e6],     [435500, 6.4786e6],     [436500, 6.4775e6], ]) <p>Let's see how these lines are placed relative to the survey bounds. We can see A is fully enclosed whilst B has some segments outside.</p> In\u00a0[7]: Copied! <pre>ax = volve_3d.segysak.plot_bounds()\nax.plot(arb_line_A[:, 0], arb_line_A[:, 1], \".-\", label=\"Arb Line A\")\nax.plot(arb_line_B[:, 0], arb_line_B[:, 1], \".-\", label=\"Arb Line B\")\nax.legend()\n</pre> ax = volve_3d.segysak.plot_bounds() ax.plot(arb_line_A[:, 0], arb_line_A[:, 1], \".-\", label=\"Arb Line A\") ax.plot(arb_line_B[:, 0], arb_line_B[:, 1], \".-\", label=\"Arb Line B\") ax.legend() Out[7]: <pre>&lt;matplotlib.legend.Legend at 0x7f2d4ffdd000&gt;</pre> <p>Let's extract line A.</p> <p>We specify a <code>bin_spacing_hint</code> which is our desired bin spacing along the line. The function use this hint to calculate the closest binspacing that maintains uniform sampling.</p> <p>We have also specified the <code>method='linear'</code>, this is the default but you can specify and method that <code>DataArray.interp</code> accepts</p> In\u00a0[8]: Copied! <pre>from time import time\n\ntic = time()\nline_A = volve_3d.segysak.interp_line(arb_line_A, bin_spacing_hint=10)\ntoc = time()\nprint(f\"That took {toc-tic} seconds\")\n</pre> from time import time  tic = time() line_A = volve_3d.segysak.interp_line(arb_line_A, bin_spacing_hint=10) toc = time() print(f\"That took {toc-tic} seconds\") <pre>That took 0.18013238906860352 seconds\n</pre> In\u00a0[9]: Copied! <pre>line_A.data.T.plot(yincrease=False, cmap=\"RdBu\", vmin=-10, vmax=10)\n</pre> line_A.data.T.plot(yincrease=False, cmap=\"RdBu\", vmin=-10, vmax=10) Out[9]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7f2d4cd507f0&gt;</pre> <p>Now, let's extract line B. Note that blank traces are inserted where the line extends outside the survey bounds.</p> In\u00a0[10]: Copied! <pre>tic = time()\nline_B = volve_3d.segysak.interp_line(arb_line_B, bin_spacing_hint=10)\ntoc = time()\nprint(f\"That took {toc-tic} seconds\")\n</pre> tic = time() line_B = volve_3d.segysak.interp_line(arb_line_B, bin_spacing_hint=10) toc = time() print(f\"That took {toc-tic} seconds\") <pre>That took 0.14196348190307617 seconds\n</pre> In\u00a0[11]: Copied! <pre>line_B.data.T.plot(yincrease=False, cmap=\"RdBu\", vmin=-10, vmax=10)\n</pre> line_B.data.T.plot(yincrease=False, cmap=\"RdBu\", vmin=-10, vmax=10) Out[11]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7f2d4cc305b0&gt;</pre> In\u00a0[12]: Copied! <pre>import shapefile\nfrom pprint import pprint\n</pre> import shapefile from pprint import pprint <p>Load the shapefile and get the list of points from the first shape object <code>sf.shape(0)</code>.</p> In\u00a0[13]: Copied! <pre>sf = shapefile.Reader(pathlib.Path(\"data/arbitrary_line.shp\"))\n\n# see the shapes in the file -&gt; there is just one `Shape #0`\nprint(sf.shapes())\n\n# get the first shape\npetrel_line = sf.shape(0)\npetrel_line_points = np.asarray(petrel_line.points)\n</pre> sf = shapefile.Reader(pathlib.Path(\"data/arbitrary_line.shp\"))  # see the shapes in the file -&gt; there is just one `Shape #0` print(sf.shapes())  # get the first shape petrel_line = sf.shape(0) petrel_line_points = np.asarray(petrel_line.points) <pre>Shapes: [Shape #0: POLYLINEZ]\n</pre> <p>Load the segy containing the line that Petrel extracted along this geometry</p> In\u00a0[14]: Copied! <pre>line_extracted_by_petrel_path = pathlib.Path(\"data/volve10r12-full-twt-arb.sgy\")\nprint(\n    f\"{line_extracted_by_petrel_path} exists: {line_extracted_by_petrel_path.exists()}\"\n)\nline_extracted_by_petrel = xr.open_dataset(\n    line_extracted_by_petrel_path,\n    dim_byte_fields={\"cdp\":21,},\n    extra_byte_fields={\"cdp_x\":73, \"cdp_y\":77} \n)\n</pre> line_extracted_by_petrel_path = pathlib.Path(\"data/volve10r12-full-twt-arb.sgy\") print(     f\"{line_extracted_by_petrel_path} exists: {line_extracted_by_petrel_path.exists()}\" ) line_extracted_by_petrel = xr.open_dataset(     line_extracted_by_petrel_path,     dim_byte_fields={\"cdp\":21,},     extra_byte_fields={\"cdp_x\":73, \"cdp_y\":77}  ) <pre>data/volve10r12-full-twt-arb.sgy exists: True\n</pre> <p>Extract the line using segysak</p> In\u00a0[15]: Copied! <pre>tic = time()\nline_extracted_by_segysak = volve_3d.segysak.interp_line(\n    petrel_line.points,\n    bin_spacing_hint=10,\n    line_method=\"linear\",\n    xysel_method=\"linear\",\n)\nline_extracted_by_petrel.segysak.scale_coords()\ntoc = time()\nprint(f\"That took {toc-tic} seconds\")\n</pre> tic = time() line_extracted_by_segysak = volve_3d.segysak.interp_line(     petrel_line.points,     bin_spacing_hint=10,     line_method=\"linear\",     xysel_method=\"linear\", ) line_extracted_by_petrel.segysak.scale_coords() toc = time() print(f\"That took {toc-tic} seconds\") <pre>That took 0.10639739036560059 seconds\n</pre> <p>Plot the extracted lines side by side</p> In\u00a0[16]: Copied! <pre>fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n\nline_extracted_by_segysak.data.T.plot(yincrease=False, vmin=-10, vmax=10, cmap='RdBu', ax=axs[0])\naxs[0].set_title(\"segysak\")\n\nline_extracted_by_petrel.data.T.plot(yincrease=False, vmin=-10, vmax=10, cmap='RdBu', ax=axs[1])\naxs[1].set_title(\"petrel\")\n</pre> fig, axs = plt.subplots(1, 2, figsize=(16, 8))  line_extracted_by_segysak.data.T.plot(yincrease=False, vmin=-10, vmax=10, cmap='RdBu', ax=axs[0]) axs[0].set_title(\"segysak\")  line_extracted_by_petrel.data.T.plot(yincrease=False, vmin=-10, vmax=10, cmap='RdBu', ax=axs[1]) axs[1].set_title(\"petrel\") Out[16]: <pre>Text(0.5, 1.0, 'petrel')</pre> <p>Plot the geometry, trace header locations along with the volve 3d bound box, to make sure things line up.</p> In\u00a0[17]: Copied! <pre>plt.figure(figsize=(10, 10))\nax = volve_3d.segysak.plot_bounds(ax=plt.gca())\n\n# plot path\nax.plot(petrel_line_points[:, 0], petrel_line_points[:, 1])\nax.scatter(petrel_line_points[:, 0], petrel_line_points[:, 1])\n\n# plot trace positons from extracted lines based on header\nax.scatter(\n    line_extracted_by_segysak.cdp_x,\n    line_extracted_by_segysak.cdp_y,\n    marker=\"x\",\n    color=\"k\",\n)\nax.scatter(\n    line_extracted_by_petrel.cdp_x,\n    line_extracted_by_petrel.cdp_y,\n    marker=\"+\",\n    color=\"r\",\n)\n\nax.set_xlim(434500, 435500)\nax.set_ylim(6477800, 6478600)\nplt.legend(labels=[\"bounds\", \"geom\", \"corner\", \"segysak\", \"petrel\"])\n</pre> plt.figure(figsize=(10, 10)) ax = volve_3d.segysak.plot_bounds(ax=plt.gca())  # plot path ax.plot(petrel_line_points[:, 0], petrel_line_points[:, 1]) ax.scatter(petrel_line_points[:, 0], petrel_line_points[:, 1])  # plot trace positons from extracted lines based on header ax.scatter(     line_extracted_by_segysak.cdp_x,     line_extracted_by_segysak.cdp_y,     marker=\"x\",     color=\"k\", ) ax.scatter(     line_extracted_by_petrel.cdp_x,     line_extracted_by_petrel.cdp_y,     marker=\"+\",     color=\"r\", )  ax.set_xlim(434500, 435500) ax.set_ylim(6477800, 6478600) plt.legend(labels=[\"bounds\", \"geom\", \"corner\", \"segysak\", \"petrel\"]) Out[17]: <pre>&lt;matplotlib.legend.Legend at 0x7f2d4c8c8df0&gt;</pre> In\u00a0[18]: Copied! <pre>f12_dev = pd.read_csv(\"data/well_f12_deviation.asc\", comment=\"#\", sep='\\s+')\nf12_dev_pos = wpp.deviation(*f12_dev[[\"MD\", \"INCL\", \"AZIM_GN\"]].values.T)\n\n# depth values in MD that we want to sample the seismic cube at\nnew_depths = np.arange(0, f12_dev[\"MD\"].max(), 1)\n\n# use minimum curvature and resample to 1m interval\nf12_dev_pos = f12_dev_pos.minimum_curvature().resample(new_depths)\n\n# adjust position of deviation to local coordinates and TVDSS\nf12_dev_pos.to_wellhead(\n    6478566.23,\n    435050.21,\n    inplace=True,\n)\nf12_dev_pos.to_tvdss(\n    54.9,\n    inplace=True,\n)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nvolve_3d.segysak.plot_bounds(ax=ax)\nsc = ax.scatter(f12_dev_pos.easting, f12_dev_pos.northing, c=f12_dev_pos.depth, s=1)\nplt.colorbar(sc, label=\"F12 Depth\")\nax.set_aspect(\"equal\")\n</pre> f12_dev = pd.read_csv(\"data/well_f12_deviation.asc\", comment=\"#\", sep='\\s+') f12_dev_pos = wpp.deviation(*f12_dev[[\"MD\", \"INCL\", \"AZIM_GN\"]].values.T)  # depth values in MD that we want to sample the seismic cube at new_depths = np.arange(0, f12_dev[\"MD\"].max(), 1)  # use minimum curvature and resample to 1m interval f12_dev_pos = f12_dev_pos.minimum_curvature().resample(new_depths)  # adjust position of deviation to local coordinates and TVDSS f12_dev_pos.to_wellhead(     6478566.23,     435050.21,     inplace=True, ) f12_dev_pos.to_tvdss(     54.9,     inplace=True, )  fig, ax = plt.subplots(figsize=(10, 5)) volve_3d.segysak.plot_bounds(ax=ax) sc = ax.scatter(f12_dev_pos.easting, f12_dev_pos.northing, c=f12_dev_pos.depth, s=1) plt.colorbar(sc, label=\"F12 Depth\") ax.set_aspect(\"equal\") <p>We can easily sample the seismic cube by converting the positional log to <code>iline</code> and <code>xline</code> using the Affine transform for our data. We also need to convert the TVDSS values of the data to TWT (in this case we will just use a constant velocity).</p> <p>In both instances we will create custom xarray.DataArray instances because this allows us to relate the coordinate systems of well samples (on the new dimension <code>well</code>) to the <code>iline</code> and <code>xline</code> dimensions of the cube.</p> In\u00a0[19]: Copied! <pre># need the inverse to go from xy to il/xl\naffine = volve_3d.segysak.get_affine_transform().inverted()\nilxl = affine.transform(np.dstack([f12_dev_pos.easting, f12_dev_pos.northing])[0])\n\nf12_dev_ilxl = dict(\n    iline=xr.DataArray(ilxl[:, 0], dims=\"well\", coords={\"well\": range(ilxl.shape[0])}),\n    xline=xr.DataArray(ilxl[:, 1], dims=\"well\", coords={\"well\": range(ilxl.shape[0])}),\n)\n\ntwt = xr.DataArray(\n    -1.0 * f12_dev_pos.depth * 2000 / 2400,  # 2400 m/s to convert to TWT - and negate,\n    dims=\"well\",\n    coords={\"well\": range(ilxl.shape[0])},\n)\n\nf12_dev_ilxl\n</pre> # need the inverse to go from xy to il/xl affine = volve_3d.segysak.get_affine_transform().inverted() ilxl = affine.transform(np.dstack([f12_dev_pos.easting, f12_dev_pos.northing])[0])  f12_dev_ilxl = dict(     iline=xr.DataArray(ilxl[:, 0], dims=\"well\", coords={\"well\": range(ilxl.shape[0])}),     xline=xr.DataArray(ilxl[:, 1], dims=\"well\", coords={\"well\": range(ilxl.shape[0])}), )  twt = xr.DataArray(     -1.0 * f12_dev_pos.depth * 2000 / 2400,  # 2400 m/s to convert to TWT - and negate,     dims=\"well\",     coords={\"well\": range(ilxl.shape[0])}, )  f12_dev_ilxl Out[19]: <pre>{'iline': &lt;xarray.DataArray (well: 3520)&gt; Size: 28kB\n array([10150.73832115, 10150.73832115, 10150.73832115, ...,\n        10127.96024075, 10127.95847287, 10127.95670499])\n Coordinates:\n   * well     (well) int64 28kB 0 1 2 3 4 5 6 ... 3514 3515 3516 3517 3518 3519,\n 'xline': &lt;xarray.DataArray (well: 3520)&gt; Size: 28kB\n array([2276.47797909, 2276.47797909, 2276.47797909, ..., 2247.46429975,\n        2247.40054283, 2247.33678592])\n Coordinates:\n   * well     (well) int64 28kB 0 1 2 3 4 5 6 ... 3514 3515 3516 3517 3518 3519}</pre> <p>The DataArrays with the new axes can be passed to interp which will perform interpolation for us on the new dimension <code>well</code>.</p> <p>We can also plot the well path on our seismic extracted along the well path.</p> In\u00a0[20]: Copied! <pre>sel = volve_3d.interp(**f12_dev_ilxl)\nfig, axs = plt.subplots(figsize=(20, 10))\nsel.data.T.plot(ax=axs, yincrease=False)\ntwt.plot(color=\"k\", ax=axs)\n</pre> sel = volve_3d.interp(**f12_dev_ilxl) fig, axs = plt.subplots(figsize=(20, 10)) sel.data.T.plot(ax=axs, yincrease=False) twt.plot(color=\"k\", ax=axs) Out[20]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f2d4c71a080&gt;]</pre> <p>To extract the data along the well path we just need to interpolate using the additional <code>twt</code> DataArray.</p> In\u00a0[21]: Copied! <pre>well_seismic = volve_3d.interp(**f12_dev_ilxl, samples=twt)\nwell_seismic.data.plot()\n</pre> well_seismic = volve_3d.interp(**f12_dev_ilxl, samples=twt) well_seismic.data.plot() Out[21]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f2d4c5cc940&gt;]</pre>"},{"location":"examples/example_extract_arbitrary_line.html#extract-an-arbitrary-line-from-a-3d-volume","title":"Extract an arbitrary line from a 3D volume\u00b6","text":"<p>Arbitrary lines are often defined as piecewise lines on time/z slices or basemap views that draw a path through features of interest or for example between well locations.</p> <p>By extracting an arbitrary line we hope to end up with a uniformly sampled vertical section of data that traverses the path where the sampling interval is of the order of the bin interval of the dataset.</p>"},{"location":"examples/example_extract_arbitrary_line.html#load-small-3d-volume-from-volve","title":"Load Small 3D Volume from Volve\u00b6","text":""},{"location":"examples/example_extract_arbitrary_line.html#arbitrary-lines","title":"Arbitrary Lines\u00b6","text":"<p>We define a line as lists of cdp_x &amp; cdp_y points. These can be inside or outside of the survey, but obviously should intersect it in order to be useful.</p>"},{"location":"examples/example_extract_arbitrary_line.html#petrel-shapefile","title":"Petrel Shapefile\u00b6","text":"<p>We have an arbitrary line geometry defined over this small survey region stored in a shape file exported from Petrel.</p> <p>Let's load that and extract an arbitrary line using segysak. We also have the seismic data extracted along that line by Petrel, so we can see how that compares.</p>"},{"location":"examples/example_extract_arbitrary_line.html#well-paths","title":"Well Paths\u00b6","text":"<p>Well paths can also be treated as an arbitrary line. In this example we will use the Affine transform to convert well X and Y locations to the seismic local grid, and the Xarray <code>interp</code> method to extract a seismic trace along the well bore.</p> <p>First we have to load the well bore deviation and use <code>wellpathpy</code> to convert it to XYZ coordinates with a higher sampling rate.</p>"},{"location":"examples/example_extract_data_on_a_horizon.html","title":"Extract data at the intersection of a horizon and 3D volume","text":"In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\nimport xarray as xr\n\n%matplotlib inline\n\nfrom segysak.segy import (\n    get_segy_texthead,\n    segy_header_scan,\n    segy_header_scrape,\n)\n\nfrom os import path\n</pre> import matplotlib.pyplot as plt import xarray as xr  %matplotlib inline  from segysak.segy import (     get_segy_texthead,     segy_header_scan,     segy_header_scrape, )  from os import path In\u00a0[3]: Copied! <pre>volve_3d_path = path.join(\"data\", \"volve10r12-full-twt-sub3d.sgy\")\nprint(\"3D\", volve_3d_path, path.exists(volve_3d_path))\n</pre> volve_3d_path = path.join(\"data\", \"volve10r12-full-twt-sub3d.sgy\") print(\"3D\", volve_3d_path, path.exists(volve_3d_path)) <pre>3D data/volve10r12-full-twt-sub3d.sgy True\n</pre> In\u00a0[4]: Copied! <pre>get_segy_texthead(volve_3d_path)\n</pre> get_segy_texthead(volve_3d_path) Out[4]: Text HeaderC 1 SEGY OUTPUT FROM Petrel 2017.2 Saturday, June 06 2020 10:15:00              C 2 Name: ST10010ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534 \u00ddCroC 3                                                                             C 4 First inline: 10090  Last inline: 10150                                     C 5 First xline:  2150   Last xline:  2351                                      C 6 CRS: ED50-UTM31 (\"MENTOR:ED50-UTM31:European 1950 Based UTM, Zone 31 North, C 7 X min: 433955.09 max: 436589.56 delta: 2634.47                              C 8 Y min: 6477439.46 max: 6478790.23 delta: 1350.77                            C 9 Time min: -3402.00 max: -2.00 delta: 3400.00                                C10 Lat min: 58.25'52.8804\"N max: 58.26'37.9493\"N delta: 0.00'45.0689\"          C11 Long min: 1.52'7.1906\"E max: 1.54'50.9616\"E delta: 0.02'43.7710\"            C12 Trace min: -3400.00 max: -4.00 delta: 3396.00                               C13 Seismic (template) min: -58.55 max: 54.55 delta: 113.10                     C14 Amplitude (data) min: -58.55 max: 54.55 delta: 113.10                       C15 Trace sample format: IEEE floating point                                    C16 Coordinate scale factor: 100.00000                                          C17                                                                             C18 Binary header locations:                                                    C19 Sample interval             : bytes 17-18                                   C20 Number of samples per trace : bytes 21-22                                   C21 Trace date format           : bytes 25-26                                   C22                                                                             C23 Trace header locations:                                                     C24 Inline number               : bytes 5-8                                     C25 Xline number                : bytes 21-24                                   C26 Coordinate scale factor     : bytes 71-72                                   C27 X coordinate                : bytes 73-76                                   C28 Y coordinate                : bytes 77-80                                   C29 Trace start time/depth      : bytes 109-110                                 C30 Number of samples per trace : bytes 115-116                                 C31 Sample interval             : bytes 117-118                                 C32                                                                             C33                                                                             C34                                                                             C35                                                                             C36                                                                             C37                                                                             C38                                                                             C39                                                                             C40 END EBCDIC                                                                   In\u00a0[5]: Copied! <pre>volve_3d = xr.open_dataset(\n    volve_3d_path,\n    dim_byte_fields={\"iline\": 5, \"xline\": 21},\n    extra_byte_fields={\"cdp_x\": 73, \"cdp_y\": 77},\n)\nvolve_3d.segysak.scale_coords()\nvolve_3d.data\n</pre> volve_3d = xr.open_dataset(     volve_3d_path,     dim_byte_fields={\"iline\": 5, \"xline\": 21},     extra_byte_fields={\"cdp_x\": 73, \"cdp_y\": 77}, ) volve_3d.segysak.scale_coords() volve_3d.data Out[5]: <pre>&lt;xarray.DataArray 'data' (iline: 61, xline: 202, samples: 850)&gt; Size: 42MB\n[10473700 values with dtype=float32]\nCoordinates:\n  * iline    (iline) int16 122B 10090 10091 10092 10093 ... 10148 10149 10150\n  * xline    (xline) int16 404B 2150 2151 2152 2153 2154 ... 2348 2349 2350 2351\n  * samples  (samples) float32 3kB 4.0 8.0 12.0 ... 3.392e+03 3.396e+03 3.4e+03\nAttributes:\n    seisnc:   {\"source_file\": \"data/volve10r12-full-twt-sub3d.sgy\", \"measurem...\n    text:     C 1 SEGY OUTPUT FROM Petrel 2017.2 Saturday, June 06 2020 10:15...</pre>xarray.DataArray'data'<ul><li>iline: 61</li><li>xline: 202</li><li>samples: 850</li></ul><ul><li>...<pre>[10473700 values with dtype=float32]</pre></li><li>Coordinates: (3)<ul><li>iline(iline)int1610090 10091 10092 ... 10149 10150<pre>array([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150], dtype=int16)</pre></li><li>xline(xline)int162150 2151 2152 ... 2349 2350 2351<pre>array([2150, 2151, 2152, ..., 2349, 2350, 2351], dtype=int16)</pre></li><li>samples(samples)float324.0 8.0 12.0 ... 3.396e+03 3.4e+03<pre>array([   4.,    8.,   12., ..., 3392., 3396., 3400.], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150],\n      dtype='int16', name='iline'))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159,\n       ...\n       2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351],\n      dtype='int16', name='xline', length=202))</pre></li><li>samplesPandasIndex<pre>PandasIndex(Index([   4.0,    8.0,   12.0,   16.0,   20.0,   24.0,   28.0,   32.0,   36.0,\n         40.0,\n       ...\n       3364.0, 3368.0, 3372.0, 3376.0, 3380.0, 3384.0, 3388.0, 3392.0, 3396.0,\n       3400.0],\n      dtype='float32', name='samples', length=850))</pre></li></ul></li><li>Attributes: (2)seisnc :{\"source_file\": \"data/volve10r12-full-twt-sub3d.sgy\", \"measurement_system\": \"m\", \"sample_rate\": 4.0}text :C 1 SEGY OUTPUT FROM Petrel 2017.2 Saturday, June 06 2020 10:15:00               C 2 Name: ST10010ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534 \u00ddCro C 3                                                                              C 4 First inline: 10090  Last inline: 10150                                      C 5 First xline:  2150   Last xline:  2351                                       C 6 CRS: ED50-UTM31 (\"MENTOR:ED50-UTM31:European 1950 Based UTM, Zone 31 North,  C 7 X min: 433955.09 max: 436589.56 delta: 2634.47                               C 8 Y min: 6477439.46 max: 6478790.23 delta: 1350.77                             C 9 Time min: -3402.00 max: -2.00 delta: 3400.00                                 C10 Lat min: 58.25'52.8804\"N max: 58.26'37.9493\"N delta: 0.00'45.0689\"           C11 Long min: 1.52'7.1906\"E max: 1.54'50.9616\"E delta: 0.02'43.7710\"             C12 Trace min: -3400.00 max: -4.00 delta: 3396.00                                C13 Seismic (template) min: -58.55 max: 54.55 delta: 113.10                      C14 Amplitude (data) min: -58.55 max: 54.55 delta: 113.10                        C15 Trace sample format: IEEE floating point                                     C16 Coordinate scale factor: 100.00000                                           C17                                                                              C18 Binary header locations:                                                     C19 Sample interval             : bytes 17-18                                    C20 Number of samples per trace : bytes 21-22                                    C21 Trace date format           : bytes 25-26                                    C22                                                                              C23 Trace header locations:                                                      C24 Inline number               : bytes 5-8                                      C25 Xline number                : bytes 21-24                                    C26 Coordinate scale factor     : bytes 71-72                                    C27 X coordinate                : bytes 73-76                                    C28 Y coordinate                : bytes 77-80                                    C29 Trace start time/depth      : bytes 109-110                                  C30 Number of samples per trace : bytes 115-116                                  C31 Sample interval             : bytes 117-118                                  C32                                                                              C33                                                                              C34                                                                              C35                                                                              C36                                                                              C37                                                                              C38                                                                              C39                                                                              C40 END EBCDIC                                                                  </li></ul> In\u00a0[6]: Copied! <pre>top_hugin_path = path.join(\"data\", \"hor_twt_hugin_fm_top.dat\")\nprint(\"Top Hugin\", top_hugin_path, path.exists(top_hugin_path))\n</pre> top_hugin_path = path.join(\"data\", \"hor_twt_hugin_fm_top.dat\") print(\"Top Hugin\", top_hugin_path, path.exists(top_hugin_path)) <pre>Top Hugin data/hor_twt_hugin_fm_top.dat True\n</pre> In\u00a0[7]: Copied! <pre>import pandas as pd\n\ntop_hugin_df = pd.read_csv(top_hugin_path, names=[\"cdp_x\", \"cdp_y\", \"samples\"], sep=\" \")\ntop_hugin_df.head()\n</pre> import pandas as pd  top_hugin_df = pd.read_csv(top_hugin_path, names=[\"cdp_x\", \"cdp_y\", \"samples\"], sep=\" \") top_hugin_df.head() Out[7]: cdp_x cdp_y samples 0 432186.713151 6.477029e+06 2776.275147 1 432189.737524 6.477041e+06 2779.657715 2 432192.761898 6.477053e+06 2780.465088 3 432195.786271 6.477066e+06 2780.949951 4 432198.810645 6.477078e+06 2781.769775 <p>Would be good to plot a seismic (iline,xline) section in Pyvista as well</p> In\u00a0[8]: Copied! <pre># import pyvista as pv\n\n# point_cloud = pv.PolyData(-1*top_hugin_df.to_numpy(), cmap='viridis')\n# point_cloud.plot(eye_dome_lighting=True)\n</pre> # import pyvista as pv  # point_cloud = pv.PolyData(-1*top_hugin_df.to_numpy(), cmap='viridis') # point_cloud.plot(eye_dome_lighting=True) <p>Alternativey we can use the points to output a <code>xarray.Dataset</code> which comes with coordinates for plotting already gridded up for Pyvista.</p> In\u00a0[9]: Copied! <pre>top_hugin_ds = volve_3d.seis.surface_from_points(\n    top_hugin_df, \"samples\", right=(\"cdp_x\", \"cdp_y\")\n)\ntop_hugin_ds\n</pre> top_hugin_ds = volve_3d.seis.surface_from_points(     top_hugin_df, \"samples\", right=(\"cdp_x\", \"cdp_y\") ) top_hugin_ds Out[9]: <pre>&lt;xarray.Dataset&gt; Size: 99kB\nDimensions:  (iline: 61, xline: 202)\nCoordinates:\n  * iline    (iline) int16 122B 10090 10091 10092 10093 ... 10148 10149 10150\n  * xline    (xline) int16 404B 2150 2151 2152 2153 2154 ... 2348 2349 2350 2351\n    samples  (iline, xline) float64 99kB 2.741e+03 2.742e+03 ... 2.635e+03\nData variables:\n    *empty*\nAttributes:\n    seisnc:   {\"coord_scalar\": -100.0, \"coord_scaled\": true}</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>iline: 61</li><li>xline: 202</li></ul></li><li>Coordinates: (3)<ul><li>iline(iline)int1610090 10091 10092 ... 10149 10150<pre>array([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150], dtype=int16)</pre></li><li>xline(xline)int162150 2151 2152 ... 2349 2350 2351<pre>array([2150, 2151, 2152, ..., 2349, 2350, 2351], dtype=int16)</pre></li><li>samples(iline, xline)float642.741e+03 2.742e+03 ... 2.635e+03<pre>array([[2740.59485122, 2742.24524139, 2743.93300054, ..., 2595.47726491,\n        2595.96933417, 2596.44448208],\n       [2741.6191921 , 2743.0088903 , 2744.83246656, ..., 2595.38667951,\n        2595.8743274 , 2596.34089321],\n       [2742.10649676, 2743.93585433, 2745.54148252, ..., 2595.35977674,\n        2595.85903865, 2596.33157239],\n       ...,\n       [2796.93456887, 2796.92284985, 2796.97657312, ..., 2629.01956717,\n        2630.53165788, 2632.67245508],\n       [2796.98192079, 2796.95385786, 2796.95774686, ..., 2629.63486248,\n        2631.2087454 , 2633.7785019 ],\n       [2796.99194158, 2796.92309694, 2796.89424697, ..., 2630.07582204,\n        2631.81238694, 2634.64023552]])</pre></li></ul></li><li>Data variables: (0)<ul></ul></li><li>Indexes: (2)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150],\n      dtype='int16', name='iline'))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159,\n       ...\n       2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351],\n      dtype='int16', name='xline', length=202))</pre></li></ul></li><li>Attributes: (1)seisnc :{\"coord_scalar\": -100.0, \"coord_scaled\": true}</li></ul> In\u00a0[10]: Copied! <pre># the twt values from the points now in a form we can relate to the xarray cube.\nplt.imshow(top_hugin_ds.samples)\n</pre> # the twt values from the points now in a form we can relate to the xarray cube. plt.imshow(top_hugin_ds.samples) Out[10]: <pre>&lt;matplotlib.image.AxesImage at 0x7fbb29a2f460&gt;</pre> In\u00a0[11]: Copied! <pre># point_cloud = pv.StructuredGrid(\n#     top_hugin_ds.cdp_x.values,\n#     top_hugin_ds.cdp_y.values,top_hugin_ds.twt.values,\n#     cmap='viridis')\n# point_cloud.plot(eye_dome_lighting=True)\n</pre> # point_cloud = pv.StructuredGrid( #     top_hugin_ds.cdp_x.values, #     top_hugin_ds.cdp_y.values,top_hugin_ds.twt.values, #     cmap='viridis') # point_cloud.plot(eye_dome_lighting=True) <p>Extracting horizon amplitudes requires us to interpolate the cube onto the 3D horizon.</p> In\u00a0[12]: Copied! <pre>top_hugin_amp = volve_3d.data.interp(\n    {\"iline\": top_hugin_ds.iline, \"xline\": top_hugin_ds.xline, \"samples\": top_hugin_ds.samples}\n)\n</pre> top_hugin_amp = volve_3d.data.interp(     {\"iline\": top_hugin_ds.iline, \"xline\": top_hugin_ds.xline, \"samples\": top_hugin_ds.samples} ) In\u00a0[13]: Copied! <pre>#\nfig = plt.figure(figsize=(15, 5))\ntop_hugin_amp.plot(cmap=\"bwr\")\ncs = plt.contour(\n    top_hugin_amp.xline, top_hugin_amp.iline, top_hugin_ds.samples, levels=20, colors=\"grey\"\n)\nplt.clabel(cs, fontsize=14, fmt=\"%.1f\")\n</pre> # fig = plt.figure(figsize=(15, 5)) top_hugin_amp.plot(cmap=\"bwr\") cs = plt.contour(     top_hugin_amp.xline, top_hugin_amp.iline, top_hugin_ds.samples, levels=20, colors=\"grey\" ) plt.clabel(cs, fontsize=14, fmt=\"%.1f\") Out[13]: <pre>&lt;a list of 24 text.Text objects&gt;</pre>"},{"location":"examples/example_extract_data_on_a_horizon.html#extract-data-at-the-intersection-of-a-horizon-and-3d-volume","title":"Extract data at the intersection of a horizon and 3D volume\u00b6","text":""},{"location":"examples/example_extract_data_on_a_horizon.html#load-small-3d-volume-from-volve","title":"Load Small 3D Volume from Volve\u00b6","text":""},{"location":"examples/example_extract_data_on_a_horizon.html#load-up-horizon-data","title":"Load up horizon data\u00b6","text":""},{"location":"examples/example_extract_data_on_a_horizon.html#horizon-amplitude-extraction","title":"Horizon Amplitude Extraction\u00b6","text":""},{"location":"examples/example_merge_surveys.html","title":"Merging Seismic Data Cubes","text":"<p>Often we receive seismic data cubes which we wish to merge that have different geometries. This workflow will guide you through the process of merging two such cubes.</p> In\u00a0[2]: Copied! <pre>from segysak import create3d_dataset\nfrom scipy.optimize import curve_fit\nimport numpy as np\nimport xarray as xr\nimport matplotlib.pyplot as plt\nfrom matplotlib.transforms import Affine2D\nfrom segysak import open_seisnc, create_seismic_dataset\n\nfrom dask.distributed import Client\n\n# start a dask client for processing\nclient = Client()\n</pre> from segysak import create3d_dataset from scipy.optimize import curve_fit import numpy as np import xarray as xr import matplotlib.pyplot as plt from matplotlib.transforms import Affine2D from segysak import open_seisnc, create_seismic_dataset  from dask.distributed import Client  # start a dask client for processing client = Client() <p>Creating some synthetic seismic surveys for us to play with. The online documentation uses a smaller example, but you can stress dask by commenting the dimension lines for smaller data and uncommenting the large desktop size volumes.</p> In\u00a0[3]: Copied! <pre># large desktop size merge\n# i1, i2, iN = 100, 750, 651\n# x1, x2, xN = 300, 1250, 951\n# t1, t2, tN = 0, 1848, 463\n# trans_x, trans_y = 10000, 2000\n\n# online example size merge\ni1, i2, iN = 650, 750, 101\nx1, x2, xN = 1150, 1250, 101\nt1, t2, tN = 0, 200, 201\ntrans_x, trans_y = 9350, 1680\n\niline = np.linspace(i1, i2, iN, dtype=int)\nxline = np.linspace(x1, x2, xN, dtype=int)\ntwt = np.linspace(t1, t2, tN, dtype=int)\n\naffine_survey1 = Affine2D()\naffine_survey1.rotate_deg(20).translate(trans_x, trans_y)\n\n# create an empty dataset and empty coordinates\nsurvey_1 = create_seismic_dataset(twt=twt, iline=iline, xline=xline)\nsurvey_1[\"cdp_x\"] = ((\"iline\", \"xline\"), np.empty((iN, xN)))\nsurvey_1[\"cdp_y\"] = ((\"iline\", \"xline\"), np.empty((iN, xN)))\n\nstacked_iline = survey_1.iline.broadcast_like(survey_1.cdp_x).stack(\n    {\"ravel\": (..., \"xline\")}\n)\nstacked_xline = survey_1.xline.broadcast_like(survey_1.cdp_x).stack(\n    {\"ravel\": (..., \"xline\")}\n)\n\npoints = np.dstack([stacked_iline, stacked_xline])\npoints_xy = affine_survey1.transform(points[0])\n# put xy back in stacked and unstack into survey_1\nsurvey_1[\"cdp_x\"] = stacked_iline.copy(data=points_xy[:, 0]).unstack()\nsurvey_1[\"cdp_y\"] = stacked_iline.copy(data=points_xy[:, 1]).unstack()\n\n# and the same for survey_2 but with a different affine to survey 1\naffine_survey2 = Affine2D()\naffine_survey2.rotate_deg(120).translate(11000, 3000)\n\nsurvey_2 = create_seismic_dataset(twt=twt, iline=iline, xline=xline)\nsurvey_2[\"cdp_x\"] = ((\"iline\", \"xline\"), np.empty((iN, xN)))\nsurvey_2[\"cdp_y\"] = ((\"iline\", \"xline\"), np.empty((iN, xN)))\n\nstacked_iline = survey_1.iline.broadcast_like(survey_1.cdp_x).stack(\n    {\"ravel\": (..., \"xline\")}\n)\nstacked_xline = survey_1.xline.broadcast_like(survey_1.cdp_x).stack(\n    {\"ravel\": (..., \"xline\")}\n)\n\npoints = np.dstack([stacked_iline, stacked_xline])\npoints_xy = affine_survey2.transform(points[0])\n# put xy back in stacked and unstack into survey_1\nsurvey_2[\"cdp_x\"] = stacked_iline.copy(data=points_xy[:, 0]).unstack()\nsurvey_2[\"cdp_y\"] = stacked_iline.copy(data=points_xy[:, 1]).unstack()\n</pre> # large desktop size merge # i1, i2, iN = 100, 750, 651 # x1, x2, xN = 300, 1250, 951 # t1, t2, tN = 0, 1848, 463 # trans_x, trans_y = 10000, 2000  # online example size merge i1, i2, iN = 650, 750, 101 x1, x2, xN = 1150, 1250, 101 t1, t2, tN = 0, 200, 201 trans_x, trans_y = 9350, 1680  iline = np.linspace(i1, i2, iN, dtype=int) xline = np.linspace(x1, x2, xN, dtype=int) twt = np.linspace(t1, t2, tN, dtype=int)  affine_survey1 = Affine2D() affine_survey1.rotate_deg(20).translate(trans_x, trans_y)  # create an empty dataset and empty coordinates survey_1 = create_seismic_dataset(twt=twt, iline=iline, xline=xline) survey_1[\"cdp_x\"] = ((\"iline\", \"xline\"), np.empty((iN, xN))) survey_1[\"cdp_y\"] = ((\"iline\", \"xline\"), np.empty((iN, xN)))  stacked_iline = survey_1.iline.broadcast_like(survey_1.cdp_x).stack(     {\"ravel\": (..., \"xline\")} ) stacked_xline = survey_1.xline.broadcast_like(survey_1.cdp_x).stack(     {\"ravel\": (..., \"xline\")} )  points = np.dstack([stacked_iline, stacked_xline]) points_xy = affine_survey1.transform(points[0]) # put xy back in stacked and unstack into survey_1 survey_1[\"cdp_x\"] = stacked_iline.copy(data=points_xy[:, 0]).unstack() survey_1[\"cdp_y\"] = stacked_iline.copy(data=points_xy[:, 1]).unstack()  # and the same for survey_2 but with a different affine to survey 1 affine_survey2 = Affine2D() affine_survey2.rotate_deg(120).translate(11000, 3000)  survey_2 = create_seismic_dataset(twt=twt, iline=iline, xline=xline) survey_2[\"cdp_x\"] = ((\"iline\", \"xline\"), np.empty((iN, xN))) survey_2[\"cdp_y\"] = ((\"iline\", \"xline\"), np.empty((iN, xN)))  stacked_iline = survey_1.iline.broadcast_like(survey_1.cdp_x).stack(     {\"ravel\": (..., \"xline\")} ) stacked_xline = survey_1.xline.broadcast_like(survey_1.cdp_x).stack(     {\"ravel\": (..., \"xline\")} )  points = np.dstack([stacked_iline, stacked_xline]) points_xy = affine_survey2.transform(points[0]) # put xy back in stacked and unstack into survey_1 survey_2[\"cdp_x\"] = stacked_iline.copy(data=points_xy[:, 0]).unstack() survey_2[\"cdp_y\"] = stacked_iline.copy(data=points_xy[:, 1]).unstack() <p>Let's check that there is a bit of overlap between the two surveys.</p> In\u00a0[4]: Copied! <pre># plotting every 10th line\n\nplt.figure(figsize=(10, 10))\nsurvey_1_plot = plt.plot(\n    survey_1.cdp_x.values[::10, ::10], survey_1.cdp_y.values[::10, ::10], color=\"grey\"\n)\nsurvey_2_plot = plt.plot(\n    survey_2.cdp_x.values[::10, ::10], survey_2.cdp_y.values[::10, ::10], color=\"blue\"\n)\n# plt.aspect(\"equal\")\nplt.legend([survey_1_plot[0], survey_2_plot[0]], [\"survey_1\", \"survey_2\"])\n</pre> # plotting every 10th line  plt.figure(figsize=(10, 10)) survey_1_plot = plt.plot(     survey_1.cdp_x.values[::10, ::10], survey_1.cdp_y.values[::10, ::10], color=\"grey\" ) survey_2_plot = plt.plot(     survey_2.cdp_x.values[::10, ::10], survey_2.cdp_y.values[::10, ::10], color=\"blue\" ) # plt.aspect(\"equal\") plt.legend([survey_1_plot[0], survey_2_plot[0]], [\"survey_1\", \"survey_2\"]) Out[4]: <pre>&lt;matplotlib.legend.Legend at 0x7f75e4ee2650&gt;</pre> <p>Let's output these two datasets to disk so we can use lazy loading from seisnc. If the files are large, it's good practice to do this, because you will probably run out of memory.</p> <p>We are going to fill survey one with a values of 1 and survey 2 with a value of 2, just for simplicity in this example. We also tell the dtype to be <code>np.float32</code> to reduce memory usage.</p> In\u00a0[5]: Copied! <pre># save out with new geometry\nsurvey_1[\"data\"] = (\n    (\"iline\", \"xline\", \"twt\"),\n    np.full((iN, xN, tN), 1, dtype=np.float32),\n)\nsurvey_1.seisio.to_netcdf(\"data/survey_1.seisnc\")\ndel survey_1\nsurvey_2[\"data\"] = (\n    (\"iline\", \"xline\", \"twt\"),\n    np.full((iN, xN, tN), 2, dtype=np.float32),\n)\nsurvey_2.seisio.to_netcdf(\"data/survey_2.seisnc\")\ndel survey_2\n</pre> # save out with new geometry survey_1[\"data\"] = (     (\"iline\", \"xline\", \"twt\"),     np.full((iN, xN, tN), 1, dtype=np.float32), ) survey_1.seisio.to_netcdf(\"data/survey_1.seisnc\") del survey_1 survey_2[\"data\"] = (     (\"iline\", \"xline\", \"twt\"),     np.full((iN, xN, tN), 2, dtype=np.float32), ) survey_2.seisio.to_netcdf(\"data/survey_2.seisnc\") del survey_2 <p>Let us reimport the surveys we created but in a chunked (lazy) way.</p> In\u00a0[6]: Copied! <pre>survey_1 = open_seisnc(\"data/survey_1.seisnc\", chunks=dict(iline=10, xline=10, twt=100))\nsurvey_2 = open_seisnc(\"data/survey_2.seisnc\", chunks=dict(iline=10, xline=10, twt=100))\n</pre> survey_1 = open_seisnc(\"data/survey_1.seisnc\", chunks=dict(iline=10, xline=10, twt=100)) survey_2 = open_seisnc(\"data/survey_2.seisnc\", chunks=dict(iline=10, xline=10, twt=100)) <pre>/tmp/ipykernel_1997/814101917.py:1: DeprecationWarning: open_seisnc will be removed in v0.6, please use the Xarray engine ds = xr.open_dataset(netcdf_file) method instead.\n  survey_1 = open_seisnc(\"data/survey_1.seisnc\", chunks=dict(iline=10, xline=10, twt=100))\n/tmp/ipykernel_1997/814101917.py:2: DeprecationWarning: open_seisnc will be removed in v0.6, please use the Xarray engine ds = xr.open_dataset(netcdf_file) method instead.\n  survey_2 = open_seisnc(\"data/survey_2.seisnc\", chunks=dict(iline=10, xline=10, twt=100))\n</pre> <p>Check that the survey is chunked by looking at the printout for our datasets. Lazy and chunked data will have a <code>dask.array&lt;chunksize=</code> where the values are usually displayed.</p> In\u00a0[7]: Copied! <pre>survey_1\n</pre> survey_1 Out[7]: <pre>&lt;xarray.Dataset&gt; Size: 8MB\nDimensions:  (iline: 101, xline: 101, twt: 201)\nCoordinates:\n  * iline    (iline) int64 808B 650 651 652 653 654 655 ... 746 747 748 749 750\n  * xline    (xline) int64 808B 1150 1151 1152 1153 1154 ... 1247 1248 1249 1250\n  * twt      (twt) int64 2kB 0 1 2 3 4 5 6 7 ... 193 194 195 196 197 198 199 200\n    cdp_x    (iline, xline) float64 82kB dask.array&lt;chunksize=(10, 10), meta=np.ndarray&gt;\n    cdp_y    (iline, xline) float64 82kB dask.array&lt;chunksize=(10, 10), meta=np.ndarray&gt;\nData variables:\n    data     (iline, xline, twt) float32 8MB dask.array&lt;chunksize=(10, 10, 100), meta=np.ndarray&gt;\nAttributes: (12/17)\n    text:                \n    ns:                  None\n    sample_rate:         None\n    measurement_system:  None\n    d3_domain:           None\n    epsg:                None\n    ...                  ...\n    percentiles:         None\n    coord_scalar:        None\n    coord_scaled:        None\n    dimensions:          None\n    vert_dimension:      None\n    vert_domain:         None</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>iline: 101</li><li>xline: 101</li><li>twt: 201</li></ul></li><li>Coordinates: (5)<ul><li>iline(iline)int64650 651 652 653 ... 747 748 749 750<pre>array([650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663,\n       664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677,\n       678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691,\n       692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705,\n       706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719,\n       720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733,\n       734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747,\n       748, 749, 750])</pre></li><li>xline(xline)int641150 1151 1152 ... 1248 1249 1250<pre>array([1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161,\n       1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173,\n       1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n       1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197,\n       1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209,\n       1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221,\n       1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233,\n       1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245,\n       1246, 1247, 1248, 1249, 1250])</pre></li><li>twt(twt)int640 1 2 3 4 5 ... 196 197 198 199 200<pre>array([  0,   1,   2, ..., 198, 199, 200])</pre></li><li>cdp_x(iline, xline)float64dask.array&lt;chunksize=(10, 10), meta=np.ndarray&gt;  Array   Chunk   Bytes   79.70 kiB   800 B   Shape   (101, 101)   (10, 10)   Dask graph   121 chunks in 2 graph layers   Data type   float64 numpy.ndarray  101 101 </li><li>cdp_y(iline, xline)float64dask.array&lt;chunksize=(10, 10), meta=np.ndarray&gt;  Array   Chunk   Bytes   79.70 kiB   800 B   Shape   (101, 101)   (10, 10)   Dask graph   121 chunks in 2 graph layers   Data type   float64 numpy.ndarray  101 101 </li></ul></li><li>Data variables: (1)<ul><li>data(iline, xline, twt)float32dask.array&lt;chunksize=(10, 10, 100), meta=np.ndarray&gt;  Array   Chunk   Bytes   7.82 MiB   39.06 kiB   Shape   (101, 101, 201)   (10, 10, 100)   Dask graph   363 chunks in 2 graph layers   Data type   float32 numpy.ndarray  201 101 101 </li></ul></li><li>Indexes: (3)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([650, 651, 652, 653, 654, 655, 656, 657, 658, 659,\n       ...\n       741, 742, 743, 744, 745, 746, 747, 748, 749, 750],\n      dtype='int64', name='iline', length=101))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159,\n       ...\n       1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250],\n      dtype='int64', name='xline', length=101))</pre></li><li>twtPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       191, 192, 193, 194, 195, 196, 197, 198, 199, 200],\n      dtype='int64', name='twt', length=201))</pre></li></ul></li><li>Attributes: (17)text :ns :Nonesample_rate :Nonemeasurement_system :Noned3_domain :Noneepsg :Nonecorner_points :Nonecorner_points_xy :Nonesource_file :Nonesrd :Nonedatatype :Nonepercentiles :Nonecoord_scalar :Nonecoord_scaled :Nonedimensions :Nonevert_dimension :Nonevert_domain :None</li></ul> <p>It will help with our other functions if we load the cdp_x and cdp_y locations into memory. This can be done using the <code>load()</code> method.</p> In\u00a0[8]: Copied! <pre>survey_1[\"cdp_x\"] = survey_1[\"cdp_x\"].load()\nsurvey_1[\"cdp_y\"] = survey_1[\"cdp_y\"].load()\nsurvey_2[\"cdp_x\"] = survey_2[\"cdp_x\"].load()\nsurvey_2[\"cdp_y\"] = survey_2[\"cdp_y\"].load()\n</pre> survey_1[\"cdp_x\"] = survey_1[\"cdp_x\"].load() survey_1[\"cdp_y\"] = survey_1[\"cdp_y\"].load() survey_2[\"cdp_x\"] = survey_2[\"cdp_x\"].load() survey_2[\"cdp_y\"] = survey_2[\"cdp_y\"].load() In\u00a0[9]: Copied! <pre>survey_1\n</pre> survey_1 Out[9]: <pre>&lt;xarray.Dataset&gt; Size: 8MB\nDimensions:  (iline: 101, xline: 101, twt: 201)\nCoordinates:\n  * iline    (iline) int64 808B 650 651 652 653 654 655 ... 746 747 748 749 750\n  * xline    (xline) int64 808B 1150 1151 1152 1153 1154 ... 1247 1248 1249 1250\n  * twt      (twt) int64 2kB 0 1 2 3 4 5 6 7 ... 193 194 195 196 197 198 199 200\n    cdp_x    (iline, xline) float64 82kB 9.567e+03 9.567e+03 ... 9.627e+03\n    cdp_y    (iline, xline) float64 82kB 2.983e+03 2.984e+03 ... 3.111e+03\nData variables:\n    data     (iline, xline, twt) float32 8MB dask.array&lt;chunksize=(10, 10, 100), meta=np.ndarray&gt;\nAttributes: (12/17)\n    text:                \n    ns:                  None\n    sample_rate:         None\n    measurement_system:  None\n    d3_domain:           None\n    epsg:                None\n    ...                  ...\n    percentiles:         None\n    coord_scalar:        None\n    coord_scaled:        None\n    dimensions:          None\n    vert_dimension:      None\n    vert_domain:         None</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>iline: 101</li><li>xline: 101</li><li>twt: 201</li></ul></li><li>Coordinates: (5)<ul><li>iline(iline)int64650 651 652 653 ... 747 748 749 750<pre>array([650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663,\n       664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677,\n       678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691,\n       692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705,\n       706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719,\n       720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733,\n       734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747,\n       748, 749, 750])</pre></li><li>xline(xline)int641150 1151 1152 ... 1248 1249 1250<pre>array([1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161,\n       1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173,\n       1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n       1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197,\n       1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209,\n       1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221,\n       1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233,\n       1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245,\n       1246, 1247, 1248, 1249, 1250])</pre></li><li>twt(twt)int640 1 2 3 4 5 ... 196 197 198 199 200<pre>array([  0,   1,   2, ..., 198, 199, 200])</pre></li><li>cdp_x(iline, xline)float649.567e+03 9.567e+03 ... 9.627e+03<pre>array([[9567.47703869, 9567.13501854, 9566.7929984 , ..., 9533.95906464,\n        9533.6170445 , 9533.27502435],\n       [9568.41673131, 9568.07471116, 9567.73269102, ..., 9534.89875726,\n        9534.55673712, 9534.21471697],\n       [9569.35642393, 9569.01440378, 9568.67238364, ..., 9535.83844988,\n        9535.49642974, 9535.1544096 ],\n       ...,\n       [9659.56691552, 9659.22489538, 9658.88287524, ..., 9626.04894148,\n        9625.70692133, 9625.36490119],\n       [9660.50660814, 9660.164588  , 9659.82256786, ..., 9626.9886341 ,\n        9626.64661395, 9626.30459381],\n       [9661.44630076, 9661.10428062, 9660.76226048, ..., 9627.92832672,\n        9627.58630658, 9627.24428643]])</pre></li><li>cdp_y(iline, xline)float642.983e+03 2.984e+03 ... 3.111e+03<pre>array([[2982.95960707, 2983.89929969, 2984.83899231, ..., 3075.0494839 ,\n        3075.98917652, 3076.92886914],\n       [2983.30162721, 2984.24131983, 2985.18101245, ..., 3075.39150405,\n        3076.33119667, 3077.27088929],\n       [2983.64364735, 2984.58333997, 2985.52303259, ..., 3075.73352419,\n        3076.67321681, 3077.61290943],\n       ...,\n       [3016.47758111, 3017.41727373, 3018.35696635, ..., 3108.56745795,\n        3109.50715057, 3110.44684319],\n       [3016.81960125, 3017.75929388, 3018.6989865 , ..., 3108.90947809,\n        3109.84917071, 3110.78886333],\n       [3017.1616214 , 3018.10131402, 3019.04100664, ..., 3109.25149824,\n        3110.19119086, 3111.13088348]])</pre></li></ul></li><li>Data variables: (1)<ul><li>data(iline, xline, twt)float32dask.array&lt;chunksize=(10, 10, 100), meta=np.ndarray&gt;  Array   Chunk   Bytes   7.82 MiB   39.06 kiB   Shape   (101, 101, 201)   (10, 10, 100)   Dask graph   363 chunks in 2 graph layers   Data type   float32 numpy.ndarray  201 101 101 </li></ul></li><li>Indexes: (3)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([650, 651, 652, 653, 654, 655, 656, 657, 658, 659,\n       ...\n       741, 742, 743, 744, 745, 746, 747, 748, 749, 750],\n      dtype='int64', name='iline', length=101))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159,\n       ...\n       1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250],\n      dtype='int64', name='xline', length=101))</pre></li><li>twtPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       191, 192, 193, 194, 195, 196, 197, 198, 199, 200],\n      dtype='int64', name='twt', length=201))</pre></li></ul></li><li>Attributes: (17)text :ns :Nonesample_rate :Nonemeasurement_system :Noned3_domain :Noneepsg :Nonecorner_points :Nonecorner_points_xy :Nonesource_file :Nonesrd :Nonedatatype :Nonepercentiles :Nonecoord_scalar :Nonecoord_scaled :Nonedimensions :Nonevert_dimension :Nonevert_domain :None</li></ul> <p>We will also need an affine transform which converts from il/xl on survey 2 to il/xl on survey 1. This will relate the two grids to each other. Fortunately affine transforms can be added together which makes this pretty straight forward.</p> In\u00a0[10]: Copied! <pre>ilxl2_to_ilxl1 = (\n    survey_2.seis.get_affine_transform()\n    + survey_1.seis.get_affine_transform().inverted()\n)\n</pre> ilxl2_to_ilxl1 = (     survey_2.seis.get_affine_transform()     + survey_1.seis.get_affine_transform().inverted() ) <p>Let's check the transform to see if it makes sense. The colouring of our values by inline should be the same after the transform is applied. Cool.</p> In\u00a0[11]: Copied! <pre># plotting every 50th line\n\nn = 5\n\nfig, axs = plt.subplots(ncols=2, figsize=(20, 10))\nsurvey_1_plot = axs[0].scatter(\n    survey_1.cdp_x.values[::n, ::n],\n    survey_1.cdp_y.values[::n, ::n],\n    c=survey_1.iline.broadcast_like(survey_1.cdp_x).values[::n, ::n],\n    s=20,\n)\nsurvey_2_plot = axs[0].scatter(\n    survey_2.cdp_x.values[::n, ::n],\n    survey_2.cdp_y.values[::n, ::n],\n    c=survey_2.iline.broadcast_like(survey_2.cdp_x).values[::n, ::n],\n    s=20,\n)\nplt.colorbar(survey_1_plot, ax=axs[0])\naxs[0].set_aspect(\"equal\")\n\naxs[0].set_title(\"inline numbering before transform\")\n\nnew_ilxl = ilxl2_to_ilxl1.transform(\n    np.dstack(\n        [\n            survey_2.iline.broadcast_like(survey_2.cdp_x).values.ravel(),\n            survey_2.xline.broadcast_like(survey_2.cdp_x).values.ravel(),\n        ]\n    )[0]\n).reshape((survey_2.iline.size, survey_2.xline.size, 2))\n\nsurvey_2.seis.calc_corner_points()\ns2_corner_points_in_s1 = ilxl2_to_ilxl1.transform(survey_2.attrs[\"corner_points\"])\n\nprint(\"Min Iline:\", s2_corner_points_in_s1[:, 0].min(), survey_1.iline.min().values)\nmin_iline_combined = min(\n    s2_corner_points_in_s1[:, 0].min(), survey_1.iline.min().values\n)\nprint(\"Max Iline:\", s2_corner_points_in_s1[:, 0].max(), survey_1.iline.max().values)\nmax_iline_combined = max(\n    s2_corner_points_in_s1[:, 0].max(), survey_1.iline.max().values\n)\n\nprint(\"Min Xline:\", s2_corner_points_in_s1[:, 1].min(), survey_1.xline.min().values)\nmin_xline_combined = min(\n    s2_corner_points_in_s1[:, 1].min(), survey_1.xline.min().values\n)\nprint(\"Max Xline:\", s2_corner_points_in_s1[:, 1].max(), survey_1.xline.max().values)\nmax_xline_combined = max(\n    s2_corner_points_in_s1[:, 1].max(), survey_1.xline.max().values\n)\nprint(min_iline_combined, max_iline_combined, min_xline_combined, max_xline_combined)\n\nsurvey_1_plot = axs[1].scatter(\n    survey_1.cdp_x.values[::n, ::n],\n    survey_1.cdp_y.values[::n, ::n],\n    c=survey_1.iline.broadcast_like(survey_1.cdp_x).values[::n, ::n],\n    vmin=min_iline_combined,\n    vmax=max_iline_combined,\n    s=20,\n    cmap=\"hsv\",\n)\n\n\nsurvey_2_plot = axs[1].scatter(\n    survey_2.cdp_x.values[::n, ::n],\n    survey_2.cdp_y.values[::n, ::n],\n    c=new_ilxl[::n, ::n, 0],\n    vmin=min_iline_combined,\n    vmax=max_iline_combined,\n    s=20,\n    cmap=\"hsv\",\n)\nplt.colorbar(survey_1_plot, ax=axs[1])\n\naxs[1].set_title(\"inline numbering after transform\")\naxs[1].set_aspect(\"equal\")\n</pre> # plotting every 50th line  n = 5  fig, axs = plt.subplots(ncols=2, figsize=(20, 10)) survey_1_plot = axs[0].scatter(     survey_1.cdp_x.values[::n, ::n],     survey_1.cdp_y.values[::n, ::n],     c=survey_1.iline.broadcast_like(survey_1.cdp_x).values[::n, ::n],     s=20, ) survey_2_plot = axs[0].scatter(     survey_2.cdp_x.values[::n, ::n],     survey_2.cdp_y.values[::n, ::n],     c=survey_2.iline.broadcast_like(survey_2.cdp_x).values[::n, ::n],     s=20, ) plt.colorbar(survey_1_plot, ax=axs[0]) axs[0].set_aspect(\"equal\")  axs[0].set_title(\"inline numbering before transform\")  new_ilxl = ilxl2_to_ilxl1.transform(     np.dstack(         [             survey_2.iline.broadcast_like(survey_2.cdp_x).values.ravel(),             survey_2.xline.broadcast_like(survey_2.cdp_x).values.ravel(),         ]     )[0] ).reshape((survey_2.iline.size, survey_2.xline.size, 2))  survey_2.seis.calc_corner_points() s2_corner_points_in_s1 = ilxl2_to_ilxl1.transform(survey_2.attrs[\"corner_points\"])  print(\"Min Iline:\", s2_corner_points_in_s1[:, 0].min(), survey_1.iline.min().values) min_iline_combined = min(     s2_corner_points_in_s1[:, 0].min(), survey_1.iline.min().values ) print(\"Max Iline:\", s2_corner_points_in_s1[:, 0].max(), survey_1.iline.max().values) max_iline_combined = max(     s2_corner_points_in_s1[:, 0].max(), survey_1.iline.max().values )  print(\"Min Xline:\", s2_corner_points_in_s1[:, 1].min(), survey_1.xline.min().values) min_xline_combined = min(     s2_corner_points_in_s1[:, 1].min(), survey_1.xline.min().values ) print(\"Max Xline:\", s2_corner_points_in_s1[:, 1].max(), survey_1.xline.max().values) max_xline_combined = max(     s2_corner_points_in_s1[:, 1].max(), survey_1.xline.max().values ) print(min_iline_combined, max_iline_combined, min_xline_combined, max_xline_combined)  survey_1_plot = axs[1].scatter(     survey_1.cdp_x.values[::n, ::n],     survey_1.cdp_y.values[::n, ::n],     c=survey_1.iline.broadcast_like(survey_1.cdp_x).values[::n, ::n],     vmin=min_iline_combined,     vmax=max_iline_combined,     s=20,     cmap=\"hsv\", )   survey_2_plot = axs[1].scatter(     survey_2.cdp_x.values[::n, ::n],     survey_2.cdp_y.values[::n, ::n],     c=new_ilxl[::n, ::n, 0],     vmin=min_iline_combined,     vmax=max_iline_combined,     s=20,     cmap=\"hsv\", ) plt.colorbar(survey_1_plot, ax=axs[1])  axs[1].set_title(\"inline numbering after transform\") axs[1].set_aspect(\"equal\") <pre>Min Iline: 640.7135889711899 650\nMax Iline: 756.5591820391251 750\nMin Xline: 1099.1258403243169 1150\nMax Xline: 1214.9714333922225 1250\n640.7135889711899 756.5591820391251 1099.1258403243169 1250\n</pre> <p>So we now have a transform that can convert ilxl from <code>survey_2</code> to ilxl from <code>survey_1</code> and we need to create a combined geometry for the merge. The dims need to cover the maximum range (inline, crossline) of two surveys. We are also going to use survey 1 as the base as we don't want to have to resample two cubes, although if you have a prefered geometry you could do that (custom affine transforms per survey required).</p> In\u00a0[12]: Copied! <pre># create new dims\niline_step = survey_1.iline.diff(\"iline\").mean().values\nxline_step = survey_1.xline.diff(\"xline\").mean().values\n\n# create the new dimensions\nnew_iline_dim = np.arange(\n    int(min_iline_combined // iline_step) * iline_step,\n    int(max_iline_combined) + iline_step * 2,\n    iline_step,\n    dtype=np.int32,\n)\nnew_xline_dim = np.arange(\n    int(min_xline_combined // xline_step) * xline_step,\n    int(max_xline_combined) + xline_step * 2,\n    xline_step,\n    dtype=np.int32,\n)\n\n# create a new empty dataset and a blank cdp_x for dims broadcasting\nsurvey_comb = create_seismic_dataset(\n    iline=new_iline_dim, xline=new_xline_dim, twt=survey_1.twt\n)\nsurvey_comb[\"cdp_x\"] = (\n    (\"iline\", \"xline\"),\n    np.empty((new_iline_dim.size, new_xline_dim.size)),\n)\n\n# calculate the x and y using the survey 1 affine which is our base grid and reshape to the dataset grid\nnew_cdp_xy = affine_survey1.transform(\n    np.dstack(\n        [\n            survey_comb.iline.broadcast_like(survey_comb.cdp_x).values.ravel(),\n            survey_comb.xline.broadcast_like(survey_comb.cdp_x).values.ravel(),\n        ]\n    )[0]\n)\nnew_cdp_xy_grid = new_cdp_xy.reshape((new_iline_dim.size, new_xline_dim.size, 2))\n\n# plot to check\nplt.figure(figsize=(10, 10))\n\nsurvey_comb_plot = plt.plot(\n    new_cdp_xy_grid[:, :, 0], new_cdp_xy_grid[:, :, 1], color=\"red\", alpha=0.5\n)\nsurvey_1_plot = plt.plot(\n    survey_1.cdp_x.values[::10, ::10], survey_1.cdp_y.values[::10, ::10], color=\"grey\"\n)\nsurvey_2_plot = plt.plot(\n    survey_2.cdp_x.values[::10, ::10], survey_2.cdp_y.values[::10, ::10], color=\"blue\"\n)\n\nplt.legend(\n    [survey_1_plot[0], survey_2_plot[0], survey_comb_plot[0]],\n    [\"survey_1\", \"survey_2\", \"survey_merged\"],\n)\n\n# put the new x and y in the empty dataset\nsurvey_comb[\"cdp_x\"] = ((\"iline\", \"xline\"), new_cdp_xy_grid[..., 0])\nsurvey_comb[\"cdp_y\"] = ((\"iline\", \"xline\"), new_cdp_xy_grid[..., 1])\nsurvey_comb = survey_comb.set_coords((\"cdp_x\", \"cdp_y\"))\n</pre> # create new dims iline_step = survey_1.iline.diff(\"iline\").mean().values xline_step = survey_1.xline.diff(\"xline\").mean().values  # create the new dimensions new_iline_dim = np.arange(     int(min_iline_combined // iline_step) * iline_step,     int(max_iline_combined) + iline_step * 2,     iline_step,     dtype=np.int32, ) new_xline_dim = np.arange(     int(min_xline_combined // xline_step) * xline_step,     int(max_xline_combined) + xline_step * 2,     xline_step,     dtype=np.int32, )  # create a new empty dataset and a blank cdp_x for dims broadcasting survey_comb = create_seismic_dataset(     iline=new_iline_dim, xline=new_xline_dim, twt=survey_1.twt ) survey_comb[\"cdp_x\"] = (     (\"iline\", \"xline\"),     np.empty((new_iline_dim.size, new_xline_dim.size)), )  # calculate the x and y using the survey 1 affine which is our base grid and reshape to the dataset grid new_cdp_xy = affine_survey1.transform(     np.dstack(         [             survey_comb.iline.broadcast_like(survey_comb.cdp_x).values.ravel(),             survey_comb.xline.broadcast_like(survey_comb.cdp_x).values.ravel(),         ]     )[0] ) new_cdp_xy_grid = new_cdp_xy.reshape((new_iline_dim.size, new_xline_dim.size, 2))  # plot to check plt.figure(figsize=(10, 10))  survey_comb_plot = plt.plot(     new_cdp_xy_grid[:, :, 0], new_cdp_xy_grid[:, :, 1], color=\"red\", alpha=0.5 ) survey_1_plot = plt.plot(     survey_1.cdp_x.values[::10, ::10], survey_1.cdp_y.values[::10, ::10], color=\"grey\" ) survey_2_plot = plt.plot(     survey_2.cdp_x.values[::10, ::10], survey_2.cdp_y.values[::10, ::10], color=\"blue\" )  plt.legend(     [survey_1_plot[0], survey_2_plot[0], survey_comb_plot[0]],     [\"survey_1\", \"survey_2\", \"survey_merged\"], )  # put the new x and y in the empty dataset survey_comb[\"cdp_x\"] = ((\"iline\", \"xline\"), new_cdp_xy_grid[..., 0]) survey_comb[\"cdp_y\"] = ((\"iline\", \"xline\"), new_cdp_xy_grid[..., 1]) survey_comb = survey_comb.set_coords((\"cdp_x\", \"cdp_y\")) In\u00a0[13]: Copied! <pre>survey_2_new_ilxl_loc = affine_survey2.inverted().transform(new_cdp_xy)\n</pre> survey_2_new_ilxl_loc = affine_survey2.inverted().transform(new_cdp_xy) <p>We then need to create a sampling DataArray. If we passed the iline and cross line locations from the previous cell unfortunately Xarray would broadcast the inline and xline values against each other. The simplest way is to create a new stacked flat dimension which we can unstack into a cube later.</p> <p>We also need to give it unique names so xarray doesn't confuse the new dimensions with the old dimensions.</p> In\u00a0[14]: Copied! <pre>flat = (\n    survey_comb.rename(  # use the combined survey\n        {\"iline\": \"new_iline\", \"xline\": \"new_xline\"}\n    )  # renaming inline and xline so they don't confuse xarray\n    .stack(\n        {\"flat\": (\"new_iline\", \"new_xline\")}\n    )  # and flatten the iline and xline axes using stack\n    .flat  # return just the flat coord object which is multi-index (so we can unstack later)\n)\nflat\n</pre> flat = (     survey_comb.rename(  # use the combined survey         {\"iline\": \"new_iline\", \"xline\": \"new_xline\"}     )  # renaming inline and xline so they don't confuse xarray     .stack(         {\"flat\": (\"new_iline\", \"new_xline\")}     )  # and flatten the iline and xline axes using stack     .flat  # return just the flat coord object which is multi-index (so we can unstack later) ) flat Out[14]: <pre>&lt;xarray.DataArray 'flat' (flat: 18054)&gt; Size: 144kB\narray([(640, 1099), (640, 1100), (640, 1101), ..., (757, 1249), (757, 1250),\n       (757, 1251)], dtype=object)\nCoordinates:\n    cdp_x      (flat) float64 144kB 9.576e+03 9.575e+03 ... 9.634e+03 9.633e+03\n    cdp_y      (flat) float64 144kB 2.932e+03 2.933e+03 ... 3.114e+03 3.114e+03\n  * flat       (flat) object 144kB MultiIndex\n  * new_iline  (flat) int32 72kB 640 640 640 640 640 640 ... 757 757 757 757 757\n  * new_xline  (flat) int32 72kB 1099 1100 1101 1102 ... 1248 1249 1250 1251</pre>xarray.DataArray'flat'<ul><li>flat: 18054</li></ul><ul><li>MultiIndex<pre>array([(640, 1099), (640, 1100), (640, 1101), ..., (757, 1249), (757, 1250),\n       (757, 1251)], dtype=object)</pre></li><li>Coordinates: (5)<ul><li>cdp_x(flat)float649.576e+03 9.575e+03 ... 9.633e+03<pre>array([9575.52313979, 9575.18111964, 9574.8390995 , ..., 9634.16415492,\n       9633.82213478, 9633.48011463])</pre></li><li>cdp_y(flat)float642.932e+03 2.933e+03 ... 3.114e+03<pre>array([2931.61508197, 2932.55477459, 2933.49446721, ..., 3112.58533186,\n       3113.52502448, 3114.4647171 ])</pre></li><li>flat(flat)objectMultiIndex<pre>array([(640, 1099), (640, 1100), (640, 1101), ..., (757, 1249), (757, 1250),\n       (757, 1251)], dtype=object)</pre></li><li>new_iline(flat)int32640 640 640 640 ... 757 757 757 757<pre>array([640, 640, 640, ..., 757, 757, 757], dtype=int32)</pre></li><li>new_xline(flat)int321099 1100 1101 ... 1249 1250 1251<pre>array([1099, 1100, 1101, ..., 1249, 1250, 1251], dtype=int32)</pre></li></ul></li><li>Indexes: (1)<ul><li>flatnew_ilinenew_xlinePandasMultiIndex<pre>PandasIndex(MultiIndex([(640, 1099),\n            (640, 1100),\n            (640, 1101),\n            (640, 1102),\n            (640, 1103),\n            (640, 1104),\n            (640, 1105),\n            (640, 1106),\n            (640, 1107),\n            (640, 1108),\n            ...\n            (757, 1242),\n            (757, 1243),\n            (757, 1244),\n            (757, 1245),\n            (757, 1246),\n            (757, 1247),\n            (757, 1248),\n            (757, 1249),\n            (757, 1250),\n            (757, 1251)],\n           name='flat', length=18054))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[15]: Copied! <pre># create a dictionary of new coordinates to sample to. We use flat here because it will easily allow us to\n# unstack the data after interpolation. It's necessary to use this flat dataset, otherwise xarray will try to\n# interpolate on the cartesian product of iline and xline, which isn't really what we want.\n\nresampling_data_arrays = dict(\n    iline=xr.DataArray(survey_2_new_ilxl_loc[:, 0], dims=\"flat\", coords={\"flat\": flat}),\n    xline=xr.DataArray(survey_2_new_ilxl_loc[:, 1], dims=\"flat\", coords={\"flat\": flat}),\n)\n\nresampling_data_arrays\n</pre> # create a dictionary of new coordinates to sample to. We use flat here because it will easily allow us to # unstack the data after interpolation. It's necessary to use this flat dataset, otherwise xarray will try to # interpolate on the cartesian product of iline and xline, which isn't really what we want.  resampling_data_arrays = dict(     iline=xr.DataArray(survey_2_new_ilxl_loc[:, 0], dims=\"flat\", coords={\"flat\": flat}),     xline=xr.DataArray(survey_2_new_ilxl_loc[:, 1], dims=\"flat\", coords={\"flat\": flat}), )  resampling_data_arrays Out[15]: <pre>{'iline': &lt;xarray.DataArray (flat: 18054)&gt; Size: 144kB\n array([653.01535386, 654.00016161, 654.98496936, ..., 780.41968002,\n        781.40448778, 782.38929553])\n Coordinates:\n   * flat       (flat) object 144kB MultiIndex\n   * new_iline  (flat) int32 72kB 640 640 640 640 640 640 ... 757 757 757 757 757\n   * new_xline  (flat) int32 72kB 1099 1100 1101 1102 ... 1248 1249 1250 1251,\n 'xline': &lt;xarray.DataArray (flat: 18054)&gt; Size: 144kB\n array([1267.82560706, 1267.65195888, 1267.47831071, ..., 1126.55587331,\n        1126.38222513, 1126.20857695])\n Coordinates:\n   * flat       (flat) object 144kB MultiIndex\n   * new_iline  (flat) int32 72kB 640 640 640 640 640 640 ... 757 757 757 757 757\n   * new_xline  (flat) int32 72kB 1099 1100 1101 1102 ... 1248 1249 1250 1251}</pre> <p>The resampling can take a while with larger cubes, so it is good to use dask and to output the cube to disk at this step. When using dask for processing it is better to output to disk regularly, as this will improve how your code runs and the overall memory usage.</p> <p>If you are executing this exmample locally, the task progress can be viewed by opening the dask client.</p> In\u00a0[16]: Copied! <pre>survey_2_resamp = survey_2.interp(**resampling_data_arrays)\nsurvey_2_resamp_newgeom = (\n    survey_2_resamp.drop_vars((\"iline\", \"xline\"))\n    .unstack(\"flat\")\n    .rename({\"new_iline\": \"iline\", \"new_xline\": \"xline\"})\n    .data\n)\nsurvey_2_resamp_newgeom.to_netcdf(\"data/survey_2_1.nc\", compute=True, engine=\"h5netcdf\")\n</pre> survey_2_resamp = survey_2.interp(**resampling_data_arrays) survey_2_resamp_newgeom = (     survey_2_resamp.drop_vars((\"iline\", \"xline\"))     .unstack(\"flat\")     .rename({\"new_iline\": \"iline\", \"new_xline\": \"xline\"})     .data ) survey_2_resamp_newgeom.to_netcdf(\"data/survey_2_1.nc\", compute=True, engine=\"h5netcdf\") In\u00a0[17]: Copied! <pre>survey_2_resamp_newgeom = xr.open_dataarray(\n    \"data/survey_2_1.nc\", chunks=dict(iline=10, xline=10, twt=100), engine=\"h5netcdf\"\n)\n</pre> survey_2_resamp_newgeom = xr.open_dataarray(     \"data/survey_2_1.nc\", chunks=dict(iline=10, xline=10, twt=100), engine=\"h5netcdf\" ) In\u00a0[18]: Copied! <pre>survey_2_resamp_newgeom.expand_dims({\"survey\": [2]})\n</pre> survey_2_resamp_newgeom.expand_dims({\"survey\": [2]}) Out[18]: <pre>&lt;xarray.DataArray 'data' (survey: 1, twt: 201, iline: 118, xline: 153)&gt; Size: 15MB\ndask.array&lt;broadcast_to, shape=(1, 201, 118, 153), dtype=float32, chunksize=(1, 100, 10, 10), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * survey   (survey) int64 8B 2\n  * iline    (iline) int32 472B 640 641 642 643 644 645 ... 753 754 755 756 757\n  * xline    (xline) int32 612B 1099 1100 1101 1102 1103 ... 1248 1249 1250 1251\n  * twt      (twt) int64 2kB 0 1 2 3 4 5 6 7 ... 193 194 195 196 197 198 199 200\n    cdp_x    (iline, xline) float64 144kB dask.array&lt;chunksize=(10, 10), meta=np.ndarray&gt;\n    cdp_y    (iline, xline) float64 144kB dask.array&lt;chunksize=(10, 10), meta=np.ndarray&gt;</pre>xarray.DataArray'data'<ul><li>survey: 1</li><li>twt: 201</li><li>iline: 118</li><li>xline: 153</li></ul><ul><li>dask.array&lt;chunksize=(1, 100, 10, 10), meta=np.ndarray&gt;  Array   Chunk   Bytes   13.84 MiB   39.06 kiB   Shape   (1, 201, 118, 153)   (1, 100, 10, 10)   Dask graph   576 chunks in 3 graph layers   Data type   float32 numpy.ndarray  1 1 153 118 201 </li><li>Coordinates: (6)<ul><li>survey(survey)int642<pre>array([2])</pre></li><li>iline(iline)int32640 641 642 643 ... 754 755 756 757<pre>array([640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653,\n       654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667,\n       668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681,\n       682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,\n       696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709,\n       710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723,\n       724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737,\n       738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n       752, 753, 754, 755, 756, 757], dtype=int32)</pre></li><li>xline(xline)int321099 1100 1101 ... 1249 1250 1251<pre>array([1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110,\n       1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122,\n       1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134,\n       1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146,\n       1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158,\n       1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170,\n       1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182,\n       1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194,\n       1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206,\n       1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218,\n       1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230,\n       1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242,\n       1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251], dtype=int32)</pre></li><li>twt(twt)int640 1 2 3 4 5 ... 196 197 198 199 200<pre>array([  0,   1,   2, ..., 198, 199, 200])</pre></li><li>cdp_x(iline, xline)float64dask.array&lt;chunksize=(10, 10), meta=np.ndarray&gt;  Array   Chunk   Bytes   141.05 kiB   800 B   Shape   (118, 153)   (10, 10)   Dask graph   192 chunks in 2 graph layers   Data type   float64 numpy.ndarray  153 118 </li><li>cdp_y(iline, xline)float64dask.array&lt;chunksize=(10, 10), meta=np.ndarray&gt;  Array   Chunk   Bytes   141.05 kiB   800 B   Shape   (118, 153)   (10, 10)   Dask graph   192 chunks in 2 graph layers   Data type   float64 numpy.ndarray  153 118 </li></ul></li><li>Indexes: (4)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n       ...\n       748, 749, 750, 751, 752, 753, 754, 755, 756, 757],\n      dtype='int32', name='iline', length=118))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108,\n       ...\n       1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251],\n      dtype='int32', name='xline', length=153))</pre></li><li>twtPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       191, 192, 193, 194, 195, 196, 197, 198, 199, 200],\n      dtype='int64', name='twt', length=201))</pre></li><li>surveyPandasIndex<pre>PandasIndex(Index([2], dtype='int64', name='survey'))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[19]: Copied! <pre># concatenate survey with new dimension \"survey\".\nsurvey_comb[\"data\"] = xr.concat([survey_1.data, survey_2_resamp_newgeom], \"survey\")\n</pre> # concatenate survey with new dimension \"survey\". survey_comb[\"data\"] = xr.concat([survey_1.data, survey_2_resamp_newgeom], \"survey\") <pre>/home/runner/.local/share/hatch/env/virtual/segysak/7aGXAoce/docs/lib/python3.10/site-packages/dask/array/core.py:4832: PerformanceWarning: Increasing number of chunks by factor of 11\n  result = blockwise(\n</pre> In\u00a0[20]: Copied! <pre>survey_comb\n</pre> survey_comb Out[20]: <pre>&lt;xarray.Dataset&gt; Size: 29MB\nDimensions:  (iline: 118, xline: 153, twt: 201, survey: 2)\nCoordinates:\n  * iline    (iline) int64 944B 640 641 642 643 644 645 ... 753 754 755 756 757\n  * xline    (xline) int64 1kB 1099 1100 1101 1102 1103 ... 1248 1249 1250 1251\n  * twt      (twt) int64 2kB 0 1 2 3 4 5 6 7 ... 193 194 195 196 197 198 199 200\n    cdp_x    (iline, xline) float64 144kB 9.576e+03 9.575e+03 ... 9.633e+03\n    cdp_y    (iline, xline) float64 144kB 2.932e+03 2.933e+03 ... 3.114e+03\nDimensions without coordinates: survey\nData variables:\n    data     (survey, iline, xline, twt) float32 29MB dask.array&lt;chunksize=(1, 10, 10, 100), meta=np.ndarray&gt;\nAttributes: (12/17)\n    ns:                  None\n    sample_rate:         None\n    text:                None\n    measurement_system:  None\n    d3_domain:           None\n    epsg:                None\n    ...                  ...\n    percentiles:         None\n    coord_scalar:        None\n    coord_scaled:        None\n    dimensions:          None\n    vert_dimension:      None\n    vert_domain:         None</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>iline: 118</li><li>xline: 153</li><li>twt: 201</li><li>survey: 2</li></ul></li><li>Coordinates: (5)<ul><li>iline(iline)int64640 641 642 643 ... 754 755 756 757<pre>array([640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653,\n       654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667,\n       668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681,\n       682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695,\n       696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709,\n       710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723,\n       724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737,\n       738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n       752, 753, 754, 755, 756, 757])</pre></li><li>xline(xline)int641099 1100 1101 ... 1249 1250 1251<pre>array([1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110,\n       1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122,\n       1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134,\n       1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146,\n       1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158,\n       1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170,\n       1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182,\n       1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194,\n       1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206,\n       1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218,\n       1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230,\n       1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242,\n       1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251])</pre></li><li>twt(twt)int640 1 2 3 4 5 ... 196 197 198 199 200<pre>array([  0,   1,   2, ..., 198, 199, 200])</pre></li><li>cdp_x(iline, xline)float649.576e+03 9.575e+03 ... 9.633e+03<pre>array([[9575.52313979, 9575.18111964, 9574.8390995 , ..., 9524.22011829,\n        9523.87809815, 9523.536078  ],\n       [9576.46283241, 9576.12081227, 9575.77879212, ..., 9525.15981091,\n        9524.81779077, 9524.47577062],\n       [9577.40252503, 9577.06050489, 9576.71848474, ..., 9526.09950353,\n        9525.75748339, 9525.41546324],\n       ...,\n       [9683.58779118, 9683.24577104, 9682.90375089, ..., 9632.28476968,\n        9631.94274954, 9631.60072939],\n       [9684.5274838 , 9684.18546366, 9683.84344351, ..., 9633.2244623 ,\n        9632.88244216, 9632.54042201],\n       [9685.46717642, 9685.12515628, 9684.78313613, ..., 9634.16415492,\n        9633.82213478, 9633.48011463]])</pre></li><li>cdp_y(iline, xline)float642.932e+03 2.933e+03 ... 3.114e+03<pre>array([[2931.61508197, 2932.55477459, 2933.49446721, ..., 3072.56897509,\n        3073.50866771, 3074.44836033],\n       [2931.95710212, 2932.89679474, 2933.83648736, ..., 3072.91099523,\n        3073.85068785, 3074.79038047],\n       [2932.29912226, 2933.23881488, 2934.1785075 , ..., 3073.25301538,\n        3074.192708  , 3075.13240062],\n       ...,\n       [2970.94739845, 2971.88709108, 2972.8267837 , ..., 3111.90129157,\n        3112.84098419, 3113.78067681],\n       [2971.2894186 , 2972.22911122, 2973.16880384, ..., 3112.24331172,\n        3113.18300434, 3114.12269696],\n       [2971.63143874, 2972.57113136, 2973.51082398, ..., 3112.58533186,\n        3113.52502448, 3114.4647171 ]])</pre></li></ul></li><li>Data variables: (1)<ul><li>data(survey, iline, xline, twt)float32dask.array&lt;chunksize=(1, 10, 10, 100), meta=np.ndarray&gt;  Array   Chunk   Bytes   27.69 MiB   39.06 kiB   Shape   (2, 118, 153, 201)   (1, 10, 10, 100)   Dask graph   1872 chunks in 22 graph layers   Data type   float32 numpy.ndarray  2 1 201 153 118 </li></ul></li><li>Indexes: (3)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n       ...\n       748, 749, 750, 751, 752, 753, 754, 755, 756, 757],\n      dtype='int64', name='iline', length=118))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108,\n       ...\n       1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251],\n      dtype='int64', name='xline', length=153))</pre></li><li>twtPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       191, 192, 193, 194, 195, 196, 197, 198, 199, 200],\n      dtype='int64', name='twt', length=201))</pre></li></ul></li><li>Attributes: (17)ns :Nonesample_rate :Nonetext :Nonemeasurement_system :Noned3_domain :Noneepsg :Nonecorner_points :Nonecorner_points_xy :Nonesource_file :Nonesrd :Nonedatatype :Nonepercentiles :Nonecoord_scalar :Nonecoord_scaled :Nonedimensions :Nonevert_dimension :Nonevert_domain :None</li></ul> <p>We can check the merged surveys by looking at some plots. If we select just the first survey the values will be 1. If we select just the second survey the values will be 2. And if we take the mean along the survey dimension, then where the surveys overlap the values will be 1.5.</p> <p>For seismic data, a better form of conditioning and reduction might be required for merging traces together to ensure a smoother seam.</p> In\u00a0[21]: Copied! <pre>sel = survey_comb.sel(xline=1190)\n\nfig, axs = plt.subplots(ncols=3, figsize=(30, 10))\n\nsel.isel(survey=0).data.T.plot(ax=axs[0], yincrease=False, vmin=1, vmax=2)\nsel.isel(survey=1).data.T.plot(ax=axs[1], yincrease=False, vmin=1, vmax=2)\nsel.data.mean(\"survey\").T.plot(ax=axs[2], yincrease=False, vmin=1, vmax=2)\n</pre> sel = survey_comb.sel(xline=1190)  fig, axs = plt.subplots(ncols=3, figsize=(30, 10))  sel.isel(survey=0).data.T.plot(ax=axs[0], yincrease=False, vmin=1, vmax=2) sel.isel(survey=1).data.T.plot(ax=axs[1], yincrease=False, vmin=1, vmax=2) sel.data.mean(\"survey\").T.plot(ax=axs[2], yincrease=False, vmin=1, vmax=2) Out[21]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7f75a812c5b0&gt;</pre> In\u00a0[22]: Copied! <pre>survey_comb.twt\n</pre> survey_comb.twt Out[22]: <pre>&lt;xarray.DataArray 'twt' (twt: 201)&gt; Size: 2kB\narray([  0,   1,   2, ..., 198, 199, 200])\nCoordinates:\n  * twt      (twt) int64 2kB 0 1 2 3 4 5 6 7 ... 193 194 195 196 197 198 199 200</pre>xarray.DataArray'twt'<ul><li>twt: 201</li></ul><ul><li>0 1 2 3 4 5 6 7 8 9 10 ... 190 191 192 193 194 195 196 197 198 199 200<pre>array([  0,   1,   2, ..., 198, 199, 200])</pre></li><li>Coordinates: (1)<ul><li>twt(twt)int640 1 2 3 4 5 ... 196 197 198 199 200<pre>array([  0,   1,   2, ..., 198, 199, 200])</pre></li></ul></li><li>Indexes: (1)<ul><li>twtPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       191, 192, 193, 194, 195, 196, 197, 198, 199, 200],\n      dtype='int64', name='twt', length=201))</pre></li></ul></li><li>Attributes: (0)</li></ul> <p>And if we inspect a time slice.</p> In\u00a0[23]: Copied! <pre>sel = survey_comb.sel(twt=100)\n\nfig, axs = plt.subplots(ncols=3, figsize=(30, 10))\n\nsel.isel(survey=0).data.T.plot(ax=axs[0], yincrease=False, vmin=1, vmax=2)\nsel.isel(survey=1).data.T.plot(ax=axs[1], yincrease=False, vmin=1, vmax=2)\nsel.data.mean(\"survey\").T.plot(ax=axs[2], yincrease=False, vmin=1, vmax=2)\n</pre> sel = survey_comb.sel(twt=100)  fig, axs = plt.subplots(ncols=3, figsize=(30, 10))  sel.isel(survey=0).data.T.plot(ax=axs[0], yincrease=False, vmin=1, vmax=2) sel.isel(survey=1).data.T.plot(ax=axs[1], yincrease=False, vmin=1, vmax=2) sel.data.mean(\"survey\").T.plot(ax=axs[2], yincrease=False, vmin=1, vmax=2) Out[23]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7f7590f8dae0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/example_merge_surveys.html#merging-seismic-data-cubes","title":"Merging Seismic Data Cubes\u00b6","text":""},{"location":"examples/example_merge_surveys.html#how-to-merge-two-different-geometries","title":"How to merge two different geometries\u00b6","text":""},{"location":"examples/example_merge_surveys.html#resampling-survey_2-to-survey_1-geometry","title":"Resampling <code>survey_2</code> to <code>survey_1</code> geometry\u00b6","text":"<p>Resampling one dataset to another requires us to tell Xarray/dask where we want the new traces to be. First we can convert the x and y coordinates of the combined survey to the il xl of survey 2 using the affine transform of survey 2. This transform works il/xl to x and y and therefore we need it inverted.</p>"},{"location":"examples/example_merge_surveys.html#combining-cubes","title":"Combining Cubes\u00b6","text":"<p>The last step is combining the cube is to load the two datasets to be combined and concatenate them together along a new axis. This will simplify reduction processes later.</p>"},{"location":"examples/example_segy_headers.html","title":"Working with SEG-Y headers","text":"In\u00a0[2]: Copied! <pre>from segysak.segy import segy_header_scan\n\n# default just needs the file name\nscan = segy_header_scan(\"data/volve10r12-full-twt-sub3d.sgy\")\nscan\n</pre> from segysak.segy import segy_header_scan  # default just needs the file name scan = segy_header_scan(\"data/volve10r12-full-twt-sub3d.sgy\") scan Out[2]: byte_loc count mean std min 25% 50% 75% max TRACE_SEQUENCE_LINE 1 1000.0 100.54 57.831072 1.0 50.75 100.5 150.25 202.0 TRACE_SEQUENCE_FILE 5 1000.0 10091.98 1.407687 10090.0 10091.00 10092.0 10093.00 10094.0 FieldRecord 9 1000.0 10091.98 1.407687 10090.0 10091.00 10092.0 10093.00 10094.0 TraceNumber 13 1000.0 100.54 57.831072 1.0 50.75 100.5 150.25 202.0 EnergySourcePoint 17 1000.0 0.00 0.000000 0.0 0.00 0.0 0.00 0.0 ... ... ... ... ... ... ... ... ... ... SourceMeasurementMantissa 225 1000.0 0.00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceMeasurementExponent 229 1000.0 0.00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceMeasurementUnit 231 1000.0 0.00 0.000000 0.0 0.00 0.0 0.00 0.0 UnassignedInt1 233 1000.0 0.00 0.000000 0.0 0.00 0.0 0.00 0.0 UnassignedInt2 237 1000.0 0.00 0.000000 0.0 0.00 0.0 0.00 0.0 <p>91 rows \u00d7 9 columns</p> <p>If you want to see the full DataFrame in a notebook, use the <code>pandas</code> options context manager.</p> In\u00a0[3]: Copied! <pre>import pandas as pd\nfrom IPython.display import display\n\nwith pd.option_context(\"display.max_rows\", 91):\n    display(scan)\n</pre> import pandas as pd from IPython.display import display  with pd.option_context(\"display.max_rows\", 91):     display(scan) byte_loc count mean std min 25% 50% 75% max TRACE_SEQUENCE_LINE 1 1000.0 1.005400e+02 57.831072 1.0 5.075000e+01 100.5 1.502500e+02 202.0 TRACE_SEQUENCE_FILE 5 1000.0 1.009198e+04 1.407687 10090.0 1.009100e+04 10092.0 1.009300e+04 10094.0 FieldRecord 9 1000.0 1.009198e+04 1.407687 10090.0 1.009100e+04 10092.0 1.009300e+04 10094.0 TraceNumber 13 1000.0 1.005400e+02 57.831072 1.0 5.075000e+01 100.5 1.502500e+02 202.0 EnergySourcePoint 17 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 CDP 21 1000.0 2.249540e+03 57.831072 2150.0 2.199750e+03 2249.5 2.299250e+03 2351.0 CDP_TRACE 25 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 TraceIdentificationCode 29 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 NSummedTraces 31 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 NStackedTraces 33 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 DataUse 35 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 offset 37 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 ReceiverGroupElevation 41 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceSurfaceElevation 45 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceDepth 49 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 ReceiverDatumElevation 53 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceDatumElevation 57 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceWaterDepth 61 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GroupWaterDepth 65 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 ElevationScalar 69 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 SourceGroupScalar 71 1000.0 -1.000000e+02 0.000000 -100.0 -1.000000e+02 -100.0 -1.000000e+02 -100.0 SourceX 73 1000.0 4.351992e+07 70152.496037 43396267.0 4.345933e+07 43519976.5 4.358062e+07 43641261.0 SourceY 77 1000.0 6.477772e+08 17532.885301 647744704.0 6.477622e+08 647777222.0 6.477923e+08 647809133.0 GroupX 81 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GroupY 85 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 CoordinateUnits 89 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 WeatheringVelocity 91 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SubWeatheringVelocity 93 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceUpholeTime 95 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GroupUpholeTime 97 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceStaticCorrection 99 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GroupStaticCorrection 101 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TotalStaticApplied 103 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 LagTimeA 105 1000.0 4.000000e+00 0.000000 4.0 4.000000e+00 4.0 4.000000e+00 4.0 LagTimeB 107 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 DelayRecordingTime 109 1000.0 4.000000e+00 0.000000 4.0 4.000000e+00 4.0 4.000000e+00 4.0 MuteTimeStart 111 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 MuteTimeEND 113 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TRACE_SAMPLE_COUNT 115 1000.0 8.500000e+02 0.000000 850.0 8.500000e+02 850.0 8.500000e+02 850.0 TRACE_SAMPLE_INTERVAL 117 1000.0 4.000000e+03 0.000000 4000.0 4.000000e+03 4000.0 4.000000e+03 4000.0 GainType 119 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 InstrumentGainConstant 121 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 InstrumentInitialGain 123 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 Correlated 125 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 SweepFrequencyStart 127 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SweepFrequencyEnd 129 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SweepLength 131 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SweepType 133 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 SweepTraceTaperLengthStart 135 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SweepTraceTaperLengthEnd 137 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TaperType 139 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 AliasFilterFrequency 141 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 AliasFilterSlope 143 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 NotchFilterFrequency 145 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 NotchFilterSlope 147 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 LowCutFrequency 149 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 HighCutFrequency 151 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 LowCutSlope 153 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 HighCutSlope 155 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 YearDataRecorded 157 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 DayOfYear 159 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 HourOfDay 161 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 MinuteOfHour 163 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SecondOfMinute 165 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TimeBaseCode 167 1000.0 1.000000e+00 0.000000 1.0 1.000000e+00 1.0 1.000000e+00 1.0 TraceWeightingFactor 169 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GeophoneGroupNumberRoll1 171 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GeophoneGroupNumberFirstTraceOrigField 173 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GeophoneGroupNumberLastTraceOrigField 175 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 GapSize 177 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 OverTravel 179 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 CDP_X 181 1000.0 4.351992e+07 70152.496037 43396267.0 4.345933e+07 43519976.5 4.358062e+07 43641261.0 CDP_Y 185 1000.0 6.477772e+08 17532.885301 647744704.0 6.477622e+08 647777222.0 6.477923e+08 647809133.0 INLINE_3D 189 1000.0 1.009198e+04 1.407687 10090.0 1.009100e+04 10092.0 1.009300e+04 10094.0 CROSSLINE_3D 193 1000.0 2.249540e+03 57.831072 2150.0 2.199750e+03 2249.5 2.299250e+03 2351.0 ShotPoint 197 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 ShotPointScalar 201 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TraceValueMeasurementUnit 203 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TransductionConstantMantissa 205 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TransductionConstantPower 209 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TransductionUnit 211 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 TraceIdentifier 213 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 ScalarTraceHeader 215 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceType 217 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceEnergyDirectionMantissa 219 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceEnergyDirectionExponent 223 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceMeasurementMantissa 225 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceMeasurementExponent 229 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 SourceMeasurementUnit 231 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 UnassignedInt1 233 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 UnassignedInt2 237 1000.0 0.000000e+00 0.000000 0.0 0.000000e+00 0.0 0.000000e+00 0.0 <p>Often lots of header fields don't get filled, so lets filter by the standard deviation column <code>std</code>. In fact, there are so few here we don't need the context manager. As you can see, for <code>segy_loader</code> or <code>segy_converter</code> we will need to tell those functions that the byte location for iline and xline are 189 and 193 respectively, and the byte locations for cdp_x and cdp_y are either 73 and 77 or 181 and 185 which are identical pairs.</p> In\u00a0[4]: Copied! <pre># NIIIICCCEEEE...\nscan[scan[\"std\"] &gt; 0]\n</pre> # NIIIICCCEEEE... scan[scan[\"std\"] &gt; 0] Out[4]: byte_loc count mean std min 25% 50% 75% max TRACE_SEQUENCE_LINE 1 1000.0 1.005400e+02 57.831072 1.0 5.075000e+01 100.5 1.502500e+02 202.0 TRACE_SEQUENCE_FILE 5 1000.0 1.009198e+04 1.407687 10090.0 1.009100e+04 10092.0 1.009300e+04 10094.0 FieldRecord 9 1000.0 1.009198e+04 1.407687 10090.0 1.009100e+04 10092.0 1.009300e+04 10094.0 TraceNumber 13 1000.0 1.005400e+02 57.831072 1.0 5.075000e+01 100.5 1.502500e+02 202.0 CDP 21 1000.0 2.249540e+03 57.831072 2150.0 2.199750e+03 2249.5 2.299250e+03 2351.0 SourceX 73 1000.0 4.351992e+07 70152.496037 43396267.0 4.345933e+07 43519976.5 4.358062e+07 43641261.0 SourceY 77 1000.0 6.477772e+08 17532.885301 647744704.0 6.477622e+08 647777222.0 6.477923e+08 647809133.0 CDP_X 181 1000.0 4.351992e+07 70152.496037 43396267.0 4.345933e+07 43519976.5 4.358062e+07 43641261.0 CDP_Y 185 1000.0 6.477772e+08 17532.885301 647744704.0 6.477622e+08 647777222.0 6.477923e+08 647809133.0 INLINE_3D 189 1000.0 1.009198e+04 1.407687 10090.0 1.009100e+04 10092.0 1.009300e+04 10094.0 CROSSLINE_3D 193 1000.0 2.249540e+03 57.831072 2150.0 2.199750e+03 2249.5 2.299250e+03 2351.0 In\u00a0[5]: Copied! <pre>from segysak.segy import segy_header_scrape\n\nscrape = segy_header_scrape(\"data/volve10r12-full-twt-sub3d.sgy\", partial_scan=10000)\nscrape\n</pre> from segysak.segy import segy_header_scrape  scrape = segy_header_scrape(\"data/volve10r12-full-twt-sub3d.sgy\", partial_scan=10000) scrape Out[5]: TRACE_SEQUENCE_LINE TRACE_SEQUENCE_FILE FieldRecord TraceNumber EnergySourcePoint CDP CDP_TRACE TraceIdentificationCode NSummedTraces NStackedTraces ... TraceIdentifier ScalarTraceHeader SourceType SourceEnergyDirectionMantissa SourceEnergyDirectionExponent SourceMeasurementMantissa SourceMeasurementExponent SourceMeasurementUnit UnassignedInt1 UnassignedInt2 0 1 10090 10090 1 0 2150 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 2 10090 10090 2 0 2151 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 3 10090 10090 3 0 2152 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 4 10090 10090 4 0 2153 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 5 10090 10090 5 0 2154 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9995 98 10139 10139 98 0 2247 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 9996 99 10139 10139 99 0 2248 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 9997 100 10139 10139 100 0 2249 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 9998 101 10139 10139 101 0 2250 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 9999 102 10139 10139 102 0 2251 1 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 <p>10000 rows \u00d7 91 columns</p> <p>We know from the scan that many of these fields were empty so lets go ahead and filter our scrape by using the standard deviation again and passing the index which is the same as our column names.</p> In\u00a0[6]: Copied! <pre>scrape = scrape[scan[scan[\"std\"] &gt; 0].index]\nscrape\n</pre> scrape = scrape[scan[scan[\"std\"] &gt; 0].index] scrape Out[6]: TRACE_SEQUENCE_LINE TRACE_SEQUENCE_FILE FieldRecord TraceNumber CDP SourceX SourceY CDP_X CDP_Y INLINE_3D CROSSLINE_3D 0 1 10090 10090 1 2150 43640052 647744704 43640052 647744704 10090 2150 1 2 10090 10090 2 2151 43638839 647745006 43638839 647745006 10090 2151 2 3 10090 10090 3 2152 43637626 647745309 43637626 647745309 10090 2152 3 4 10090 10090 4 2153 43636413 647745611 43636413 647745611 10090 2153 4 5 10090 10090 5 2154 43635200 647745914 43635200 647745914 10090 2154 ... ... ... ... ... ... ... ... ... ... ... ... 9995 98 10139 10139 98 2247 43537224 647833471 43537224 647833471 10139 2247 9996 99 10139 10139 99 2248 43536011 647833773 43536011 647833773 10139 2248 9997 100 10139 10139 100 2249 43534798 647834076 43534798 647834076 10139 2249 9998 101 10139 10139 101 2250 43533585 647834378 43533585 647834378 10139 2250 9999 102 10139 10139 102 2251 43532372 647834681 43532372 647834681 10139 2251 <p>10000 rows \u00d7 11 columns</p> <p>We know from the scan that many of these fields were empty so lets go ahead and filter our scrape by using the standard deviation again and passing the index which is the same as our column names.</p> In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[8]: Copied! <pre>plot = scrape.hist(bins=25, figsize=(20, 10))\n</pre> plot = scrape.hist(bins=25, figsize=(20, 10)) <p>We can also just plot up the geometry to check that everything looks ok, here the line numbering and coordinates seem to match up, great!</p> In\u00a0[9]: Copied! <pre>fig, axs = plt.subplots(nrows=2, figsize=(12, 10), sharex=True, sharey=True)\n\nscrape.plot(\n    kind=\"scatter\", x=\"CDP_X\", y=\"CDP_Y\", c=\"INLINE_3D\", ax=axs[0], cmap=\"gist_ncar\"\n)\nscrape.plot(\n    kind=\"scatter\", x=\"CDP_X\", y=\"CDP_Y\", c=\"CROSSLINE_3D\", ax=axs[1], cmap=\"gist_ncar\"\n)\nfor aa in axs:\n    aa.set_aspect(\"equal\", \"box\")\n</pre> fig, axs = plt.subplots(nrows=2, figsize=(12, 10), sharex=True, sharey=True)  scrape.plot(     kind=\"scatter\", x=\"CDP_X\", y=\"CDP_Y\", c=\"INLINE_3D\", ax=axs[0], cmap=\"gist_ncar\" ) scrape.plot(     kind=\"scatter\", x=\"CDP_X\", y=\"CDP_Y\", c=\"CROSSLINE_3D\", ax=axs[1], cmap=\"gist_ncar\" ) for aa in axs:     aa.set_aspect(\"equal\", \"box\")"},{"location":"examples/example_segy_headers.html#working-with-seg-y-headers","title":"Working with SEG-Y headers\u00b6","text":"<p>Headers in SEG-Y data are additional meta information associated with each trace. In SEG-Y these are not pooled in a common data block but interleaved with the seismic trace data so we need to do some work to extract it. segysak has two helper methods for extracting information from a SEG-Y file. These are <code>segy_header_scan</code> and <code>segy_header_scrape</code>. Both of these functions return <code>pandas.DataFrame</code> objects containing header or header related information which can be used in QC, analysis and plotting.</p>"},{"location":"examples/example_segy_headers.html#scanning-the-headers","title":"Scanning the headers\u00b6","text":"<p><code>segy_header_scan</code> is primarily designed to help quickly asscertain the byte locations of key header information for loading or converting the full SEG-Y file. It does this by just looking at the first N traces (1000 by default) and returns the byte location and statistics related to the file.</p>"},{"location":"examples/example_segy_headers.html#scraping-headers","title":"Scraping Headers\u00b6","text":"<p>Scraping the header works like a scan but instead of statistics we get a DataFrame of actual trace header values. You can reduce the size of the scan by using the partial_scan keyword if required. The index of the DataFrame is the trace index and the columns are the header fields.</p>"},{"location":"examples/example_segysak_basics.html","title":"SEGY-SAK Basics","text":"In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\n\n%matplotlib inline\n\nfrom segysak import create3d_dataset\n\n# dims iline, xline, vert\ndims = (10, 5, 1000)\n\nnew_seisnc = create3d_dataset(\n    dims,\n    first_iline=1,\n    iline_step=2,\n    first_xline=10,\n    xline_step=1,\n    first_sample=0,\n    sample_rate=1,\n    vert_domain=\"TWT\",\n)\nnew_seisnc\n</pre> import matplotlib.pyplot as plt  %matplotlib inline  from segysak import create3d_dataset  # dims iline, xline, vert dims = (10, 5, 1000)  new_seisnc = create3d_dataset(     dims,     first_iline=1,     iline_step=2,     first_xline=10,     xline_step=1,     first_sample=0,     sample_rate=1,     vert_domain=\"TWT\", ) new_seisnc Out[2]: <pre>&lt;xarray.Dataset&gt; Size: 8kB\nDimensions:  (iline: 10, xline: 5, twt: 1000)\nCoordinates:\n  * iline    (iline) int64 80B 1 3 5 7 9 11 13 15 17 19\n  * xline    (xline) int64 40B 10 11 12 13 14\n  * twt      (twt) int64 8kB 0 1 2 3 4 5 6 7 ... 992 993 994 995 996 997 998 999\nData variables:\n    *empty*\nAttributes: (12/17)\n    ns:                  None\n    sample_rate:         1\n    text:                SEGY-SAK Create 3D Dataset\n    measurement_system:  None\n    d3_domain:           TWT\n    epsg:                None\n    ...                  ...\n    percentiles:         None\n    coord_scalar:        None\n    coord_scaled:        None\n    dimensions:          None\n    vert_dimension:      None\n    vert_domain:         None</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>iline: 10</li><li>xline: 5</li><li>twt: 1000</li></ul></li><li>Coordinates: (3)<ul><li>iline(iline)int641 3 5 7 9 11 13 15 17 19<pre>array([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19])</pre></li><li>xline(xline)int6410 11 12 13 14<pre>array([10, 11, 12, 13, 14])</pre></li><li>twt(twt)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li></ul></li><li>Data variables: (0)<ul></ul></li><li>Indexes: (3)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([1, 3, 5, 7, 9, 11, 13, 15, 17, 19], dtype='int64', name='iline'))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([10, 11, 12, 13, 14], dtype='int64', name='xline'))</pre></li><li>twtPandasIndex<pre>PandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='twt', length=1000))</pre></li></ul></li><li>Attributes: (17)ns :Nonesample_rate :1text :SEGY-SAK Create 3D Datasetmeasurement_system :Noned3_domain :TWTepsg :Nonecorner_points :[(1, 10), (19, 10), (19, 14), (1, 14), (1, 10)]corner_points_xy :Nonesource_file :Nonesrd :Nonedatatype :Nonepercentiles :Nonecoord_scalar :Nonecoord_scaled :Nonedimensions :Nonevert_dimension :Nonevert_domain :None</li></ul> In\u00a0[3]: Copied! <pre># select iline value 9\nxarray_selection = new_seisnc.sel(iline=9)\n# select xline value 12\nxarray_selection = new_seisnc.sel(xline=12)\n# select iline and xline intersection point\nxarray_selection = new_seisnc.sel(iline=9, xline=12)\n# key error\n# xarray_selection = new_seisnc.sel(twt=8.5)\n# select nearest twt slice\nxarray_selection = new_seisnc.sel(twt=8.5, method=\"nearest\")\n# select a range\nxarray_selection = new_seisnc.sel(iline=[9, 11, 13])\n# select a subcube\n# also slices can be used to select ranges as if they were indices!\nxarray_selection = new_seisnc.sel(iline=slice(9, 13), xline=[10, 11, 12])\n# index selection principles are similar\nxarray_selection = new_seisnc.sel(iline=slice(1, 4))\n\n# putting it altogether to extract a sub-cropped horizon slice at odd interval\nxarray_selection = new_seisnc.sel(\n    twt=8.5, iline=new_seisnc.iline[:4], xline=[10, 12], method=\"nearest\"\n)\nxarray_selection\n</pre> # select iline value 9 xarray_selection = new_seisnc.sel(iline=9) # select xline value 12 xarray_selection = new_seisnc.sel(xline=12) # select iline and xline intersection point xarray_selection = new_seisnc.sel(iline=9, xline=12) # key error # xarray_selection = new_seisnc.sel(twt=8.5) # select nearest twt slice xarray_selection = new_seisnc.sel(twt=8.5, method=\"nearest\") # select a range xarray_selection = new_seisnc.sel(iline=[9, 11, 13]) # select a subcube # also slices can be used to select ranges as if they were indices! xarray_selection = new_seisnc.sel(iline=slice(9, 13), xline=[10, 11, 12]) # index selection principles are similar xarray_selection = new_seisnc.sel(iline=slice(1, 4))  # putting it altogether to extract a sub-cropped horizon slice at odd interval xarray_selection = new_seisnc.sel(     twt=8.5, iline=new_seisnc.iline[:4], xline=[10, 12], method=\"nearest\" ) xarray_selection Out[3]: <pre>&lt;xarray.Dataset&gt; Size: 56B\nDimensions:  (iline: 4, xline: 2)\nCoordinates:\n  * iline    (iline) int64 32B 1 3 5 7\n  * xline    (xline) int64 16B 10 12\n    twt      int64 8B 9\nData variables:\n    *empty*\nAttributes: (12/17)\n    ns:                  None\n    sample_rate:         1\n    text:                SEGY-SAK Create 3D Dataset\n    measurement_system:  None\n    d3_domain:           TWT\n    epsg:                None\n    ...                  ...\n    percentiles:         None\n    coord_scalar:        None\n    coord_scaled:        None\n    dimensions:          None\n    vert_dimension:      None\n    vert_domain:         None</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>iline: 4</li><li>xline: 2</li></ul></li><li>Coordinates: (3)<ul><li>iline(iline)int641 3 5 7<pre>array([1, 3, 5, 7])</pre></li><li>xline(xline)int6410 12<pre>array([10, 12])</pre></li><li>twt()int649<pre>array(9)</pre></li></ul></li><li>Data variables: (0)<ul></ul></li><li>Indexes: (2)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([1, 3, 5, 7], dtype='int64', name='iline'))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([10, 12], dtype='int64', name='xline'))</pre></li></ul></li><li>Attributes: (17)ns :Nonesample_rate :1text :SEGY-SAK Create 3D Datasetmeasurement_system :Noned3_domain :TWTepsg :Nonecorner_points :[(1, 10), (19, 10), (19, 14), (1, 14), (1, 10)]corner_points_xy :Nonesource_file :Nonesrd :Nonedatatype :Nonepercentiles :Nonecoord_scalar :Nonecoord_scaled :Nonedimensions :Nonevert_dimension :Nonevert_domain :None</li></ul> In\u00a0[4]: Copied! <pre># get the dims\ndims = new_seisnc.sizes\nprint(dims)\ndkeys = (\"xline\", \"iline\", \"twt\")\ndsize = [dims[key] for key in dkeys]\nprint(\"keys:\", dkeys, \"sizes:\", dsize)\n</pre> # get the dims dims = new_seisnc.sizes print(dims) dkeys = (\"xline\", \"iline\", \"twt\") dsize = [dims[key] for key in dkeys] print(\"keys:\", dkeys, \"sizes:\", dsize) <pre>Frozen({'iline': 10, 'xline': 5, 'twt': 1000})\nkeys: ('xline', 'iline', 'twt') sizes: [5, 10, 1000]\n</pre> In\u00a0[5]: Copied! <pre>import numpy as np\n\n# create some data and the dimension shapes\nxline_, iline_, twt_ = np.meshgrid(new_seisnc.iline, new_seisnc.xline, new_seisnc.twt)\ndata = np.sin(twt_ / 100) + 2 * iline_ * np.cos(xline_ / 20 + 10)\n\n# assign the data to dataset by passing in a tuple of the dimension keys and the new data\nnew_seisnc[\"data\"] = (dkeys, data)\n\nfig, axs = plt.subplots(ncols=3, figsize=(15, 5))\n\n# axes are wrong for seismic\nnew_seisnc.data.sel(iline=7).plot(ax=axs[0])\n\n# rotate the cube\nnew_seisnc.data.transpose(\"twt\", \"iline\", \"xline\").sel(iline=7).plot(\n    ax=axs[1], yincrease=False\n)\n\n# xline is the same?\nnew_seisnc.data.transpose(\"twt\", \"iline\", \"xline\").isel(xline=2).plot(\n    ax=axs[2], yincrease=False\n)\n</pre> import numpy as np  # create some data and the dimension shapes xline_, iline_, twt_ = np.meshgrid(new_seisnc.iline, new_seisnc.xline, new_seisnc.twt) data = np.sin(twt_ / 100) + 2 * iline_ * np.cos(xline_ / 20 + 10)  # assign the data to dataset by passing in a tuple of the dimension keys and the new data new_seisnc[\"data\"] = (dkeys, data)  fig, axs = plt.subplots(ncols=3, figsize=(15, 5))  # axes are wrong for seismic new_seisnc.data.sel(iline=7).plot(ax=axs[0])  # rotate the cube new_seisnc.data.transpose(\"twt\", \"iline\", \"xline\").sel(iline=7).plot(     ax=axs[1], yincrease=False )  # xline is the same? new_seisnc.data.transpose(\"twt\", \"iline\", \"xline\").isel(xline=2).plot(     ax=axs[2], yincrease=False ) Out[5]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7f22817541f0&gt;</pre> <p>Accessing the coordinates or data as a <code>numpy</code> array.</p> In\u00a0[6]: Copied! <pre># Data can be dropped out of xarray but you then need to manage the coordinates and the dimension\n# labels\n\nnew_seisnc.iline.values\n</pre> # Data can be dropped out of xarray but you then need to manage the coordinates and the dimension # labels  new_seisnc.iline.values Out[6]: <pre>array([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19])</pre> <p><code>xarray.Dataset</code> and <code>DataArray</code> have lots of great built in methods in addition to plot.</p> In\u00a0[7]: Copied! <pre>print(new_seisnc.data.mean())\n</pre> print(new_seisnc.data.mean()) <pre>&lt;xarray.DataArray 'data' ()&gt; Size: 8B\narray(-10.76364169)\n</pre> In\u00a0[8]: Copied! <pre>print(new_seisnc.data.max())\n</pre> print(new_seisnc.data.max()) <pre>&lt;xarray.DataArray 'data' ()&gt; Size: 8B\narray(0.08882943)\n</pre> <p><code>numpy</code> functions also work on <code>xarray</code> objects but this returns a new <code>DataArray</code> not a <code>ndarray</code>.</p> In\u00a0[9]: Copied! <pre>print(np.sum(new_seisnc.data))\n</pre> print(np.sum(new_seisnc.data)) <pre>&lt;xarray.DataArray 'data' ()&gt; Size: 8B\narray(-538182.08430941)\n</pre> <p>With <code>xarray</code> you can apply operations along 1 or more dimensions to reduce the dataset. This could be useful for collapsing gathers for example by applying the mean along the <code>offset</code> dimension. Here we combine a <code>numpy</code> operation <code>abs</code> which returns an <code>DataArray</code> and then sum along the time dimension to create a grid without the time dimension. Along with using masks this is a fundamental building block for performing horizonal sculpting.</p> In\u00a0[10]: Copied! <pre>map_data = np.abs(new_seisnc.data).sum(dim=\"twt\")\nimg = map_data.plot()\n</pre> map_data = np.abs(new_seisnc.data).sum(dim=\"twt\") img = map_data.plot() <p>Sometimes we need to modify the dimensions because they were read wrong or to scale them. Modify your dimension from the seisnc and then put it back using <code>assign_coords</code>.</p> In\u00a0[11]: Copied! <pre>new_seisnc.assign_coords(iline=new_seisnc.iline * 10, twt=new_seisnc.twt + 1500)\n</pre> new_seisnc.assign_coords(iline=new_seisnc.iline * 10, twt=new_seisnc.twt + 1500) Out[11]: <pre>&lt;xarray.Dataset&gt; Size: 408kB\nDimensions:  (xline: 5, iline: 10, twt: 1000)\nCoordinates:\n  * xline    (xline) int64 40B 10 11 12 13 14\n  * iline    (iline) int64 80B 10 30 50 70 90 110 130 150 170 190\n  * twt      (twt) int64 8kB 1500 1501 1502 1503 1504 ... 2496 2497 2498 2499\nData variables:\n    data     (xline, iline, twt) float64 400kB -16.22 -16.21 ... -1.803 -1.811\nAttributes: (12/17)\n    ns:                  None\n    sample_rate:         1\n    text:                SEGY-SAK Create 3D Dataset\n    measurement_system:  None\n    d3_domain:           TWT\n    epsg:                None\n    ...                  ...\n    percentiles:         None\n    coord_scalar:        None\n    coord_scaled:        None\n    dimensions:          None\n    vert_dimension:      None\n    vert_domain:         None</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>xline: 5</li><li>iline: 10</li><li>twt: 1000</li></ul></li><li>Coordinates: (3)<ul><li>xline(xline)int6410 11 12 13 14<pre>array([10, 11, 12, 13, 14])</pre></li><li>iline(iline)int6410 30 50 70 90 110 130 150 170 190<pre>array([ 10,  30,  50,  70,  90, 110, 130, 150, 170, 190])</pre></li><li>twt(twt)int641500 1501 1502 ... 2497 2498 2499<pre>array([1500, 1501, 1502, ..., 2497, 2498, 2499])</pre></li></ul></li><li>Data variables: (1)<ul><li>data(xline, iline, twt)float64-16.22 -16.21 ... -1.803 -1.811<pre>array([[[-16.2166637 , -16.20666387, -16.19666503, ..., -16.73527165,\n         -16.7437957 , -16.75226703],\n        [-14.96704323, -14.9570434 , -14.94704456, ..., -15.48565118,\n         -15.49417523, -15.50264656],\n        [-13.56787701, -13.55787718, -13.54787834, ..., -14.08648496,\n         -14.09500901, -14.10348034],\n        ...,\n        [ -4.86226845,  -4.85226862,  -4.84226978, ...,  -5.3808764 ,\n          -5.38940045,  -5.39787179],\n        [ -2.90121334,  -2.8912135 ,  -2.88121467, ...,  -3.41982129,\n          -3.42834533,  -3.43681667],\n        [ -0.91117026,  -0.90117042,  -0.89117159, ...,  -1.42977821,\n          -1.43830225,  -1.44677359]],\n\n       [[-17.83833007, -17.82833024, -17.8183314 , ..., -18.35693802,\n         -18.36546207, -18.3739334 ],\n        [-16.46374755, -16.45374772, -16.44374888, ..., -16.9823555 ,\n         -16.99087955, -16.99935089],\n        [-14.92466471, -14.91466488, -14.90466604, ..., -15.44327266,\n         -15.45179671, -15.46026805],\n...\n        [ -6.32094899,  -6.31094915,  -6.30095032, ...,  -6.83955694,\n          -6.84808099,  -6.85655232],\n        [ -3.77157734,  -3.7615775 ,  -3.75157867, ...,  -4.29018529,\n          -4.29870933,  -4.30718067],\n        [ -1.18452133,  -1.1745215 ,  -1.16452267, ...,  -1.70312928,\n          -1.71165333,  -1.72012467]],\n\n       [[-22.70332918, -22.69332935, -22.68333051, ..., -23.22193713,\n         -23.23046118, -23.23893251],\n        [-20.95386052, -20.94386069, -20.93386185, ..., -21.47246847,\n         -21.48099252, -21.48946385],\n        [-18.99502781, -18.98502798, -18.97502915, ..., -19.51363576,\n         -19.52215981, -19.53063115],\n        ...,\n        [ -6.80717583,  -6.797176  ,  -6.78717717, ...,  -7.32578378,\n          -7.33430783,  -7.34277917],\n        [ -4.06169867,  -4.05169884,  -4.0417    , ...,  -4.58030662,\n          -4.58883067,  -4.597302  ],\n        [ -1.27563836,  -1.26563852,  -1.25563969, ...,  -1.79424631,\n          -1.80277036,  -1.81124169]]])</pre></li></ul></li><li>Indexes: (3)<ul><li>xlinePandasIndex<pre>PandasIndex(Index([10, 11, 12, 13, 14], dtype='int64', name='xline'))</pre></li><li>ilinePandasIndex<pre>PandasIndex(Index([10, 30, 50, 70, 90, 110, 130, 150, 170, 190], dtype='int64', name='iline'))</pre></li><li>twtPandasIndex<pre>PandasIndex(Index([1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509,\n       ...\n       2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499],\n      dtype='int64', name='twt', length=1000))</pre></li></ul></li><li>Attributes: (17)ns :Nonesample_rate :1text :SEGY-SAK Create 3D Datasetmeasurement_system :Noned3_domain :TWTepsg :Nonecorner_points :[(1, 10), (19, 10), (19, 14), (1, 14), (1, 10)]corner_points_xy :Nonesource_file :Nonesrd :Nonedatatype :Nonepercentiles :Nonecoord_scalar :Nonecoord_scaled :Nonedimensions :Nonevert_dimension :Nonevert_domain :None</li></ul>"},{"location":"examples/example_segysak_basics.html#segy-sak-basics","title":"SEGY-SAK Basics\u00b6","text":"<p>SEGY-SAK offers a number of utilities to create and load seismic data using <code>xarray</code> and <code>segyio</code>. In general SEGY-SAK uses <code>xarray.Dataset</code> to store the data and provides an interface to additional seismic specific functionality by adding the <code>.segysak</code> and <code>.seisio</code> names-spaces to an <code>xarray.Dataset</code> (just <code>dataset</code> from now on). That sounds complicated but let us walk through some examples together.</p>"},{"location":"examples/example_segysak_basics.html#creating-empty-3d-geometry","title":"Creating empty 3D geometry\u00b6","text":"<p>In segysak we use the term <code>seisnc</code> to refer to a <code>dataset</code> which is compatible with segysak's functionality and which has the additional names spaces registered with <code>xarray</code>, for all intensive purposes it is an <code>xarray.Dataset</code> but with defined dimensions and coordinates and some extended functionality. The <code>seisnc</code> dimensions are defined depending on what type of seismic it is (2D, 3D, gathers, etc.)</p> <p>To create an empty 3D instance of <code>seisnc</code> use the <code>create3d_dataset</code>. The function creates a new <code>seisnc</code> based upon definitions for the dimensions, <code>iline</code> numbering, <code>xline</code> numbering and the vertical sampling.</p>"},{"location":"examples/example_segysak_basics.html#dimension-based-selection-and-transformation","title":"Dimension based selection and transformation\u00b6","text":"<p>As you can see from the print out of the previous cell, we have three dimensions in this dataset. They are <code>iline</code>, <code>xline</code> and <code>twt</code> (although the order, number and names might change depending on the make up of our volume). The ordering isn't import to <code>xarray</code> because it uses labels, and accessing data is done using these labels rather than indexing directly into the data like <code>numpy</code>. <code>xarray</code> also makes it further convenient by allowing us to select based on the dimension values using the <code>.sel</code> method with tools for selecting nearest or ranges as well. If necessary you can also select by index using the <code>.isel</code> method.</p>"},{"location":"examples/example_segysak_basics.html#coordinates-selection","title":"Coordinates Selection\u00b6","text":"<p>Usually for seismic the X and Y coordinates labelled <code>cdp_x</code> and <code>cdp_y</code> in seisnc are rotated and scaled relative to the grid geometry and now seisnc dimensions <code>iline</code>, <code>xline</code> and <code>twt</code>. For <code>xarray</code> this means you cannot use the <code>.sel</code> and <code>.isel</code> methods to select data for <code>cdp_x</code> and <code>cdp_y</code>. SEGY-SAK supplies more natural interfaces to access data using X and Y coordinates and this is available through the <code>.segysak.xysel</code> method, covered in other examples.</p>"},{"location":"examples/example_segysak_basics.html#adding-data-to-an-empty-seisnc","title":"Adding data to an empty seisnc\u00b6","text":"<p>Because <code>xarray</code> needs to understand the dimensions of any data you assign it must be explicitly communicated either via labels or creating an <code>xarray.DataArray</code> first.</p>"},{"location":"examples/example_segysak_basics.html#other-useful-methods","title":"Other Useful Methods\u00b6","text":""},{"location":"examples/example_segysak_dask.html","title":"Using dask","text":"In\u00a0[1]: Copied! <pre>import warnings\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import warnings  warnings.filterwarnings(\"ignore\") In\u00a0[2]: Copied! <pre>import numpy as np\nfrom segysak import open_seisnc, segy\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import numpy as np from segysak import open_seisnc, segy  import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[3]: Copied! <pre>from dask.distributed import Client\n\nclient = Client()\nclient\n</pre> from dask.distributed import Client  client = Client() client Out[3]: Client <p>Client-67525284-2a62-11ef-885a-d1ab993961eb</p> Connection method: Cluster object Cluster type: distributed.LocalCluster Dashboard:  http://127.0.0.1:8787/status Cluster Info LocalCluster <p>e55dfa3a</p> Dashboard: http://127.0.0.1:8787/status Workers: 4                  Total threads: 4                  Total memory: 15.61 GiB                  Status: running Using processes: True Scheduler Info Scheduler <p>Scheduler-e64ca168-8b43-425d-a40d-d3b0c8f0aa70</p> Comm: tcp://127.0.0.1:45097                      Workers: 4                      Dashboard: http://127.0.0.1:8787/status Total threads: 4                      Started: Just now                      Total memory: 15.61 GiB                      Workers Worker: 0 Comm:  tcp://127.0.0.1:37991                          Total threads:  1                          Dashboard:  http://127.0.0.1:39195/status Memory:  3.90 GiB                          Nanny:  tcp://127.0.0.1:41499                          Local directory:  /tmp/dask-scratch-space/worker-1_nx7uq3                          Worker: 1 Comm:  tcp://127.0.0.1:34231                          Total threads:  1                          Dashboard:  http://127.0.0.1:37155/status Memory:  3.90 GiB                          Nanny:  tcp://127.0.0.1:40575                          Local directory:  /tmp/dask-scratch-space/worker-x_0r1h_4                          Worker: 2 Comm:  tcp://127.0.0.1:39543                          Total threads:  1                          Dashboard:  http://127.0.0.1:33943/status Memory:  3.90 GiB                          Nanny:  tcp://127.0.0.1:42991                          Local directory:  /tmp/dask-scratch-space/worker-qfm7jxfq                          Worker: 3 Comm:  tcp://127.0.0.1:43379                          Total threads:  1                          Dashboard:  http://127.0.0.1:42931/status Memory:  3.90 GiB                          Nanny:  tcp://127.0.0.1:36679                          Local directory:  /tmp/dask-scratch-space/worker-oue78iaq                          <p>We can also scale the cluster to be a bit smaller.</p> In\u00a0[4]: Copied! <pre>client.cluster.scale(2, memory=\"0.5gb\")\nclient\n</pre> client.cluster.scale(2, memory=\"0.5gb\") client Out[4]: Client <p>Client-67525284-2a62-11ef-885a-d1ab993961eb</p> Connection method: Cluster object Cluster type: distributed.LocalCluster Dashboard:  http://127.0.0.1:8787/status Cluster Info LocalCluster <p>e55dfa3a</p> Dashboard: http://127.0.0.1:8787/status Workers: 3                  Total threads: 3                  Total memory: 11.70 GiB                  Status: running Using processes: True Scheduler Info Scheduler <p>Scheduler-e64ca168-8b43-425d-a40d-d3b0c8f0aa70</p> Comm: tcp://127.0.0.1:45097                      Workers: 4                      Dashboard: http://127.0.0.1:8787/status Total threads: 4                      Started: Just now                      Total memory: 15.61 GiB                      Workers Worker: 0 Comm:  tcp://127.0.0.1:37991                          Total threads:  1                          Dashboard:  http://127.0.0.1:39195/status Memory:  3.90 GiB                          Nanny:  tcp://127.0.0.1:41499                          Local directory:  /tmp/dask-scratch-space/worker-1_nx7uq3                          Worker: 1 Comm:  tcp://127.0.0.1:34231                          Total threads:  1                          Dashboard:  http://127.0.0.1:37155/status Memory:  3.90 GiB                          Nanny:  tcp://127.0.0.1:40575                          Local directory:  /tmp/dask-scratch-space/worker-x_0r1h_4                          Worker: 2 Comm:  tcp://127.0.0.1:39543                          Total threads:  1                          Dashboard:  http://127.0.0.1:33943/status Memory:  3.90 GiB                          Nanny:  tcp://127.0.0.1:42991                          Local directory:  /tmp/dask-scratch-space/worker-qfm7jxfq                          Worker: 3 Comm:  tcp://127.0.0.1:43379                          Total threads:  1                          Dashboard:  http://127.0.0.1:42931/status Memory:  3.90 GiB                          Nanny:  tcp://127.0.0.1:36679                          Local directory:  /tmp/dask-scratch-space/worker-oue78iaq                          In\u00a0[5]: Copied! <pre>segy_file = \"data/volve10r12-full-twt-sub3d.sgy\"\nseisnc_file = \"data/volve10r12-full-twt-sub3d.seisnc\"\nsegy.segy_converter(segy_file, seisnc_file, iline=189, xline=193, cdp_x=181, cdp_y=185)\n</pre> segy_file = \"data/volve10r12-full-twt-sub3d.sgy\" seisnc_file = \"data/volve10r12-full-twt-sub3d.seisnc\" segy.segy_converter(segy_file, seisnc_file, iline=189, xline=193, cdp_x=181, cdp_y=185) <pre>header_loaded\nis_3d\nFast direction is CROSSLINE_3D\n</pre> <p>By specifying the chunks argument to the <code>open_seisnc</code> command we can ask dask to fetch the data in chunks of size n. In this example the <code>iline</code> dimension will be chunked in groups of 100. The valid arguments to chunks depends on the dataset but any dimension can be used.</p> <p>Even though the seis of the dataset is <code>2.14GB</code> it hasn't yet been loaded into memory, not will <code>dask</code> load it entirely unless the operation demands it.</p> In\u00a0[6]: Copied! <pre>seisnc = open_seisnc(\"data/volve10r12-full-twt-sub3d.seisnc\", chunks={\"iline\": 100})\nseisnc.seis.humanbytes\n</pre> seisnc = open_seisnc(\"data/volve10r12-full-twt-sub3d.seisnc\", chunks={\"iline\": 100}) seisnc.seis.humanbytes Out[6]: <pre>'40.05 MB'</pre> <p>Lets see what our dataset looks like. See that the variables are <code>dask.array</code>. This means they are references to the on disk data. The dimensions must be loaded so <code>dask</code> knows how to manage your dataset.</p> In\u00a0[7]: Copied! <pre>seisnc\n</pre> seisnc Out[7]: <pre>&lt;xarray.Dataset&gt; Size: 42MB\nDimensions:  (iline: 61, xline: 202, twt: 850)\nCoordinates:\n  * iline    (iline) int16 122B 10090 10091 10092 10093 ... 10148 10149 10150\n  * xline    (xline) int16 404B 2150 2151 2152 2153 2154 ... 2348 2349 2350 2351\n  * twt      (twt) float64 7kB 4.0 8.0 12.0 16.0 ... 3.392e+03 3.396e+03 3.4e+03\n    cdp_x    (iline, xline) float32 49kB dask.array&lt;chunksize=(61, 202), meta=np.ndarray&gt;\n    cdp_y    (iline, xline) float32 49kB dask.array&lt;chunksize=(61, 202), meta=np.ndarray&gt;\nData variables:\n    data     (iline, xline, twt) float32 42MB dask.array&lt;chunksize=(61, 202, 850), meta=np.ndarray&gt;\nAttributes: (12/17)\n    sample_rate:         4.0\n    text:                C 1 SEGY OUTPUT FROM Petrel 2017.2 Saturday, June 06...\n    measurement_system:  m\n    source_file:         volve10r12-full-twt-sub3d.sgy\n    percentiles:         [-6.97198262e+00 -6.52054033e+00 -1.49142619e+00 -5....\n    coord_scalar:        -100.0\n    ...                  ...\n    srd:                 None\n    datatype:            None\n    coord_scaled:        None\n    dimensions:          None\n    vert_dimension:      None\n    vert_domain:         None</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>iline: 61</li><li>xline: 202</li><li>twt: 850</li></ul></li><li>Coordinates: (5)<ul><li>iline(iline)int1610090 10091 10092 ... 10149 10150<pre>array([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150], dtype=int16)</pre></li><li>xline(xline)int162150 2151 2152 ... 2349 2350 2351<pre>array([2150, 2151, 2152, ..., 2349, 2350, 2351], dtype=int16)</pre></li><li>twt(twt)float644.0 8.0 12.0 ... 3.396e+03 3.4e+03<pre>array([   4.,    8.,   12., ..., 3392., 3396., 3400.])</pre></li><li>cdp_x(iline, xline)float32dask.array&lt;chunksize=(61, 202), meta=np.ndarray&gt;  Array   Chunk   Bytes   48.13 kiB   48.13 kiB   Shape   (61, 202)   (61, 202)   Dask graph   1 chunks in 2 graph layers   Data type   float32 numpy.ndarray  202 61 </li><li>cdp_y(iline, xline)float32dask.array&lt;chunksize=(61, 202), meta=np.ndarray&gt;  Array   Chunk   Bytes   48.13 kiB   48.13 kiB   Shape   (61, 202)   (61, 202)   Dask graph   1 chunks in 2 graph layers   Data type   float32 numpy.ndarray  202 61 </li></ul></li><li>Data variables: (1)<ul><li>data(iline, xline, twt)float32dask.array&lt;chunksize=(61, 202, 850), meta=np.ndarray&gt;  Array   Chunk   Bytes   39.95 MiB   39.95 MiB   Shape   (61, 202, 850)   (61, 202, 850)   Dask graph   1 chunks in 2 graph layers   Data type   float32 numpy.ndarray  850 202 61 </li></ul></li><li>Indexes: (3)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150],\n      dtype='int16', name='iline'))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159,\n       ...\n       2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351],\n      dtype='int16', name='xline', length=202))</pre></li><li>twtPandasIndex<pre>PandasIndex(Index([   4.0,    8.0,   12.0,   16.0,   20.0,   24.0,   28.0,   32.0,   36.0,\n         40.0,\n       ...\n       3364.0, 3368.0, 3372.0, 3376.0, 3380.0, 3384.0, 3388.0, 3392.0, 3396.0,\n       3400.0],\n      dtype='float64', name='twt', length=850))</pre></li></ul></li><li>Attributes: (17)sample_rate :4.0text :C 1 SEGY OUTPUT FROM Petrel 2017.2 Saturday, June 06 2020 10:15:00               C 2 Name: ST10010ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534 \u00ddCro C 3                                                                              C 4 First inline: 10090  Last inline: 10150                                      C 5 First xline:  2150   Last xline:  2351                                       C 6 CRS: ED50-UTM31 (\"MENTOR:ED50-UTM31:European 1950 Based UTM, Zone 31 North,  C 7 X min: 433955.09 max: 436589.56 delta: 2634.47                               C 8 Y min: 6477439.46 max: 6478790.23 delta: 1350.77                             C 9 Time min: -3402.00 max: -2.00 delta: 3400.00                                 C10 Lat min: 58.25'52.8804\"N max: 58.26'37.9493\"N delta: 0.00'45.0689\"           C11 Long min: 1.52'7.1906\"E max: 1.54'50.9616\"E delta: 0.02'43.7710\"             C12 Trace min: -3400.00 max: -4.00 delta: 3396.00                                C13 Seismic (template) min: -58.55 max: 54.55 delta: 113.10                      C14 Amplitude (data) min: -58.55 max: 54.55 delta: 113.10                        C15 Trace sample format: IEEE floating point                                     C16 Coordinate scale factor: 100.00000                                           C17                                                                              C18 Binary header locations:                                                     C19 Sample interval             : bytes 17-18                                    C20 Number of samples per trace : bytes 21-22                                    C21 Trace date format           : bytes 25-26                                    C22                                                                              C23 Trace header locations:                                                      C24 Inline number               : bytes 5-8                                      C25 Xline number                : bytes 21-24                                    C26 Coordinate scale factor     : bytes 71-72                                    C27 X coordinate                : bytes 73-76                                    C28 Y coordinate                : bytes 77-80                                    C29 Trace start time/depth      : bytes 109-110                                  C30 Number of samples per trace : bytes 115-116                                  C31 Sample interval             : bytes 117-118                                  C32                                                                              C33                                                                              C34                                                                              C35                                                                              C36                                                                              C37                                                                              C38                                                                              C39                                                                              C40 END EBCDIC                                                                  measurement_system :msource_file :volve10r12-full-twt-sub3d.sgypercentiles :[-6.97198262e+00 -6.52054033e+00 -1.49142619e+00 -5.19284718e-05   1.44614165e+00  6.34640887e+00  6.88808892e+00]coord_scalar :-100.0ns :Noned3_domain :Noneepsg :Nonecorner_points :Nonecorner_points_xy :Nonesrd :Nonedatatype :Nonecoord_scaled :Nonedimensions :Nonevert_dimension :Nonevert_domain :None</li></ul> In\u00a0[8]: Copied! <pre>mean = seisnc.data.mean()\nmean\n</pre> mean = seisnc.data.mean() mean Out[8]: <pre>&lt;xarray.DataArray 'data' ()&gt; Size: 4B\ndask.array&lt;mean_agg-aggregate, shape=(), dtype=float32, chunksize=(), chunktype=numpy.ndarray&gt;</pre>xarray.DataArray'data'<ul><li>dask.array&lt;chunksize=(), meta=np.ndarray&gt;  Array   Chunk   Bytes   4 B   4 B   Shape   ()   ()   Dask graph   1 chunks in 4 graph layers   Data type   float32 numpy.ndarray  </li><li>Coordinates: (0)<ul></ul></li><li>Indexes: (0)<ul></ul></li><li>Attributes: (0)</li></ul> <p>Whoa-oh, the mean is what? Yeah, <code>dask</code> won't calculate anything until you ask it to. This means you can string computations together into a task graph for lazy evaluation. To get the mean try this</p> In\u00a0[9]: Copied! <pre>mean.compute().values\n</pre> mean.compute().values Out[9]: <pre>array(-7.317369e-05, dtype=float32)</pre> In\u00a0[10]: Copied! <pre>fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))\n\niline = seisnc.sel(iline=10100).transpose(\"twt\", \"xline\").data\nxline = seisnc.sel(xline=2349).transpose(\"twt\", \"iline\").data\nzslice = seisnc.sel(twt=2900, method=\"nearest\").transpose(\"iline\", \"xline\").data\n\nq = iline.quantile([0, 0.001, 0.5, 0.999, 1]).values\nrq = np.max(np.abs([q[1], q[-2]]))\n\niline.plot(robust=True, ax=axs[0, 0], yincrease=False)\nxline.plot(robust=True, ax=axs[0, 1], yincrease=False)\nzslice.plot(robust=True, ax=axs[0, 2])\n\nimshow_kwargs = dict(\n    cmap=\"seismic\", aspect=\"auto\", vmin=-rq, vmax=rq, interpolation=\"bicubic\"\n)\n\naxs[1, 0].imshow(iline.values, **imshow_kwargs)\naxs[1, 0].set_title(\"iline\")\naxs[1, 1].imshow(xline.values, **imshow_kwargs)\naxs[1, 1].set_title(\"xline\")\naxs[1, 2].imshow(zslice.values, origin=\"lower\", **imshow_kwargs)\naxs[1, 2].set_title(\"twt\")\n</pre> fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))  iline = seisnc.sel(iline=10100).transpose(\"twt\", \"xline\").data xline = seisnc.sel(xline=2349).transpose(\"twt\", \"iline\").data zslice = seisnc.sel(twt=2900, method=\"nearest\").transpose(\"iline\", \"xline\").data  q = iline.quantile([0, 0.001, 0.5, 0.999, 1]).values rq = np.max(np.abs([q[1], q[-2]]))  iline.plot(robust=True, ax=axs[0, 0], yincrease=False) xline.plot(robust=True, ax=axs[0, 1], yincrease=False) zslice.plot(robust=True, ax=axs[0, 2])  imshow_kwargs = dict(     cmap=\"seismic\", aspect=\"auto\", vmin=-rq, vmax=rq, interpolation=\"bicubic\" )  axs[1, 0].imshow(iline.values, **imshow_kwargs) axs[1, 0].set_title(\"iline\") axs[1, 1].imshow(xline.values, **imshow_kwargs) axs[1, 1].set_title(\"xline\") axs[1, 2].imshow(zslice.values, origin=\"lower\", **imshow_kwargs) axs[1, 2].set_title(\"twt\") Out[10]: <pre>Text(0.5, 1.0, 'twt')</pre>"},{"location":"examples/example_segysak_dask.html#using-dask","title":"Using dask\u00b6","text":"<p>dask is a Python package built upon the scientific stack to enable scalling of Python through interactive sessions to multi-core and multi-node.</p> <p>Of particular relevance to SEGY-SAK is that <code>xrray.Dataset</code> loads naturally into <code>dask</code>.</p>"},{"location":"examples/example_segysak_dask.html#imports-and-setup","title":"Imports and Setup\u00b6","text":"<p>Here we import the plotting tools, <code>numpy</code> and setup the <code>dask.Client</code> which will auto start a <code>localcluster</code>. Printing the client returns details about the dashboard link and resources.</p>"},{"location":"examples/example_segysak_dask.html#lazy-loading-from-seisnc-using-chunking","title":"Lazy loading from SEISNC using chunking\u00b6","text":"<p>If your data is in SEG-Y to use dask it must be converted to SEISNC. If you do this with the CLI it only need happen once.</p>"},{"location":"examples/example_segysak_dask.html#operations-on-seisnc-using-dask","title":"Operations on SEISNC using <code>dask</code>\u00b6","text":"<p>In this simple example we calculate the mean, of the entire cube. If you check the dashboard (when running this example yourself). You can see the task graph and task stream execution.</p>"},{"location":"examples/example_segysak_dask.html#plotting-with-dask","title":"Plotting with <code>dask</code>\u00b6","text":"<p>The lazy loading of data means we can plot what we want using <code>xarray</code> style slicing and <code>dask</code> will fetch only the data we need.</p>"},{"location":"examples/example_segysak_segy_vectorisation.html","title":"SEG-Y to Vector DataFrames and Back","text":"In\u00a0[2]: Copied! <pre>import pathlib\nimport xarray as xr\nfrom IPython.display import display\n\nvolve_3d_path = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\")\nprint(\"3D\", volve_3d_path.exists())\n\nvolve_3d = xr.open_dataset(volve_3d_path, dim_byte_fields={'iline': 5, 'xline': 21}, extra_byte_fields={'cdp_x': 73, 'cdp_y': 77})\n</pre> import pathlib import xarray as xr from IPython.display import display  volve_3d_path = pathlib.Path(\"data/volve10r12-full-twt-sub3d.sgy\") print(\"3D\", volve_3d_path.exists())  volve_3d = xr.open_dataset(volve_3d_path, dim_byte_fields={'iline': 5, 'xline': 21}, extra_byte_fields={'cdp_x': 73, 'cdp_y': 77}) <pre>3D True\n</pre> In\u00a0[3]: Copied! <pre>volve_3d_df = volve_3d.to_dataframe()\ndisplay(volve_3d_df)\n</pre> volve_3d_df = volve_3d.to_dataframe() display(volve_3d_df) cdp_x cdp_y data iline xline samples 10090 2150 4.0 43640052 647744704 0.020575 8.0 43640052 647744704 0.022041 12.0 43640052 647744704 0.019659 16.0 43640052 647744704 0.025421 20.0 43640052 647744704 0.025436 ... ... ... ... ... ... 10150 2351 3384.0 43414413 647878266 0.000000 3388.0 43414413 647878266 0.000000 3392.0 43414413 647878266 0.000000 3396.0 43414413 647878266 0.000000 3400.0 43414413 647878266 0.000000 <p>10473700 rows \u00d7 3 columns</p> <p>We can remove the multi-index by resetting the index of the DataFrame. Vectorized workflows such as machine learning can then be easily applied to the DataFrame.</p> In\u00a0[4]: Copied! <pre>volve_3d_df_reindex = volve_3d_df.reset_index()\ndisplay(volve_3d_df_reindex)\n</pre> volve_3d_df_reindex = volve_3d_df.reset_index() display(volve_3d_df_reindex) iline xline samples cdp_x cdp_y data 0 10090 2150 4.0 43640052 647744704 0.020575 1 10090 2150 8.0 43640052 647744704 0.022041 2 10090 2150 12.0 43640052 647744704 0.019659 3 10090 2150 16.0 43640052 647744704 0.025421 4 10090 2150 20.0 43640052 647744704 0.025436 ... ... ... ... ... ... ... 10473695 10150 2351 3384.0 43414413 647878266 0.000000 10473696 10150 2351 3388.0 43414413 647878266 0.000000 10473697 10150 2351 3392.0 43414413 647878266 0.000000 10473698 10150 2351 3396.0 43414413 647878266 0.000000 10473699 10150 2351 3400.0 43414413 647878266 0.000000 <p>10473700 rows \u00d7 6 columns</p> In\u00a0[5]: Copied! <pre>volve_3d_df_multi = volve_3d_df_reindex.set_index([\"iline\", \"xline\", \"samples\"])\ndisplay(volve_3d_df_multi)\nvolve_3d_ds = volve_3d_df_multi.to_xarray()\ndisplay(volve_3d_ds)\n</pre> volve_3d_df_multi = volve_3d_df_reindex.set_index([\"iline\", \"xline\", \"samples\"]) display(volve_3d_df_multi) volve_3d_ds = volve_3d_df_multi.to_xarray() display(volve_3d_ds) cdp_x cdp_y data iline xline samples 10090 2150 4.0 43640052 647744704 0.020575 8.0 43640052 647744704 0.022041 12.0 43640052 647744704 0.019659 16.0 43640052 647744704 0.025421 20.0 43640052 647744704 0.025436 ... ... ... ... ... ... 10150 2351 3384.0 43414413 647878266 0.000000 3388.0 43414413 647878266 0.000000 3392.0 43414413 647878266 0.000000 3396.0 43414413 647878266 0.000000 3400.0 43414413 647878266 0.000000 <p>10473700 rows \u00d7 3 columns</p> <pre>&lt;xarray.Dataset&gt; Size: 126MB\nDimensions:  (iline: 61, xline: 202, samples: 850)\nCoordinates:\n  * iline    (iline) int16 122B 10090 10091 10092 10093 ... 10148 10149 10150\n  * xline    (xline) int16 404B 2150 2151 2152 2153 2154 ... 2348 2349 2350 2351\n  * samples  (samples) float32 3kB 4.0 8.0 12.0 ... 3.392e+03 3.396e+03 3.4e+03\nData variables:\n    cdp_x    (iline, xline, samples) int32 42MB 43640052 43640052 ... 43414413\n    cdp_y    (iline, xline, samples) int32 42MB 647744704 ... 647878266\n    data     (iline, xline, samples) float32 42MB 0.02057 0.02204 ... 0.0 0.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>iline: 61</li><li>xline: 202</li><li>samples: 850</li></ul></li><li>Coordinates: (3)<ul><li>iline(iline)int1610090 10091 10092 ... 10149 10150<pre>array([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150], dtype=int16)</pre></li><li>xline(xline)int162150 2151 2152 ... 2349 2350 2351<pre>array([2150, 2151, 2152, ..., 2349, 2350, 2351], dtype=int16)</pre></li><li>samples(samples)float324.0 8.0 12.0 ... 3.396e+03 3.4e+03<pre>array([   4.,    8.,   12., ..., 3392., 3396., 3400.], dtype=float32)</pre></li></ul></li><li>Data variables: (3)<ul><li>cdp_x(iline, xline, samples)int3243640052 43640052 ... 43414413<pre>array([[[43640052, 43640052, 43640052, ..., 43640052, 43640052,\n         43640052],\n        [43638839, 43638839, 43638839, ..., 43638839, 43638839,\n         43638839],\n        [43637626, 43637626, 43637626, ..., 43637626, 43637626,\n         43637626],\n        ...,\n        [43398692, 43398692, 43398692, ..., 43398692, 43398692,\n         43398692],\n        [43397480, 43397480, 43397480, ..., 43397480, 43397480,\n         43397480],\n        [43396267, 43396267, 43396267, ..., 43396267, 43396267,\n         43396267]],\n\n       [[43640354, 43640354, 43640354, ..., 43640354, 43640354,\n         43640354],\n        [43639141, 43639141, 43639141, ..., 43639141, 43639141,\n         43639141],\n        [43637928, 43637928, 43637928, ..., 43637928, 43637928,\n         43637928],\n...\n        [43416536, 43416536, 43416536, ..., 43416536, 43416536,\n         43416536],\n        [43415323, 43415323, 43415323, ..., 43415323, 43415323,\n         43415323],\n        [43414110, 43414110, 43414110, ..., 43414110, 43414110,\n         43414110]],\n\n       [[43658198, 43658198, 43658198, ..., 43658198, 43658198,\n         43658198],\n        [43656985, 43656985, 43656985, ..., 43656985, 43656985,\n         43656985],\n        [43655772, 43655772, 43655772, ..., 43655772, 43655772,\n         43655772],\n        ...,\n        [43416839, 43416839, 43416839, ..., 43416839, 43416839,\n         43416839],\n        [43415626, 43415626, 43415626, ..., 43415626, 43415626,\n         43415626],\n        [43414413, 43414413, 43414413, ..., 43414413, 43414413,\n         43414413]]], dtype=int32)</pre></li><li>cdp_y(iline, xline, samples)int32647744704 647744704 ... 647878266<pre>array([[[647744704, 647744704, 647744704, ..., 647744704, 647744704,\n         647744704],\n        [647745006, 647745006, 647745006, ..., 647745006, 647745006,\n         647745006],\n        [647745309, 647745309, 647745309, ..., 647745309, 647745309,\n         647745309],\n        ...,\n        [647804889, 647804889, 647804889, ..., 647804889, 647804889,\n         647804889],\n        [647805192, 647805192, 647805192, ..., 647805192, 647805192,\n         647805192],\n        [647805494, 647805494, 647805494, ..., 647805494, 647805494,\n         647805494]],\n\n       [[647745917, 647745917, 647745917, ..., 647745917, 647745917,\n         647745917],\n        [647746219, 647746219, 647746219, ..., 647746219, 647746219,\n         647746219],\n        [647746522, 647746522, 647746522, ..., 647746522, 647746522,\n         647746522],\n...\n        [647876448, 647876448, 647876448, ..., 647876448, 647876448,\n         647876448],\n        [647876751, 647876751, 647876751, ..., 647876751, 647876751,\n         647876751],\n        [647877053, 647877053, 647877053, ..., 647877053, 647877053,\n         647877053]],\n\n       [[647817476, 647817476, 647817476, ..., 647817476, 647817476,\n         647817476],\n        [647817778, 647817778, 647817778, ..., 647817778, 647817778,\n         647817778],\n        [647818081, 647818081, 647818081, ..., 647818081, 647818081,\n         647818081],\n        ...,\n        [647877661, 647877661, 647877661, ..., 647877661, 647877661,\n         647877661],\n        [647877963, 647877963, 647877963, ..., 647877963, 647877963,\n         647877963],\n        [647878266, 647878266, 647878266, ..., 647878266, 647878266,\n         647878266]]], dtype=int32)</pre></li><li>data(iline, xline, samples)float320.02057 0.02204 0.01966 ... 0.0 0.0<pre>array([[[ 2.05745399e-02,  2.20407024e-02,  1.96589418e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 1.23256370e-02,  1.59417503e-02,  1.38800517e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [-1.63372169e-05,  4.92771342e-03,  3.24293785e-03, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        ...,\n        [ 5.47898412e-02,  5.22681139e-02,  5.00054434e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 6.27492070e-02,  6.00568764e-02,  5.75365834e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 6.91987872e-02,  6.67222738e-02,  6.40410781e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n\n       [[-3.46487835e-02, -3.38801444e-02, -3.20093483e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [-4.10056897e-02, -4.02579159e-02, -3.83855253e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [-5.01492284e-02, -4.94211540e-02, -4.75146063e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n...\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [-8.89039040e-03, -9.03325155e-03, -8.77474993e-03, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [-8.80905986e-03, -9.18869302e-03, -8.81006569e-03, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n\n       [[ 2.17335932e-02,  1.93170868e-02,  2.07530260e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 2.12477669e-02,  1.89855732e-02,  2.04903930e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 2.13435888e-02,  1.91475898e-02,  2.07546689e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        ...,\n        [ 6.19068742e-03,  5.15908375e-03,  6.23933598e-03, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 7.53349811e-03,  6.49160147e-03,  7.51641765e-03, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 9.05360654e-03,  7.92511553e-03,  8.82008299e-03, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n      dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150],\n      dtype='int16', name='iline'))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159,\n       ...\n       2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351],\n      dtype='int16', name='xline', length=202))</pre></li><li>samplesPandasIndex<pre>PandasIndex(Index([   4.0,    8.0,   12.0,   16.0,   20.0,   24.0,   28.0,   32.0,   36.0,\n         40.0,\n       ...\n       3364.0, 3368.0, 3372.0, 3376.0, 3380.0, 3384.0, 3388.0, 3392.0, 3396.0,\n       3400.0],\n      dtype='float32', name='samples', length=850))</pre></li></ul></li><li>Attributes: (0)</li></ul> <p>The resulting dataset requires some changes to make it compatible again for export to SEGY. Firstly, the attributes need to be set. The simplest way is to copy these from the original SEG-Y input. Otherwise they can be set manually. <code>segysak</code> specifically needs the <code>sample_rate</code> and the <code>coord_scalar</code> attributes.</p> In\u00a0[6]: Copied! <pre>volve_3d_ds.attrs = volve_3d.attrs\ndisplay(volve_3d_ds.attrs)\n</pre> volve_3d_ds.attrs = volve_3d.attrs display(volve_3d_ds.attrs) <pre>{'seisnc': '{\"coord_scalar\": -100.0, \"coord_scaled\": false}'}</pre> <p>The <code>cdp_x</code> and <code>cdp_y</code> positions must be reduced to 2D along the vertical axis \"twt\" and set as coordinates.</p> In\u00a0[7]: Copied! <pre>volve_3d_ds[\"cdp_x\"]\n</pre> volve_3d_ds[\"cdp_x\"] Out[7]: <pre>&lt;xarray.DataArray 'cdp_x' (iline: 61, xline: 202, samples: 850)&gt; Size: 42MB\narray([[[43640052, 43640052, 43640052, ..., 43640052, 43640052,\n         43640052],\n        [43638839, 43638839, 43638839, ..., 43638839, 43638839,\n         43638839],\n        [43637626, 43637626, 43637626, ..., 43637626, 43637626,\n         43637626],\n        ...,\n        [43398692, 43398692, 43398692, ..., 43398692, 43398692,\n         43398692],\n        [43397480, 43397480, 43397480, ..., 43397480, 43397480,\n         43397480],\n        [43396267, 43396267, 43396267, ..., 43396267, 43396267,\n         43396267]],\n\n       [[43640354, 43640354, 43640354, ..., 43640354, 43640354,\n         43640354],\n        [43639141, 43639141, 43639141, ..., 43639141, 43639141,\n         43639141],\n        [43637928, 43637928, 43637928, ..., 43637928, 43637928,\n         43637928],\n...\n        [43416536, 43416536, 43416536, ..., 43416536, 43416536,\n         43416536],\n        [43415323, 43415323, 43415323, ..., 43415323, 43415323,\n         43415323],\n        [43414110, 43414110, 43414110, ..., 43414110, 43414110,\n         43414110]],\n\n       [[43658198, 43658198, 43658198, ..., 43658198, 43658198,\n         43658198],\n        [43656985, 43656985, 43656985, ..., 43656985, 43656985,\n         43656985],\n        [43655772, 43655772, 43655772, ..., 43655772, 43655772,\n         43655772],\n        ...,\n        [43416839, 43416839, 43416839, ..., 43416839, 43416839,\n         43416839],\n        [43415626, 43415626, 43415626, ..., 43415626, 43415626,\n         43415626],\n        [43414413, 43414413, 43414413, ..., 43414413, 43414413,\n         43414413]]], dtype=int32)\nCoordinates:\n  * iline    (iline) int16 122B 10090 10091 10092 10093 ... 10148 10149 10150\n  * xline    (xline) int16 404B 2150 2151 2152 2153 2154 ... 2348 2349 2350 2351\n  * samples  (samples) float32 3kB 4.0 8.0 12.0 ... 3.392e+03 3.396e+03 3.4e+03</pre>xarray.DataArray'cdp_x'<ul><li>iline: 61</li><li>xline: 202</li><li>samples: 850</li></ul><ul><li>43640052 43640052 43640052 43640052 ... 43414413 43414413 43414413<pre>array([[[43640052, 43640052, 43640052, ..., 43640052, 43640052,\n         43640052],\n        [43638839, 43638839, 43638839, ..., 43638839, 43638839,\n         43638839],\n        [43637626, 43637626, 43637626, ..., 43637626, 43637626,\n         43637626],\n        ...,\n        [43398692, 43398692, 43398692, ..., 43398692, 43398692,\n         43398692],\n        [43397480, 43397480, 43397480, ..., 43397480, 43397480,\n         43397480],\n        [43396267, 43396267, 43396267, ..., 43396267, 43396267,\n         43396267]],\n\n       [[43640354, 43640354, 43640354, ..., 43640354, 43640354,\n         43640354],\n        [43639141, 43639141, 43639141, ..., 43639141, 43639141,\n         43639141],\n        [43637928, 43637928, 43637928, ..., 43637928, 43637928,\n         43637928],\n...\n        [43416536, 43416536, 43416536, ..., 43416536, 43416536,\n         43416536],\n        [43415323, 43415323, 43415323, ..., 43415323, 43415323,\n         43415323],\n        [43414110, 43414110, 43414110, ..., 43414110, 43414110,\n         43414110]],\n\n       [[43658198, 43658198, 43658198, ..., 43658198, 43658198,\n         43658198],\n        [43656985, 43656985, 43656985, ..., 43656985, 43656985,\n         43656985],\n        [43655772, 43655772, 43655772, ..., 43655772, 43655772,\n         43655772],\n        ...,\n        [43416839, 43416839, 43416839, ..., 43416839, 43416839,\n         43416839],\n        [43415626, 43415626, 43415626, ..., 43415626, 43415626,\n         43415626],\n        [43414413, 43414413, 43414413, ..., 43414413, 43414413,\n         43414413]]], dtype=int32)</pre></li><li>Coordinates: (3)<ul><li>iline(iline)int1610090 10091 10092 ... 10149 10150<pre>array([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150], dtype=int16)</pre></li><li>xline(xline)int162150 2151 2152 ... 2349 2350 2351<pre>array([2150, 2151, 2152, ..., 2349, 2350, 2351], dtype=int16)</pre></li><li>samples(samples)float324.0 8.0 12.0 ... 3.396e+03 3.4e+03<pre>array([   4.,    8.,   12., ..., 3392., 3396., 3400.], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150],\n      dtype='int16', name='iline'))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159,\n       ...\n       2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351],\n      dtype='int16', name='xline', length=202))</pre></li><li>samplesPandasIndex<pre>PandasIndex(Index([   4.0,    8.0,   12.0,   16.0,   20.0,   24.0,   28.0,   32.0,   36.0,\n         40.0,\n       ...\n       3364.0, 3368.0, 3372.0, 3376.0, 3380.0, 3384.0, 3388.0, 3392.0, 3396.0,\n       3400.0],\n      dtype='float32', name='samples', length=850))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[8]: Copied! <pre>volve_3d_ds[\"cdp_x\"] = volve_3d_ds[\"cdp_x\"].mean(dim=[\"samples\"])\nvolve_3d_ds[\"cdp_y\"] = volve_3d_ds[\"cdp_y\"].mean(dim=[\"samples\"])\nvolve_3d_ds = volve_3d_ds.set_coords([\"cdp_x\", \"cdp_y\"])\nvolve_3d_ds\n</pre> volve_3d_ds[\"cdp_x\"] = volve_3d_ds[\"cdp_x\"].mean(dim=[\"samples\"]) volve_3d_ds[\"cdp_y\"] = volve_3d_ds[\"cdp_y\"].mean(dim=[\"samples\"]) volve_3d_ds = volve_3d_ds.set_coords([\"cdp_x\", \"cdp_y\"]) volve_3d_ds Out[8]: <pre>&lt;xarray.Dataset&gt; Size: 42MB\nDimensions:  (iline: 61, xline: 202, samples: 850)\nCoordinates:\n    cdp_x    (iline, xline) float64 99kB 4.364e+07 4.364e+07 ... 4.341e+07\n    cdp_y    (iline, xline) float64 99kB 6.477e+08 6.477e+08 ... 6.479e+08\n  * iline    (iline) int16 122B 10090 10091 10092 10093 ... 10148 10149 10150\n  * xline    (xline) int16 404B 2150 2151 2152 2153 2154 ... 2348 2349 2350 2351\n  * samples  (samples) float32 3kB 4.0 8.0 12.0 ... 3.392e+03 3.396e+03 3.4e+03\nData variables:\n    data     (iline, xline, samples) float32 42MB 0.02057 0.02204 ... 0.0 0.0\nAttributes:\n    seisnc:   {\"coord_scalar\": -100.0, \"coord_scaled\": false}</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>iline: 61</li><li>xline: 202</li><li>samples: 850</li></ul></li><li>Coordinates: (5)<ul><li>cdp_x(iline, xline)float644.364e+07 4.364e+07 ... 4.341e+07<pre>array([[43640052., 43638839., 43637626., ..., 43398692., 43397480.,\n        43396267.],\n       [43640354., 43639141., 43637928., ..., 43398995., 43397782.,\n        43396569.],\n       [43640657., 43639444., 43638231., ..., 43399297., 43398084.,\n        43396872.],\n       ...,\n       [43657593., 43656380., 43655167., ..., 43416234., 43415021.,\n        43413808.],\n       [43657895., 43656683., 43655470., ..., 43416536., 43415323.,\n        43414110.],\n       [43658198., 43656985., 43655772., ..., 43416839., 43415626.,\n        43414413.]])</pre></li><li>cdp_y(iline, xline)float646.477e+08 6.477e+08 ... 6.479e+08<pre>array([[6.47744704e+08, 6.47745006e+08, 6.47745309e+08, ...,\n        6.47804889e+08, 6.47805192e+08, 6.47805494e+08],\n       [6.47745917e+08, 6.47746219e+08, 6.47746522e+08, ...,\n        6.47806102e+08, 6.47806405e+08, 6.47806707e+08],\n       [6.47747130e+08, 6.47747432e+08, 6.47747735e+08, ...,\n        6.47807315e+08, 6.47807617e+08, 6.47807920e+08],\n       ...,\n       [6.47815050e+08, 6.47815352e+08, 6.47815655e+08, ...,\n        6.47875235e+08, 6.47875538e+08, 6.47875840e+08],\n       [6.47816263e+08, 6.47816565e+08, 6.47816868e+08, ...,\n        6.47876448e+08, 6.47876751e+08, 6.47877053e+08],\n       [6.47817476e+08, 6.47817778e+08, 6.47818081e+08, ...,\n        6.47877661e+08, 6.47877963e+08, 6.47878266e+08]])</pre></li><li>iline(iline)int1610090 10091 10092 ... 10149 10150<pre>array([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150], dtype=int16)</pre></li><li>xline(xline)int162150 2151 2152 ... 2349 2350 2351<pre>array([2150, 2151, 2152, ..., 2349, 2350, 2351], dtype=int16)</pre></li><li>samples(samples)float324.0 8.0 12.0 ... 3.396e+03 3.4e+03<pre>array([   4.,    8.,   12., ..., 3392., 3396., 3400.], dtype=float32)</pre></li></ul></li><li>Data variables: (1)<ul><li>data(iline, xline, samples)float320.02057 0.02204 0.01966 ... 0.0 0.0<pre>array([[[ 2.05745399e-02,  2.20407024e-02,  1.96589418e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 1.23256370e-02,  1.59417503e-02,  1.38800517e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [-1.63372169e-05,  4.92771342e-03,  3.24293785e-03, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        ...,\n        [ 5.47898412e-02,  5.22681139e-02,  5.00054434e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 6.27492070e-02,  6.00568764e-02,  5.75365834e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 6.91987872e-02,  6.67222738e-02,  6.40410781e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n\n       [[-3.46487835e-02, -3.38801444e-02, -3.20093483e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [-4.10056897e-02, -4.02579159e-02, -3.83855253e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [-5.01492284e-02, -4.94211540e-02, -4.75146063e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n...\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [-8.89039040e-03, -9.03325155e-03, -8.77474993e-03, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [-8.80905986e-03, -9.18869302e-03, -8.81006569e-03, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n\n       [[ 2.17335932e-02,  1.93170868e-02,  2.07530260e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 2.12477669e-02,  1.89855732e-02,  2.04903930e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 2.13435888e-02,  1.91475898e-02,  2.07546689e-02, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        ...,\n        [ 6.19068742e-03,  5.15908375e-03,  6.23933598e-03, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 7.53349811e-03,  6.49160147e-03,  7.51641765e-03, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 9.05360654e-03,  7.92511553e-03,  8.82008299e-03, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n      dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099,\n       10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109,\n       10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119,\n       10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129,\n       10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139,\n       10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149,\n       10150],\n      dtype='int16', name='iline'))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159,\n       ...\n       2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351],\n      dtype='int16', name='xline', length=202))</pre></li><li>samplesPandasIndex<pre>PandasIndex(Index([   4.0,    8.0,   12.0,   16.0,   20.0,   24.0,   28.0,   32.0,   36.0,\n         40.0,\n       ...\n       3364.0, 3368.0, 3372.0, 3376.0, 3380.0, 3384.0, 3388.0, 3392.0, 3396.0,\n       3400.0],\n      dtype='float32', name='samples', length=850))</pre></li></ul></li><li>Attributes: (1)seisnc :{\"coord_scalar\": -100.0, \"coord_scaled\": false}</li></ul> <p>Afterwards, use the <code>to_segy</code> method as normal to return to SEGY.</p> In\u00a0[9]: Copied! <pre>volve_3d_ds.seisio.to_segy(\"data/test.segy\", iline=189, xline=193, trace_header_map={'cdp_x':181, 'cdp_y':185})\n</pre> volve_3d_ds.seisio.to_segy(\"data/test.segy\", iline=189, xline=193, trace_header_map={'cdp_x':181, 'cdp_y':185})"},{"location":"examples/example_segysak_segy_vectorisation.html#seg-y-to-vector-dataframes-and-back","title":"SEG-Y to Vector DataFrames and Back\u00b6","text":"<p>The connection of segysak to <code>xarray</code> greatly simplifies the process of vectorising segy 3D data and returning it to SEGY. To do this, one can use the close relationship between <code>pandas</code> and <code>xarray</code>.</p>"},{"location":"examples/example_segysak_segy_vectorisation.html#loading-data","title":"Loading Data\u00b6","text":"<p>We start by loading data normally using the <code>segy_loader</code> utility. For this example we will use the Volve example sub-cube.</p>"},{"location":"examples/example_segysak_segy_vectorisation.html#vectorisation","title":"Vectorisation\u00b6","text":"<p>Once the data is loaded it can be converted to a <code>pandas.DataFrame</code> directly from the loaded <code>Dataset</code>. The Dataframe is multi-index and contains columns for each variable in the originally loaded dataset. This includes the seismic amplitude as <code>data</code> and the <code>cdp_x</code> and <code>cdp_y</code> locations. If you require smaller volumes from the input data, you can use xarray selection methods prior to conversion to a DataFrame.</p>"},{"location":"examples/example_segysak_segy_vectorisation.html#return-to-xarray","title":"Return to Xarray\u00b6","text":"<p>It is possible to return the DataFrame to the Dataset for output to SEGY. To do this the multi-index must be reset. Afterward, <code>pandas</code> provides the <code>to_xarray</code> method.</p>"},{"location":"examples/example_segysak_segy_vectorisation.html#very-large-datasets","title":"Very large datasets\u00b6","text":"<p>If you have a very large dataset (SEG-Y file), it may be possible to use <code>ds.to_dask_dataframe()</code> which can perform operations, including the writing of data in a lazy manner.</p>"},{"location":"examples/example_working_with_3d_gathers.html","title":"Working with 3D Gathers","text":"In\u00a0[2]: Copied! <pre>import pathlib\nfrom IPython.display import display\nimport pandas as pd\nimport xarray as xr\nimport numpy as np\nfrom segysak.segy import segy_loader, segy_header_scan\nimport matplotlib.pyplot as plt\n</pre> import pathlib from IPython.display import display import pandas as pd import xarray as xr import numpy as np from segysak.segy import segy_loader, segy_header_scan import matplotlib.pyplot as plt <p>This example uses a subset of the Penobscot 3D with data exported from the OpendTect project.</p> <p>First we scan the data to determine which byte locations contain the relevant information. We will need to provide a byte location for the offset variable so a 4th dimension can be created when loaded the data.</p> In\u00a0[3]: Copied! <pre>segy_file = pathlib.Path(\"data/3D_gathers_pstm_nmo.sgy\")\nwith pd.option_context(\"display.max_rows\", 100):\n    display(segy_header_scan(segy_file))\n</pre> segy_file = pathlib.Path(\"data/3D_gathers_pstm_nmo.sgy\") with pd.option_context(\"display.max_rows\", 100):     display(segy_header_scan(segy_file)) byte_loc count mean std min 25% 50% 75% max TRACE_SEQUENCE_LINE 1 1000.0 2.797410e+02 186.100369 1.0 125.75 250.5 421.25 671.0 TRACE_SEQUENCE_FILE 5 1000.0 3.055600e+01 17.664623 1.0 15.00 30.0 46.00 61.0 FieldRecord 9 1000.0 1.290329e+03 0.470085 1290.0 1290.00 1290.0 1291.00 1291.0 TraceNumber 13 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 EnergySourcePoint 17 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 CDP 21 1000.0 1.154085e+03 3.039245 1150.0 1152.00 1154.0 1156.00 1160.0 CDP_TRACE 25 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 TraceIdentificationCode 29 1000.0 1.000000e+00 0.000000 1.0 1.00 1.0 1.00 1.0 NSummedTraces 31 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 NStackedTraces 33 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 DataUse 35 1000.0 1.000000e+00 0.000000 1.0 1.00 1.0 1.00 1.0 offset 37 1000.0 1.652800e+03 883.231146 175.0 875.00 1625.0 2425.00 3175.0 ReceiverGroupElevation 41 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceSurfaceElevation 45 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceDepth 49 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 ReceiverDatumElevation 53 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceDatumElevation 57 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceWaterDepth 61 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 GroupWaterDepth 65 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 ElevationScalar 69 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceGroupScalar 71 1000.0 -1.000000e+01 0.000000 -10.0 -10.00 -10.0 -10.00 -10.0 SourceX 73 1000.0 7.337142e+06 685.656940 7336198.0 7336642.00 7337085.0 7337586.00 7338472.0 SourceY 77 1000.0 4.895109e+07 333.157506 48950580.0 48950812.00 48951044.0 48951276.00 48951739.0 GroupX 81 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 GroupY 85 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 CoordinateUnits 89 1000.0 1.000000e+00 0.000000 1.0 1.00 1.0 1.00 1.0 WeatheringVelocity 91 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SubWeatheringVelocity 93 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceUpholeTime 95 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 GroupUpholeTime 97 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceStaticCorrection 99 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 GroupStaticCorrection 101 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 TotalStaticApplied 103 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 LagTimeA 105 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 LagTimeB 107 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 DelayRecordingTime 109 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 MuteTimeStart 111 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 MuteTimeEND 113 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 TRACE_SAMPLE_COUNT 115 1000.0 7.510000e+02 0.000000 751.0 751.00 751.0 751.00 751.0 TRACE_SAMPLE_INTERVAL 117 1000.0 4.000000e+03 0.000000 4000.0 4000.00 4000.0 4000.00 4000.0 GainType 119 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 InstrumentGainConstant 121 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 InstrumentInitialGain 123 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 Correlated 125 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SweepFrequencyStart 127 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SweepFrequencyEnd 129 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SweepLength 131 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SweepType 133 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SweepTraceTaperLengthStart 135 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SweepTraceTaperLengthEnd 137 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 TaperType 139 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 AliasFilterFrequency 141 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 AliasFilterSlope 143 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 NotchFilterFrequency 145 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 NotchFilterSlope 147 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 LowCutFrequency 149 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 HighCutFrequency 151 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 LowCutSlope 153 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 HighCutSlope 155 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 YearDataRecorded 157 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 DayOfYear 159 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 HourOfDay 161 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 MinuteOfHour 163 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SecondOfMinute 165 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 TimeBaseCode 167 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 TraceWeightingFactor 169 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 GeophoneGroupNumberRoll1 171 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 GeophoneGroupNumberFirstTraceOrigField 173 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 GeophoneGroupNumberLastTraceOrigField 175 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 GapSize 177 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 OverTravel 179 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 CDP_X 181 1000.0 7.337142e+06 685.656940 7336198.0 7336642.00 7337085.0 7337586.00 7338472.0 CDP_Y 185 1000.0 4.895109e+07 333.157506 48950580.0 48950812.00 48951044.0 48951276.00 48951739.0 INLINE_3D 189 1000.0 1.290329e+03 0.470085 1290.0 1290.00 1290.0 1291.00 1291.0 CROSSLINE_3D 193 1000.0 1.154085e+03 3.039245 1150.0 1152.00 1154.0 1156.00 1160.0 ShotPoint 197 1000.0 3.055600e+01 17.664623 1.0 15.00 30.0 46.00 61.0 ShotPointScalar 201 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 TraceValueMeasurementUnit 203 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 TransductionConstantMantissa 205 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 TransductionConstantPower 209 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 TransductionUnit 211 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 TraceIdentifier 213 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 ScalarTraceHeader 215 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceType 217 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceEnergyDirectionMantissa 219 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceEnergyDirectionExponent 223 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceMeasurementMantissa 225 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceMeasurementExponent 229 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 SourceMeasurementUnit 231 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 UnassignedInt1 233 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 UnassignedInt2 237 1000.0 0.000000e+00 0.000000 0.0 0.00 0.0 0.00 0.0 In\u00a0[4]: Copied! <pre>penobscot_3d_gath = xr.open_dataset(\n    segy_file, dim_byte_fields={\"iline\":189, \"xline\":193, \"offset\":37}, extra_byte_fields={\"cdp_x\":181, \"cdp_y\":185}\n)\n\n# coordinates are not scaled automatically, to use the coordinate scalar found in the trace headers, we run scale_coords()\npenobscot_3d_gath.segysak.scale_coords()\n</pre> penobscot_3d_gath = xr.open_dataset(     segy_file, dim_byte_fields={\"iline\":189, \"xline\":193, \"offset\":37}, extra_byte_fields={\"cdp_x\":181, \"cdp_y\":185} )  # coordinates are not scaled automatically, to use the coordinate scalar found in the trace headers, we run scale_coords() penobscot_3d_gath.segysak.scale_coords() <p>Note that the loaded Dataset has four dimensions with the additional dimension labeled offset. There are 61 offsets in this dataset or 61 traces per inline and xline location.</p> In\u00a0[5]: Copied! <pre>display(penobscot_3d_gath)\nprint(penobscot_3d_gath.offset.values)\n</pre> display(penobscot_3d_gath) print(penobscot_3d_gath.offset.values) <pre>&lt;xarray.Dataset&gt; Size: 22MB\nDimensions:  (iline: 11, xline: 11, offset: 61, samples: 751)\nCoordinates:\n  * iline    (iline) int16 22B 1290 1291 1292 1293 1294 ... 1297 1298 1299 1300\n  * xline    (xline) int16 22B 1150 1151 1152 1153 1154 ... 1157 1158 1159 1160\n  * offset   (offset) int16 122B 175 225 275 325 375 ... 3025 3075 3125 3175\n  * samples  (samples) float32 3kB 0.0 4.0 8.0 ... 2.992e+03 2.996e+03 3e+03\nData variables:\n    cdp_x    (iline, xline, offset) float64 59kB 7.336e+05 ... 7.338e+05\n    cdp_y    (iline, xline, offset) float64 59kB 4.895e+06 ... 4.895e+06\n    data     (iline, xline, offset, samples) float32 22MB ...\nAttributes:\n    seisnc:   {\"coord_scalar\": -10.0, \"coord_scaled\": true}</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>iline: 11</li><li>xline: 11</li><li>offset: 61</li><li>samples: 751</li></ul></li><li>Coordinates: (4)<ul><li>iline(iline)int161290 1291 1292 ... 1298 1299 1300<pre>array([1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300],\n      dtype=int16)</pre></li><li>xline(xline)int161150 1151 1152 ... 1158 1159 1160<pre>array([1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160],\n      dtype=int16)</pre></li><li>offset(offset)int16175 225 275 325 ... 3075 3125 3175<pre>array([ 175,  225,  275,  325,  375,  425,  475,  525,  575,  625,  675,  725,\n        775,  825,  875,  925,  975, 1025, 1075, 1125, 1175, 1225, 1275, 1325,\n       1375, 1425, 1475, 1525, 1575, 1625, 1675, 1725, 1775, 1825, 1875, 1925,\n       1975, 2025, 2075, 2125, 2175, 2225, 2275, 2325, 2375, 2425, 2475, 2525,\n       2575, 2625, 2675, 2725, 2775, 2825, 2875, 2925, 2975, 3025, 3075, 3125,\n       3175], dtype=int16)</pre></li><li>samples(samples)float320.0 4.0 8.0 ... 2.996e+03 3e+03<pre>array([   0.,    4.,    8., ..., 2992., 2996., 3000.], dtype=float32)</pre></li></ul></li><li>Data variables: (3)<ul><li>cdp_x(iline, xline, offset)float647.336e+05 7.336e+05 ... 7.338e+05<pre>array([[[733625.6, 733625.6, 733625.6, ..., 733625.6, 733625.6,\n         733625.6],\n        [733647.8, 733647.8, 733647.8, ..., 733647.8, 733647.8,\n         733647.8],\n        [733669.9, 733669.9, 733669.9, ..., 733669.9, 733669.9,\n         733669.9],\n        ...,\n        [733802.9, 733802.9, 733802.9, ..., 733802.9, 733802.9,\n         733802.9],\n        [733825.1, 733825.1, 733825.1, ..., 733825.1, 733825.1,\n         733825.1],\n        [733847.2, 733847.2, 733847.2, ..., 733847.2, 733847.2,\n         733847.2]],\n\n       [[733619.8, 733619.8, 733619.8, ..., 733619.8, 733619.8,\n         733619.8],\n        [733642. , 733642. , 733642. , ..., 733642. , 733642. ,\n         733642. ],\n        [733664.2, 733664.2, 733664.2, ..., 733664.2, 733664.2,\n         733664.2],\n...\n        [733750.8, 733750.8, 733750.8, ..., 733750.8, 733750.8,\n         733750.8],\n        [733772.9, 733772.9, 733772.9, ..., 733772.9, 733772.9,\n         733772.9],\n        [733795.1, 733795.1, 733795.1, ..., 733795.1, 733795.1,\n         733795.1]],\n\n       [[733567.7, 733567.7, 733567.7, ..., 733567.7, 733567.7,\n         733567.7],\n        [733589.9, 733589.9, 733589.9, ..., 733589.9, 733589.9,\n         733589.9],\n        [733612. , 733612. , 733612. , ..., 733612. , 733612. ,\n         733612. ],\n        ...,\n        [733745. , 733745. , 733745. , ..., 733745. , 733745. ,\n         733745. ],\n        [733767.1, 733767.1, 733767.1, ..., 733767.1, 733767.1,\n         733767.1],\n        [733789.3, 733789.3, 733789.3, ..., 733789.3, 733789.3,\n         733789.3]]])</pre></li><li>cdp_y(iline, xline, offset)float644.895e+06 4.895e+06 ... 4.895e+06<pre>array([[[4895058. , 4895058. , 4895058. , ..., 4895058. , 4895058. ,\n         4895058. ],\n        [4895069.6, 4895069.6, 4895069.6, ..., 4895069.6, 4895069.6,\n         4895069.6],\n        [4895081.2, 4895081.2, 4895081.2, ..., 4895081.2, 4895081.2,\n         4895081.2],\n        ...,\n        [4895150.7, 4895150.7, 4895150.7, ..., 4895150.7, 4895150.7,\n         4895150.7],\n        [4895162.3, 4895162.3, 4895162.3, ..., 4895162.3, 4895162.3,\n         4895162.3],\n        [4895173.9, 4895173.9, 4895173.9, ..., 4895173.9, 4895173.9,\n         4895173.9]],\n\n       [[4895069.1, 4895069.1, 4895069.1, ..., 4895069.1, 4895069.1,\n         4895069.1],\n        [4895080.7, 4895080.7, 4895080.7, ..., 4895080.7, 4895080.7,\n         4895080.7],\n        [4895092.3, 4895092.3, 4895092.3, ..., 4895092.3, 4895092.3,\n         4895092.3],\n...\n        [4895250.4, 4895250.4, 4895250.4, ..., 4895250.4, 4895250.4,\n         4895250.4],\n        [4895262. , 4895262. , 4895262. , ..., 4895262. , 4895262. ,\n         4895262. ],\n        [4895273.6, 4895273.6, 4895273.6, ..., 4895273.6, 4895273.6,\n         4895273.6]],\n\n       [[4895168.8, 4895168.8, 4895168.8, ..., 4895168.8, 4895168.8,\n         4895168.8],\n        [4895180.3, 4895180.3, 4895180.3, ..., 4895180.3, 4895180.3,\n         4895180.3],\n        [4895191.9, 4895191.9, 4895191.9, ..., 4895191.9, 4895191.9,\n         4895191.9],\n        ...,\n        [4895261.5, 4895261.5, 4895261.5, ..., 4895261.5, 4895261.5,\n         4895261.5],\n        [4895273. , 4895273. , 4895273. , ..., 4895273. , 4895273. ,\n         4895273. ],\n        [4895284.6, 4895284.6, 4895284.6, ..., 4895284.6, 4895284.6,\n         4895284.6]]])</pre></li><li>data(iline, xline, offset, samples)float32...seisnc :{\"source_file\": \"data/3D_gathers_pstm_nmo.sgy\", \"measurement_system\": \"m\", \"sample_rate\": 4.0}text :\u00e4\u0090\u0091\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0016\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0093\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0094\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0095\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0096\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0004\u0080\u0080&amp;?\u00cb\u00d1\u00c8\u00d1?&gt;\u00cb\u0080/\u00c4\u00c4?\u00ca\u00c0\u00d1&gt;\u00c5\u0080\u00c8?\u0080\u00c8\u00c7\u00c1\u0080\u00eb\u00e1\u00e5\u0005\u00df\u0080\u00ea\u00c1\u00ce\u0006\u0080\u0091\u0080\u00cb\u00c8/&gt;\u00c0/\u00ca\u00c0\u0006\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0098\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0099\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0090\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0091\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0016\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0093\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0094\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0095\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0096\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0004\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0098\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0099\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0090\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0091\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0016\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0093\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0094\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0095\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0096\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0004\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0098\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0099\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0090\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0091\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0016\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0093\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0094\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0095\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0096\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0004\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0098\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0099\u0080\u0080\u00eb\u00e1\u00e5\u0080\u00df\u0080\u00ea\u00e1\u00ee\u0091\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0094\u0090\u0080\u0080\u00e1+\u00e0\u0080\u00e8\u00e1\u00ec\u00e8\u00ed\u00a0&lt;\u0080\u00e7\u00e1\u00a0\u00e0\u00e1\u00ea\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080<pre>[5543131 values with dtype=float32]</pre></li></ul></li><li>Indexes: (4)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300], dtype='int16', name='iline'))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160], dtype='int16', name='xline'))</pre></li><li>offsetPandasIndex<pre>PandasIndex(Index([ 175,  225,  275,  325,  375,  425,  475,  525,  575,  625,  675,  725,\n        775,  825,  875,  925,  975, 1025, 1075, 1125, 1175, 1225, 1275, 1325,\n       1375, 1425, 1475, 1525, 1575, 1625, 1675, 1725, 1775, 1825, 1875, 1925,\n       1975, 2025, 2075, 2125, 2175, 2225, 2275, 2325, 2375, 2425, 2475, 2525,\n       2575, 2625, 2675, 2725, 2775, 2825, 2875, 2925, 2975, 3025, 3075, 3125,\n       3175],\n      dtype='int16', name='offset'))</pre></li><li>samplesPandasIndex<pre>PandasIndex(Index([   0.0,    4.0,    8.0,   12.0,   16.0,   20.0,   24.0,   28.0,   32.0,\n         36.0,\n       ...\n       2964.0, 2968.0, 2972.0, 2976.0, 2980.0, 2984.0, 2988.0, 2992.0, 2996.0,\n       3000.0],\n      dtype='float32', name='samples', length=751))</pre></li></ul></li><li>Attributes: (1)seisnc :{\"coord_scalar\": -10.0, \"coord_scaled\": true}</li></ul> <pre>[ 175  225  275  325  375  425  475  525  575  625  675  725  775  825\n  875  925  975 1025 1075 1125 1175 1225 1275 1325 1375 1425 1475 1525\n 1575 1625 1675 1725 1775 1825 1875 1925 1975 2025 2075 2125 2175 2225\n 2275 2325 2375 2425 2475 2525 2575 2625 2675 2725 2775 2825 2875 2925\n 2975 3025 3075 3125 3175]\n</pre> <p>Lets check that the data looks OK for a couple of offsets. We've only got a small dataset of 11x11 traces so the seismic will look at little odd at this scale.</p> In\u00a0[6]: Copied! <pre>fig, axs = plt.subplots(ncols=2, figsize=(20, 10))\n\npenobscot_3d_gath.isel(iline=5, offset=0).data.T.plot(\n    yincrease=False, ax=axs[0], vmax=5000\n)\npenobscot_3d_gath.isel(xline=5, offset=0).data.T.plot(\n    yincrease=False, ax=axs[1], vmax=5000\n)\n</pre> fig, axs = plt.subplots(ncols=2, figsize=(20, 10))  penobscot_3d_gath.isel(iline=5, offset=0).data.T.plot(     yincrease=False, ax=axs[0], vmax=5000 ) penobscot_3d_gath.isel(xline=5, offset=0).data.T.plot(     yincrease=False, ax=axs[1], vmax=5000 ) Out[6]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7f53117b9e10&gt;</pre> In\u00a0[7]: Copied! <pre>fig, axs = plt.subplots(figsize=(20, 10))\naxs.imshow(\n    penobscot_3d_gath.isel(iline=0)\n    .data.stack(stacked_offset=(\"xline\", \"offset\"))\n    .values,\n    vmin=-5000,\n    vmax=5000,\n    cmap=\"seismic\",\n    aspect=\"auto\",\n)\n</pre> fig, axs = plt.subplots(figsize=(20, 10)) axs.imshow(     penobscot_3d_gath.isel(iline=0)     .data.stack(stacked_offset=(\"xline\", \"offset\"))     .values,     vmin=-5000,     vmax=5000,     cmap=\"seismic\",     aspect=\"auto\", ) Out[7]: <pre>&lt;matplotlib.image.AxesImage at 0x7f535822bee0&gt;</pre> <p>One can easily create a common offset stack by reversing the stacked dimension arguments <code>\"offset\"</code> and <code>\"xline\"</code>.</p> In\u00a0[8]: Copied! <pre>fig, axs = plt.subplots(figsize=(20, 10))\naxs.imshow(\n    penobscot_3d_gath.isel(iline=0)\n    .data.stack(stacked_offset=(\"offset\", \"xline\"))\n    .values,\n    vmin=-5000,\n    vmax=5000,\n    cmap=\"seismic\",\n    aspect=\"auto\",\n)\n</pre> fig, axs = plt.subplots(figsize=(20, 10)) axs.imshow(     penobscot_3d_gath.isel(iline=0)     .data.stack(stacked_offset=(\"offset\", \"xline\"))     .values,     vmin=-5000,     vmax=5000,     cmap=\"seismic\",     aspect=\"auto\", ) Out[8]: <pre>&lt;matplotlib.image.AxesImage at 0x7f5310757e20&gt;</pre> In\u00a0[9]: Copied! <pre>penobscot_3d_gath\n</pre> penobscot_3d_gath Out[9]: <pre>&lt;xarray.Dataset&gt; Size: 22MB\nDimensions:  (iline: 11, xline: 11, offset: 61, samples: 751)\nCoordinates:\n  * iline    (iline) int16 22B 1290 1291 1292 1293 1294 ... 1297 1298 1299 1300\n  * xline    (xline) int16 22B 1150 1151 1152 1153 1154 ... 1157 1158 1159 1160\n  * offset   (offset) int16 122B 175 225 275 325 375 ... 3025 3075 3125 3175\n  * samples  (samples) float32 3kB 0.0 4.0 8.0 ... 2.992e+03 2.996e+03 3e+03\nData variables:\n    cdp_x    (iline, xline, offset) float64 59kB 7.336e+05 ... 7.338e+05\n    cdp_y    (iline, xline, offset) float64 59kB 4.895e+06 ... 4.895e+06\n    data     (iline, xline, offset, samples) float32 22MB ...\nAttributes:\n    seisnc:   {\"coord_scalar\": -10.0, \"coord_scaled\": true}</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>iline: 11</li><li>xline: 11</li><li>offset: 61</li><li>samples: 751</li></ul></li><li>Coordinates: (4)<ul><li>iline(iline)int161290 1291 1292 ... 1298 1299 1300<pre>array([1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300],\n      dtype=int16)</pre></li><li>xline(xline)int161150 1151 1152 ... 1158 1159 1160<pre>array([1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160],\n      dtype=int16)</pre></li><li>offset(offset)int16175 225 275 325 ... 3075 3125 3175<pre>array([ 175,  225,  275,  325,  375,  425,  475,  525,  575,  625,  675,  725,\n        775,  825,  875,  925,  975, 1025, 1075, 1125, 1175, 1225, 1275, 1325,\n       1375, 1425, 1475, 1525, 1575, 1625, 1675, 1725, 1775, 1825, 1875, 1925,\n       1975, 2025, 2075, 2125, 2175, 2225, 2275, 2325, 2375, 2425, 2475, 2525,\n       2575, 2625, 2675, 2725, 2775, 2825, 2875, 2925, 2975, 3025, 3075, 3125,\n       3175], dtype=int16)</pre></li><li>samples(samples)float320.0 4.0 8.0 ... 2.996e+03 3e+03<pre>array([   0.,    4.,    8., ..., 2992., 2996., 3000.], dtype=float32)</pre></li></ul></li><li>Data variables: (3)<ul><li>cdp_x(iline, xline, offset)float647.336e+05 7.336e+05 ... 7.338e+05<pre>array([[[733625.6, 733625.6, 733625.6, ..., 733625.6, 733625.6,\n         733625.6],\n        [733647.8, 733647.8, 733647.8, ..., 733647.8, 733647.8,\n         733647.8],\n        [733669.9, 733669.9, 733669.9, ..., 733669.9, 733669.9,\n         733669.9],\n        ...,\n        [733802.9, 733802.9, 733802.9, ..., 733802.9, 733802.9,\n         733802.9],\n        [733825.1, 733825.1, 733825.1, ..., 733825.1, 733825.1,\n         733825.1],\n        [733847.2, 733847.2, 733847.2, ..., 733847.2, 733847.2,\n         733847.2]],\n\n       [[733619.8, 733619.8, 733619.8, ..., 733619.8, 733619.8,\n         733619.8],\n        [733642. , 733642. , 733642. , ..., 733642. , 733642. ,\n         733642. ],\n        [733664.2, 733664.2, 733664.2, ..., 733664.2, 733664.2,\n         733664.2],\n...\n        [733750.8, 733750.8, 733750.8, ..., 733750.8, 733750.8,\n         733750.8],\n        [733772.9, 733772.9, 733772.9, ..., 733772.9, 733772.9,\n         733772.9],\n        [733795.1, 733795.1, 733795.1, ..., 733795.1, 733795.1,\n         733795.1]],\n\n       [[733567.7, 733567.7, 733567.7, ..., 733567.7, 733567.7,\n         733567.7],\n        [733589.9, 733589.9, 733589.9, ..., 733589.9, 733589.9,\n         733589.9],\n        [733612. , 733612. , 733612. , ..., 733612. , 733612. ,\n         733612. ],\n        ...,\n        [733745. , 733745. , 733745. , ..., 733745. , 733745. ,\n         733745. ],\n        [733767.1, 733767.1, 733767.1, ..., 733767.1, 733767.1,\n         733767.1],\n        [733789.3, 733789.3, 733789.3, ..., 733789.3, 733789.3,\n         733789.3]]])</pre></li><li>cdp_y(iline, xline, offset)float644.895e+06 4.895e+06 ... 4.895e+06<pre>array([[[4895058. , 4895058. , 4895058. , ..., 4895058. , 4895058. ,\n         4895058. ],\n        [4895069.6, 4895069.6, 4895069.6, ..., 4895069.6, 4895069.6,\n         4895069.6],\n        [4895081.2, 4895081.2, 4895081.2, ..., 4895081.2, 4895081.2,\n         4895081.2],\n        ...,\n        [4895150.7, 4895150.7, 4895150.7, ..., 4895150.7, 4895150.7,\n         4895150.7],\n        [4895162.3, 4895162.3, 4895162.3, ..., 4895162.3, 4895162.3,\n         4895162.3],\n        [4895173.9, 4895173.9, 4895173.9, ..., 4895173.9, 4895173.9,\n         4895173.9]],\n\n       [[4895069.1, 4895069.1, 4895069.1, ..., 4895069.1, 4895069.1,\n         4895069.1],\n        [4895080.7, 4895080.7, 4895080.7, ..., 4895080.7, 4895080.7,\n         4895080.7],\n        [4895092.3, 4895092.3, 4895092.3, ..., 4895092.3, 4895092.3,\n         4895092.3],\n...\n        [4895250.4, 4895250.4, 4895250.4, ..., 4895250.4, 4895250.4,\n         4895250.4],\n        [4895262. , 4895262. , 4895262. , ..., 4895262. , 4895262. ,\n         4895262. ],\n        [4895273.6, 4895273.6, 4895273.6, ..., 4895273.6, 4895273.6,\n         4895273.6]],\n\n       [[4895168.8, 4895168.8, 4895168.8, ..., 4895168.8, 4895168.8,\n         4895168.8],\n        [4895180.3, 4895180.3, 4895180.3, ..., 4895180.3, 4895180.3,\n         4895180.3],\n        [4895191.9, 4895191.9, 4895191.9, ..., 4895191.9, 4895191.9,\n         4895191.9],\n        ...,\n        [4895261.5, 4895261.5, 4895261.5, ..., 4895261.5, 4895261.5,\n         4895261.5],\n        [4895273. , 4895273. , 4895273. , ..., 4895273. , 4895273. ,\n         4895273. ],\n        [4895284.6, 4895284.6, 4895284.6, ..., 4895284.6, 4895284.6,\n         4895284.6]]])</pre></li><li>data(iline, xline, offset, samples)float32...seisnc :{\"source_file\": \"data/3D_gathers_pstm_nmo.sgy\", \"measurement_system\": \"m\", \"sample_rate\": 4.0}text :\u00e4\u0090\u0091\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0016\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0093\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0094\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0095\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0096\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0004\u0080\u0080&amp;?\u00cb\u00d1\u00c8\u00d1?&gt;\u00cb\u0080/\u00c4\u00c4?\u00ca\u00c0\u00d1&gt;\u00c5\u0080\u00c8?\u0080\u00c8\u00c7\u00c1\u0080\u00eb\u00e1\u00e5\u0005\u00df\u0080\u00ea\u00c1\u00ce\u0006\u0080\u0091\u0080\u00cb\u00c8/&gt;\u00c0/\u00ca\u00c0\u0006\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0098\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0090\u0099\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0090\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0091\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0016\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0093\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0094\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0095\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0096\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0004\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0098\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0091\u0099\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0090\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0091\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0016\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0093\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0094\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0095\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0096\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0004\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0098\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0016\u0099\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0090\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0091\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0016\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0093\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0094\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0095\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0096\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0004\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0098\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0093\u0099\u0080\u0080\u00eb\u00e1\u00e5\u0080\u00df\u0080\u00ea\u00e1\u00ee\u0091\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080 \u00e4\u0094\u0090\u0080\u0080\u00e1+\u00e0\u0080\u00e8\u00e1\u00ec\u00e8\u00ed\u00a0&lt;\u0080\u00e7\u00e1\u00a0\u00e0\u00e1\u00ea\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080\u0080<pre>[5543131 values with dtype=float32]</pre></li></ul></li><li>Indexes: (4)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300], dtype='int16', name='iline'))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160], dtype='int16', name='xline'))</pre></li><li>offsetPandasIndex<pre>PandasIndex(Index([ 175,  225,  275,  325,  375,  425,  475,  525,  575,  625,  675,  725,\n        775,  825,  875,  925,  975, 1025, 1075, 1125, 1175, 1225, 1275, 1325,\n       1375, 1425, 1475, 1525, 1575, 1625, 1675, 1725, 1775, 1825, 1875, 1925,\n       1975, 2025, 2075, 2125, 2175, 2225, 2275, 2325, 2375, 2425, 2475, 2525,\n       2575, 2625, 2675, 2725, 2775, 2825, 2875, 2925, 2975, 3025, 3075, 3125,\n       3175],\n      dtype='int16', name='offset'))</pre></li><li>samplesPandasIndex<pre>PandasIndex(Index([   0.0,    4.0,    8.0,   12.0,   16.0,   20.0,   24.0,   28.0,   32.0,\n         36.0,\n       ...\n       2964.0, 2968.0, 2972.0, 2976.0, 2980.0, 2984.0, 2988.0, 2992.0, 2996.0,\n       3000.0],\n      dtype='float32', name='samples', length=751))</pre></li></ul></li><li>Attributes: (1)seisnc :{\"coord_scalar\": -10.0, \"coord_scaled\": true}</li></ul> In\u00a0[10]: Copied! <pre>penobscot_3d_gath.drop_dims('offset')\n</pre> penobscot_3d_gath.drop_dims('offset') Out[10]: <pre>&lt;xarray.Dataset&gt; Size: 3kB\nDimensions:  (iline: 11, xline: 11, samples: 751)\nCoordinates:\n  * iline    (iline) int16 22B 1290 1291 1292 1293 1294 ... 1297 1298 1299 1300\n  * xline    (xline) int16 22B 1150 1151 1152 1153 1154 ... 1157 1158 1159 1160\n  * samples  (samples) float32 3kB 0.0 4.0 8.0 ... 2.992e+03 2.996e+03 3e+03\nData variables:\n    *empty*\nAttributes:\n    seisnc:   {\"coord_scalar\": -10.0, \"coord_scaled\": true}</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>iline: 11</li><li>xline: 11</li><li>samples: 751</li></ul></li><li>Coordinates: (3)<ul><li>iline(iline)int161290 1291 1292 ... 1298 1299 1300<pre>array([1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300],\n      dtype=int16)</pre></li><li>xline(xline)int161150 1151 1152 ... 1158 1159 1160<pre>array([1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160],\n      dtype=int16)</pre></li><li>samples(samples)float320.0 4.0 8.0 ... 2.996e+03 3e+03<pre>array([   0.,    4.,    8., ..., 2992., 2996., 3000.], dtype=float32)</pre></li></ul></li><li>Data variables: (0)<ul></ul></li><li>Indexes: (3)<ul><li>ilinePandasIndex<pre>PandasIndex(Index([1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300], dtype='int16', name='iline'))</pre></li><li>xlinePandasIndex<pre>PandasIndex(Index([1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160], dtype='int16', name='xline'))</pre></li><li>samplesPandasIndex<pre>PandasIndex(Index([   0.0,    4.0,    8.0,   12.0,   16.0,   20.0,   24.0,   28.0,   32.0,\n         36.0,\n       ...\n       2964.0, 2968.0, 2972.0, 2976.0, 2980.0, 2984.0, 2988.0, 2992.0, 2996.0,\n       3000.0],\n      dtype='float32', name='samples', length=751))</pre></li></ul></li><li>Attributes: (1)seisnc :{\"coord_scalar\": -10.0, \"coord_scaled\": true}</li></ul> In\u00a0[11]: Copied! <pre>penobscot_3d_gath.isel(offset=0).segysak.calc_corner_points()\n</pre> penobscot_3d_gath.isel(offset=0).segysak.calc_corner_points() Out[11]: <pre>[(733625.6000000001, 4895058.0),\n (733847.2000000001, 4895173.9),\n (733789.3, 4895284.600000001),\n (733567.7000000001, 4895168.8),\n (733625.6000000001, 4895058.0)]</pre> In\u00a0[12]: Copied! <pre>arb_line = np.array([(733600, 4895180.0), (733850, 4895180.0)])\n\n# we must drop the offset axis somehow, xy locations were loaded for each header position but\n# they must be reduced, we could take the mean over the dimension but here we just select the \n# first value using `offset=0`\nax = penobscot_3d_gath.isel(offset=0).segysak.plot_bounds()\nax.plot(arb_line[:, 0], arb_line[:, 1], label=\"arb_line\")\nplt.legend()\n</pre> arb_line = np.array([(733600, 4895180.0), (733850, 4895180.0)])  # we must drop the offset axis somehow, xy locations were loaded for each header position but # they must be reduced, we could take the mean over the dimension but here we just select the  # first value using `offset=0` ax = penobscot_3d_gath.isel(offset=0).segysak.plot_bounds() ax.plot(arb_line[:, 0], arb_line[:, 1], label=\"arb_line\") plt.legend() Out[12]: <pre>&lt;matplotlib.legend.Legend at 0x7f531141bf10&gt;</pre> <p>Here we need to think carefully about the <code>bin_spacing_hint</code>. We also don't want to interpolate the gathers, so we use <code>xysel_method=\"nearest\"</code>.</p> In\u00a0[13]: Copied! <pre>penobscot_3d_gath_arb = penobscot_3d_gath.segysak.xysel(\n    arb_line, method=\"nearest\"\n)\n</pre> penobscot_3d_gath_arb = penobscot_3d_gath.segysak.xysel(     arb_line, method=\"nearest\" ) In\u00a0[14]: Copied! <pre>fig, axs = plt.subplots(figsize=(20, 10))\naxs.imshow(\n    penobscot_3d_gath_arb.data.stack(\n        stacked_offset=(\n            \"cdp\",\n            \"offset\",\n        )\n    ).values,\n    vmin=-5000,\n    vmax=5000,\n    cmap=\"seismic\",\n    aspect=\"auto\",\n)\n</pre> fig, axs = plt.subplots(figsize=(20, 10)) axs.imshow(     penobscot_3d_gath_arb.data.stack(         stacked_offset=(             \"cdp\",             \"offset\",         )     ).values,     vmin=-5000,     vmax=5000,     cmap=\"seismic\",     aspect=\"auto\", ) Out[14]: <pre>&lt;matplotlib.image.AxesImage at 0x7f531146f6a0&gt;</pre> In\u00a0[15]: Copied! <pre>fig, axs = plt.subplots(ncols=2, figsize=(20, 10))\n\npenobscot_3d_gath.isel(iline=5, xline=0).transpose('samples', ...).data.plot(\n    yincrease=False, ax=axs[0], vmax=5000,\n)\n\n# the mute relates the offset to an expected twt, let's just use a linear mute for this example\nmute = penobscot_3d_gath.offset * 0.6 + 300\n# and then we can plot it up\nmute.plot(ax=axs[0], color=\"k\")\n# apply the mute to the volume\npenobscot_3d_gath_muted = penobscot_3d_gath.where(penobscot_3d_gath.samples &gt; mute)\n\n# muted\npenobscot_3d_gath_muted.isel(iline=5, xline=0).data.transpose('samples', ...).plot(\n    yincrease=False, ax=axs[1], vmax=5000\n)\n</pre> fig, axs = plt.subplots(ncols=2, figsize=(20, 10))  penobscot_3d_gath.isel(iline=5, xline=0).transpose('samples', ...).data.plot(     yincrease=False, ax=axs[0], vmax=5000, )  # the mute relates the offset to an expected twt, let's just use a linear mute for this example mute = penobscot_3d_gath.offset * 0.6 + 300 # and then we can plot it up mute.plot(ax=axs[0], color=\"k\") # apply the mute to the volume penobscot_3d_gath_muted = penobscot_3d_gath.where(penobscot_3d_gath.samples &gt; mute)  # muted penobscot_3d_gath_muted.isel(iline=5, xline=0).data.transpose('samples', ...).plot(     yincrease=False, ax=axs[1], vmax=5000 ) Out[15]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7f53111d5a80&gt;</pre> <p>Stacking is the process of averaging the gathers for constant time to create a single trace per inline and crossline location.</p> In\u00a0[16]: Copied! <pre>fig, axs = plt.subplots(ncols=3, figsize=(20, 10))\n\nplot_kwargs = dict(vmax=5000, interpolation=\"bicubic\", yincrease=False)\n\n# compare with the zero offset trace and use imshow for interpolation\npenobscot_3d_gath.isel(iline=5, offset=0).data.T.plot.imshow(ax=axs[0], **plot_kwargs)\n\n# stack the no mute data\npenobscot_3d_gath.isel(iline=5).data.mean(\"offset\").T.plot.imshow(\n    ax=axs[1], **plot_kwargs\n)\n\n# stack the muted data\npenobscot_3d_gath_muted.isel(iline=5).data.mean(\"offset\").T.plot.imshow(\n    ax=axs[2], **plot_kwargs\n)\n</pre> fig, axs = plt.subplots(ncols=3, figsize=(20, 10))  plot_kwargs = dict(vmax=5000, interpolation=\"bicubic\", yincrease=False)  # compare with the zero offset trace and use imshow for interpolation penobscot_3d_gath.isel(iline=5, offset=0).data.T.plot.imshow(ax=axs[0], **plot_kwargs)  # stack the no mute data penobscot_3d_gath.isel(iline=5).data.mean(\"offset\").T.plot.imshow(     ax=axs[1], **plot_kwargs )  # stack the muted data penobscot_3d_gath_muted.isel(iline=5).data.mean(\"offset\").T.plot.imshow(     ax=axs[2], **plot_kwargs ) Out[16]: <pre>&lt;matplotlib.image.AxesImage at 0x7f53115588e0&gt;</pre>"},{"location":"examples/example_working_with_3d_gathers.html#working-with-3d-gathers","title":"Working with 3D Gathers\u00b6","text":"<p>Gathers or pre-stack data are common in seismic interpretation and processing. In this example we will load gather data by finding and specifying the offset byte location. Learn different ways to plot and select offset data. As well as perform a simple mute and trace stack to reduce the offset dimension.</p>"},{"location":"examples/example_working_with_3d_gathers.html#plotting-gathers-sequentially","title":"Plotting Gathers Sequentially\u00b6","text":"<p>Plotting of gathers is often done in a stacked way, displaying sequential gathers along a common dimension, usually inline or crossline. Xarray provides the <code>stack</code> method which can be used to stack labelled dimensions together.</p>"},{"location":"examples/example_working_with_3d_gathers.html#arbitrary-line-extraction-on-gathers","title":"Arbitrary line extraction on Gathers\u00b6","text":"<p>Arbitrary line slicing of gathers based upon coordinates is also possible. Lets create a line that crosses the 3D.</p>"},{"location":"examples/example_working_with_3d_gathers.html#muting-and-stacking-gathers","title":"Muting and Stacking Gathers\u00b6","text":"<p>Using one of our gathers let's define a mute function before we stack the data.</p>"},{"location":"meta/faq.html","title":"FAQ","text":""},{"location":"meta/faq.html#i-have-a-hdf5-version-conflict-error","title":"I have a HDF5 version conflict error","text":"<p>If you are in a conda environment this can occur when conflicts arrise from the installed netCDF4 binaries and your system binaries. We suggest you try updating the library with your distribution package manager. Re-creating your conda environment or trying to reinstall the netCDF4 related packages.</p>"},{"location":"meta/faq.html#how-big-can-my-input-seg-y-file-be","title":"How big can my input SEG-Y file be","text":"<p>For practical purposes SEGY-SAK is a designed as a desktop focussed tool for files that fit into memory. Files on the order of 10s of Gb can be reliably loaded into memory these days. For files greater than the amount of memory available the <code>segy_convert</code> function should be used to convert directly to NETCDF4. Conversion of very large files can be slow but they can then be lazily loaded using <code>Xarray</code> and <code>dask</code>.</p>"},{"location":"meta/faq.html#why-dont-you-use-the-global-coordinates-for-dimensions","title":"Why don't you use the global coordinates for dimensions","text":"<p>Xarray requires that our dimensions be orthogonal to each other. Often seismic data is rotated relative to the global cartesian grid and therefore it is not orthogonal any more. To get around this users of seismic data regularly work with the local seismic grid defined by the inline, crossline and vertical directions. This local grid is linked to the global coordinate system through an affine transform.</p>"},{"location":"code_of_conduct.html","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct.html#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code_of_conduct.html#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct.html#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct.html#scope","title":"Scope","text":"<p>This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct.html#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at segysak. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"code_of_conduct.html#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/\u00bc/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>"}]}